{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>PRÁCTICA 1 - FUNDAMENTOS DE APRENDIZAJE AUTOMÁTICO</h1>\n",
    "<h3>Realizada la práctica por:<br/>\n",
    "    <ol>\n",
    "    -Pablo Díez del Pozo<br/>\n",
    "    -Alejandro Alcalá Álvarez\n",
    "    </ol>\n",
    " </h3>\n",
    "<h3>Grupo: 1461</h3>\n",
    "<h3>Pareja: 01</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/python3\n",
    "# -*- coding: utf-8 -*-"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Importaciones necesarias para la ejecucion del código</h3>\n",
    "<p>Podemos observar todos los import necesarios que tenemos que realizar para que la ejecución de nuestro codigo funcione a la perfección, a continuación, explicaremos cada uno de los imports y para que son necesarios:</p>\n",
    "<ol>\n",
    "    <li>Random: se utiliza para hacer las secuencias de índices aleatorios para las particiones de entrenamiento y de clasificación.\n",
    "    <li>Math: se utiliza para hacer la distribución normal para los atributos que sean continuos y asi poder calcular su probabilidad.\n",
    "    <li>Numpy: Es la libreria mas utilizada en esta práctica, debido a que almacenamos los datos en una matriz numpy y guardamos las probabilidades posterioris de los atributos en un array de matrices de numpy.\n",
    "    <li>ABC: se utiliza para haces clases y métodos abstractos.\n",
    "    <li>Datos: se utiliza para importar toda la funcionalidad de nuestro modulo Datos.\n",
    "    <li>Collections: se utiliza para contabilizar las probabilidades condicionadas y para ver cuantas clases hay en el fichero\n",
    "    <li> SortedDict: se utiliza para ordenar el diccionario que creamos con las probabilidades a priori de cada clase\n",
    "    <li>Sklearn: se utiliza para hacer el tercer apartado de esta práctica, donde nos da una implementación del algoritmo de Naive-Bayes\n",
    "    <li>Pyplot: se utiliza en el último apartado de la práctica, donde nos da una implementación para pintar la curva ROC.\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "from abc import ABCMeta, abstractmethod\n",
    "from Datos import Datos\n",
    "from collections import Counter\n",
    "from sortedcontainers import SortedDict\n",
    "from sklearn.metrics import confusion_matrix, auc\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold, cross_val_predict\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Obtener los datos de los Distintos Dataset</h3>\n",
    "<p>Aqui vamos a poder observar como vamos a codificar los datos que nos dan en un fichero a una matriz Numpy para poder tratar los datos para poder entrenarlos y clasificarlos con Naive-Bayes</p>\n",
    "<p>Vamos a ver como llamando a la clase Datos y que en su constructor le ponemos la ruta del fichero se crea la matriz numpy de los datos, pero a demás de esa matriz también guardamos información necesaria para poder entrenarlos y clasificarlos correctamente. Por ejemplo, guardamos si los atributos son continuos o discretos.</p>\n",
    "<p>A continuación, vamos a mostrar una ejecución para cada uno de los conjuntos de datos que nos dan para hacer Naive-Bayes. En la celda de abajo vereis la ejecución.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Age', 'Spectacle', 'Astigmatic', 'Tear', 'Class']\n",
      "[{'1': 0, '2': 1, '3': 2}, {'1': 0, '2': 1}, {'1': 0, '2': 1}, {'1': 0, '2': 1}, {'1': 0, '2': 1, '3': 2}]\n",
      "['TLeftSq', 'TMidSq', 'TRightSq', 'MLeftSq', 'MMidSq', 'MRightSq', 'BLeftSq', 'BMidSq', 'BRightSq', 'Class']\n",
      "[{'b': 0, 'o': 1, 'x': 2}, {'b': 0, 'o': 1, 'x': 2}, {'b': 0, 'o': 1, 'x': 2}, {'b': 0, 'o': 1, 'x': 2}, {'b': 0, 'o': 1, 'x': 2}, {'b': 0, 'o': 1, 'x': 2}, {'b': 0, 'o': 1, 'x': 2}, {'b': 0, 'o': 1, 'x': 2}, {'b': 0, 'o': 1, 'x': 2}, {'negative': 0, 'positive': 1}]\n",
      "['A1', 'A2', 'A3', 'A4', 'A5', 'A6', 'A7', 'A8', 'A9', 'A10', 'A11', 'A12', 'A13', 'A14', 'A15', 'A16', 'A17', 'A18', 'A19', 'A20', 'Class']\n",
      "[{'A11': 0, 'A12': 1, 'A13': 2, 'A14': 3}, {}, {'A30': 0, 'A31': 1, 'A32': 2, 'A33': 3, 'A34': 4}, {'A40': 0, 'A41': 1, 'A410': 2, 'A42': 3, 'A43': 4, 'A44': 5, 'A45': 6, 'A46': 7, 'A48': 8, 'A49': 9}, {}, {'A61': 0, 'A62': 1, 'A63': 2, 'A64': 3, 'A65': 4}, {'A71': 0, 'A72': 1, 'A73': 2, 'A74': 3, 'A75': 4}, {}, {'A91': 0, 'A92': 1, 'A93': 2, 'A94': 3}, {'A101': 0, 'A102': 1, 'A103': 2}, {}, {'A121': 0, 'A122': 1, 'A123': 2, 'A124': 3}, {}, {'A141': 0, 'A142': 1, 'A143': 2}, {'A151': 0, 'A152': 1, 'A153': 2}, {}, {'A171': 0, 'A172': 1, 'A173': 2, 'A174': 3}, {}, {'A191': 0, 'A192': 1}, {'A201': 0, 'A202': 1}, {'1': 0, '2': 1}]\n",
      "==============MATRIZ NUMPY DEL CONJUNTO DE DATOS LENSES=====================\n",
      "[[0. 0. 0. 0. 2.]\n",
      " [0. 0. 0. 1. 1.]\n",
      " [0. 0. 1. 0. 2.]\n",
      " [0. 0. 1. 1. 0.]\n",
      " [0. 1. 0. 0. 2.]\n",
      " [0. 1. 0. 1. 1.]\n",
      " [0. 1. 1. 0. 2.]\n",
      " [0. 1. 1. 1. 0.]\n",
      " [1. 0. 0. 0. 2.]\n",
      " [1. 0. 0. 1. 1.]\n",
      " [1. 0. 1. 0. 2.]\n",
      " [1. 0. 1. 1. 0.]\n",
      " [1. 1. 0. 0. 2.]\n",
      " [1. 1. 0. 1. 1.]\n",
      " [1. 1. 1. 0. 2.]\n",
      " [1. 1. 1. 1. 2.]\n",
      " [2. 0. 0. 0. 2.]\n",
      " [2. 0. 0. 1. 2.]\n",
      " [2. 0. 1. 0. 2.]\n",
      " [2. 0. 1. 1. 0.]\n",
      " [2. 1. 0. 0. 2.]\n",
      " [2. 1. 0. 1. 1.]\n",
      " [2. 1. 1. 0. 2.]\n",
      " [2. 1. 1. 1. 2.]]\n",
      "============================================================================\n",
      "==============MATRIZ NUMPY DEL CONJUNTO DE DATOS TIC-TAC-TOE================\n",
      "[[2. 2. 2. ... 1. 1. 1.]\n",
      " [2. 2. 2. ... 2. 1. 1.]\n",
      " [2. 2. 2. ... 1. 2. 1.]\n",
      " ...\n",
      " [1. 2. 1. ... 1. 2. 0.]\n",
      " [1. 2. 1. ... 1. 2. 0.]\n",
      " [1. 1. 2. ... 2. 2. 0.]]\n",
      "============================================================================\n",
      "==============MATRIZ NUMPY DEL CONJUNTO DE DATOS GERMAN=====================\n",
      "[[ 0.  6.  4. ...  1.  0.  0.]\n",
      " [ 1. 48.  2. ...  0.  0.  1.]\n",
      " [ 3. 12.  4. ...  0.  0.  0.]\n",
      " ...\n",
      " [ 3. 12.  2. ...  0.  0.  0.]\n",
      " [ 0. 45.  2. ...  1.  0.  1.]\n",
      " [ 1. 45.  4. ...  0.  0.  0.]]\n",
      "============================================================================\n"
     ]
    }
   ],
   "source": [
    "dataset = Datos('../Datasets/lenses.data')\n",
    "dataset2 = Datos('../Datasets/tic-tac-toe.data')\n",
    "dataset3 = Datos('../Datasets/german.data')\n",
    "print(\"==============MATRIZ NUMPY DEL CONJUNTO DE DATOS LENSES=====================\")\n",
    "print(dataset.datos)\n",
    "print(\"============================================================================\")\n",
    "print(\"==============MATRIZ NUMPY DEL CONJUNTO DE DATOS TIC-TAC-TOE================\")\n",
    "print(dataset2.datos)\n",
    "print(\"============================================================================\")\n",
    "print(\"==============MATRIZ NUMPY DEL CONJUNTO DE DATOS GERMAN=====================\")\n",
    "print(dataset3.datos)\n",
    "print(\"============================================================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Apartado 1: Estrategia de Particionado</h3>\n",
    "<p>En este apartado vamos a probar las dos estrategias de particionado de los datos que hemos tenido que implementar en esta práctica, las cuales son:</p>\n",
    "    <ol>\n",
    "        <p>- Validación Simple.</p>\n",
    "        <p>- Validación Cruzada.</p>\n",
    "    </ol>\n",
    "<p>Nuestra estrategia de <strong>validación simple</strong> consiste en meterle un porcentaje por el cual queremos dividir el conjunto de datos en dos subconjuntos de datos, donde uno lo vamos a utilizar para entrenar y el otro lo vamos a utilizar para hacer la predicción con nuestro clasificador. En la celda de abajo mostraremos el código necesario para poder realizar correctamente la validacion simple.</p>\n",
    "<p>Como podemos observar en el código de abajo de validación simple, lo que hacemos es que ponemos una semilla a random y decimos que el numero de particiones va a ser uno. A continuación, haremos un permutacion de numeros aleatorios entre el 0  y el número de datos que hay en el fichero. Por ultimo, lo que hacemos es que le creamos la partición que va a tener en su interior los dos subconjuntos de Train y Test. En esa permutación lo multiplicamos por el porcentaje que le hemos dado nosotros para crear los dos suboconjuntos.</p>\n",
    "<p>Debajo de esta celda vamos a comprobar en como funciona  la validación simple con diferentes porcentajes para obtener el subconjunto de datos de Train y Test, en los diferentes conjuntos de datos </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Particion():\n",
    "\n",
    "  # Esta clase mantiene la lista de �ndices de Train y Test para cada partici�n del conjunto de particiones\n",
    "  def __init__(self, train=[], test=[]):\n",
    "    self.indicesTrain = train\n",
    "    self.indicesTest = test\n",
    "\n",
    "  def __str__(self):\n",
    "    return \"Train: {}\\nTest:  {}\".format(str(self.indicesTrain), str(self.indicesTest))\n",
    "\n",
    "class EstrategiaParticionado:\n",
    "  # Clase abstracta\n",
    "  __metaclass__ = ABCMeta\n",
    "\n",
    "  # Lista de las particiones\n",
    "  def __init__(self, nombre=\"\"):\n",
    "    self.nombreEstrategia = nombre\n",
    "    self.numeroParticiones = 0\n",
    "    self.particiones = []\n",
    "\n",
    "  # Atributos: deben rellenarse adecuadamente para cada estrategia concreta: nombreEstrategia, numeroParticiones, listaParticiones. Se pasan en el constructor\n",
    "\n",
    "  @abstractmethod\n",
    "  # TODO: esta funcion deben ser implementadas en cada estrategia concreta\n",
    "  def creaParticiones(self, datos, seed=None):\n",
    "    pass\n",
    "\n",
    "class ValidacionSimple(EstrategiaParticionado):\n",
    "\n",
    "  def __init__(self, porcentaje):\n",
    "    self.porcentaje = porcentaje\n",
    "    super().__init__(\"Validacion simple\")\n",
    "\n",
    "  # Crea particiones segun el metodo tradicional de division de los datos segun el porcentaje deseado.\n",
    "  # Devuelve una lista de particiones (clase Particion)\n",
    "  # TODO: implementar\n",
    "  def creaParticiones(self, datos, seed=None):\n",
    "\n",
    "    np.random.seed(seed)\n",
    "    self.numeroParticiones = 1\n",
    "\n",
    "    # Generamos una lista con todos los números de datos aleatorios\n",
    "    indicesAleatorios = np.random.permutation(int(datos.numDatos))\n",
    "\n",
    "    # Creamos la particion, en funcion del porcentaje especificado\n",
    "    self.particiones = [Particion(indicesAleatorios[:int(datos.numDatos * self.porcentaje)],\n",
    "                                  indicesAleatorios[int(datos.numDatos * self.porcentaje):])]\n",
    "\n",
    "    return self.particiones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============VALIDACIÓN SIMPLE CON LENSES DATA 70%======================\n",
      "Train: [ 3  0  9 23 21  4 10  6 11 13  8 17 14  1 22 20]\n",
      "Test:  [16  7 12  2 18  5 15 19]\n",
      "=========================================================================\n",
      "==============VALIDACIÓN SIMPLE CON TIC-TAC-TOE DATA 80%=================\n",
      "Train: [929  18 772 957 107 393 323 867 763 443 503  32 804 603 109 828 750 147\n",
      " 442  33 381  13 229 457 552 557  69 303 454 398 436 166 285   2 295 605\n",
      " 344 920 169 210 659 440 576 139 712 545 445 905 741 821 827 207 111 129\n",
      " 495 433 376 273 815 126 224 608 908 667 756  52 296 548 331 184 149 396\n",
      "  60 641 842 466 415 910 671  61 843 743 197 242 182 661 554 424 322 688\n",
      " 761 274 535 214  54 318 131 232  41 609 725  24 730  46 681 762 279 482\n",
      "   0 651 140 868 211 379  82 838 145 215 700 218 135 357 736 653 846 595\n",
      " 600 585 807 572 735 921 470 231  77 309  26 581 745 356 464  89 348 112\n",
      " 226 350   5 277 187 638 458 148  45 955 267 592 930 160 555 740 372 243\n",
      " 161  72 387 254 333 724 791 720 690 395 562 392 947 680 676 626 246 198\n",
      " 787 263 954  92 672 748 186 863 894 347 917 121 501 369 291 404 771 313\n",
      " 170 324 239 620 425 943 695 874 801 643 430 130 752 282 300 865 363 656\n",
      " 508 616 611 524 286 895  99 500 778  38 891 689 280 123 511 751 635 675\n",
      " 474 775 402 337 563 781 590 627 307 351 543 629 271 814 120 294 813 272\n",
      " 456 589 925 162 480 245 408 472 106 795 321 179  35 835 684 658 610  84\n",
      " 888  14 261 515 315 899 665 870 733 774 708 892  85 301 854 155 165 845\n",
      " 467 314 391 718  40 898 889 199 913 504 468 288 537  78 328 840 698  57\n",
      " 869 465  98 116 278 269 818 573  47 119 779 158 196 713 250 306 429 793\n",
      " 558 900 100 237 816 191  23 265 386 696 290 569 646 915 134 183 235  90\n",
      " 722 798 332 950   3 204 189 851 601 949 361 556 334 844  79 935 654 421\n",
      " 150 251 927  81 912 221 883 360 228 660 236 455 481 847 693 206 682 709\n",
      " 879 522 330 806 822 783 362  56 419 195  87 171  96 902 293 637 327 631\n",
      " 491 655 411 873 826 928  67  88 878 497 520 484 365 476  80 764 619 644\n",
      " 699 632 906 507 108 848 343 956 786   8 770 538 175 897 320 420 385   4\n",
      " 686 768 407 953 650 146 122 469 780 922 810 412 574 578 311 887 834 852\n",
      " 432 861 422 782 553 297  73 192 705 876 561 647 354 438 209 880 759   1\n",
      " 527 776 230 824 819 941 803 164 167 152 505 875 382 536 287 539   9 673\n",
      " 746 951 531 742 238 613  12 157  22 789 773 639 716 836 886 477 808  64\n",
      " 939 423 737 523 952 599 679 732 532 805 703 542 945 492 859  97 739 452\n",
      " 871 359 176 449 866 190 410 794 614  65 373 512 144 378 744 830 881 125\n",
      " 364 841 319 185 919 903 253 790  39  93 942 124 142 453 101 622 541 837\n",
      " 596  68 105 416 584 521 575 414 212  29 325  86 471  37 479 664 180 194\n",
      " 544 234 707 931 645 546 248 909 463 606 283 777 685 193 529 864  42  28\n",
      " 560  10 704 310 441 862 715 516 490 747  51  71 568 355 389 138 202 893\n",
      " 528 617 219 460 694 598 731 839 298 757 401 380 397 636 624 934   6 706\n",
      " 940 168 173 904 201 405 431  34 326 723  16 633 252 570  20 113 260 399\n",
      " 630 163 502 550 666 565 143 437 749 374 446 621 493 345 702 388 118 390\n",
      " 662 670 760 222 200 825 711  49 510 335 802 358 462 275 640 849  66  76\n",
      " 714  75   7  55 270 547 178  70 444 498 823 669 587 678 256 177 923 233\n",
      " 625 525 104 268 583 615 494 244 488 607 188 259 353 258 340  30 932 223\n",
      " 850 338 946  50 855 281 907 797 514 220 349 820  94 877 766 489 217  62\n",
      " 485 513 241 448 133 156 257 136 154  91 758 924 727 486 519 342 384 533\n",
      " 426 559 276 413 765 264 530 367  74 172]\n",
      "Test:  [102 687 289  58 582  21  19 642 890 853 418 115 341 114 227 292 262 406\n",
      " 409 811  11 918 683 649 809 858 602 896 461 549 832 571 785 674 594 857\n",
      " 174 475 225 352 506 487 302 478 652 936 368 299 697 856 205 451 329 564\n",
      " 127 403 435 400 473 628 586 754 208 496 812  17 216 668 567 588 882 911\n",
      " 370  36 885 623 371 948 240 312 110 817 710 799 159 375  15 383  53 944\n",
      " 738 377 366 336 800 447 580 566 213 734 593 938 317 417 483 579 266 132\n",
      " 153  44 551 755 792 103  63 901 831 526 914 717 450 796 833 284 304 937\n",
      " 203  43 657 181 634 247 509  25 788 829 753 691 499 305 540 784 648 677\n",
      " 427 604 769 534  27 434 618 612 591 141 459 701 767 597 729 916 872 728\n",
      " 933  95 316 255 428  59 518 884 151 663 692  48 860 346  83 719 926 339\n",
      " 439 137  31 128 117 726 308 517 721 394 577 249]\n",
      "=========================================================================\n",
      "==============VALIDACIÓN SIMPLE CON GERMAN DATA 75%======================\n",
      "Train: [723 409   0 837  30 647 451 272 503 535  96 473  40  89 924 651 122  97\n",
      " 817 147 291 370 933 263 707 335 757 551 343 328 252 957  55 225 717 472\n",
      " 157 609 295 665 363 533 502  87 500 250 820 929 205 812 127 622 155 798\n",
      " 548 936 780 542 265  50 113 204 249 442 597 804 615 954 566  98 842 741\n",
      " 568 596 284 453 827 307 818 961 185 975 226 926 326 286 733 196 695 884\n",
      " 216 379 236 576 999 150 659 332 386 231  99  17 129 321 696 607 438 583\n",
      " 445 700 725 195 345 722  93 572 859 654 752 840 808 159 203 413 887 180\n",
      " 996 660 939 841  73 993 264 708 888 474 257 513 450 511 505 385 248 756\n",
      " 153 588 339 510 175 304 244 891 188 297 896 424 499 633 123 258 206 819\n",
      " 713  34 709 111 177 638 614 526 349  48 971 189 301 434 940 190 347 452\n",
      " 418 336  92 496 737  57 519 973 130 479 492 361 811 907  53 613 480 718\n",
      " 467 691  16 785 834 449 132  43  33 697 275 770 762 963 217  81 854 792\n",
      "  64 310 549   4 663 732 306 356 501 968 485 793 462 191 612 516 667  22\n",
      " 574   3 919 577 838 383 714  20 531 992  79 911 415 670 917 857 931 251\n",
      " 584 560 142 702 677 484 903  47 923 136  78 959 958 507 674 575 833 536\n",
      " 324 706  90 679 486 764 353  94 179 610 997 183 460 238 355 739 316 683\n",
      " 883 746 259 344  82 589 214 318 350 649 246 527 517 593  69 750 956 558\n",
      "   1 506 966 312 982 327 476 640 962 546 740 809  36 678 786 116 897 899\n",
      " 644 880 512  80 211 170 119 352 624 742 807 562 133 309 580 296 744 285\n",
      " 299 648  85  62  67 430 138 384 242 763 215 458 561 983 148 704 641 806\n",
      " 377 685 882  70 366 282 320 182 846 795 920 796 712 927 781 816 987 759\n",
      " 976 280 465 984 520  31 410 892 870 315 711 753 810 747 894 253 689 162\n",
      " 608 137 187 397 311  49 432 207 822 877 581  83 565  24 788 193 420 421\n",
      " 387  14 364 313 797 599 490 573 487 603 276 104 489 851 867 617 279 192\n",
      " 429 915 611 459 672  66 949  29 905 688  39 288 653 847 223 317 518 682\n",
      " 221 972 454 107 477 373 904 692 828 470 468 735 348  54 824 346 716 631\n",
      " 382 481 423 545 173 776 650   2 290 567 850 671 144  68 564 463 690 112\n",
      " 601 874  38 950 627 784 878  58 163 240 200 504 881 414 626 979 866 525\n",
      " 417 406 758 448 635 974 604 830 256 109 655 571 871 995 783   6 668 636\n",
      " 895  26 466 906 675 161 128 403 947 912 482 362 729 446 412 224 100 946\n",
      " 554 120 184 676 178  15 825 990 843 338 623 955 303  56 921  13 832 381\n",
      " 233 181  75 394 169 864 440 853 289 330 616 591 400 134 844 988 529 902\n",
      " 938 325 392 941 202 483 782 358 918 340 209 680   7 666 380 943 656 994\n",
      " 582 698  27 118 471 738 277 427 376 398 587 794  86 212 813 165 839 856\n",
      " 800 208  60 985 497 230   8 305 908 245 710 789 620 778 634 388 727 269\n",
      " 435 821 334 720 693 390 730 314 799 673 928 909 768 308 294 395 642  95\n",
      "  42 271 701 375 234 590  23 160 106 773 886 475 457 775 865 969 699 630\n",
      " 152 815 101 831 563 998 814  52 357 365 524 105 145 632  65 378 199 606\n",
      " 970 925 731 578 848 726 222 422 528 791 135 369 862 530 736 293 875 168\n",
      " 585 964 523 900 227 461 131 937 283 686 743 108 266 140 532 637 114 167\n",
      " 274 172  21 391 790 232 210 826 124 721 728 922 893 559 538  37 719 967\n",
      " 978 805 901 139 876 342 171 354 628 331  28 319]\n",
      "Test:  [ 19 439 767 556 166 237 802 553  77 745 540 779 652 508 777 639 278 368\n",
      " 748 441 550 868   9 156 186 235 703  59 302 687 444 760  12 852 625 803\n",
      " 374 845 197 910 260 539 952 835  91  10  41 464  45 787 158 772 478 586\n",
      " 198 300 393 579 766 351 146 509 829 273  76 754 951  35 960 411 948 447\n",
      " 989  25 547 389 469 298 643 715 396 774 869 889 629 986  84 769 913 287\n",
      " 103 621 498  63 102 493 944 861 247 863 456  32 934 694 761 981 115 228\n",
      " 595 219 213 914 890 126 514 154 557  88  74 953 341 916  44 433 488 176\n",
      " 201 646 855  61 930 494 645 292 860 151 399 121 254 662 705 367 401 149\n",
      " 823 270 322 491 945 661 755 657 618 268 836 404 143 977 602  72 416 141\n",
      " 125 664 437 220 261  18 323 255 681 849 873 218 991 239  51 262 174 455\n",
      " 592 534 267 426 360 965 425 419 194 408 337 407 541 164 117 543 872 801\n",
      " 658 333 537 281 544 935 428 751 879 521 598 570 765 771  46 600 243 885\n",
      " 594 555 229 858 241 515 522 619 110 371 495 898 405 669 734 436 359 942\n",
      " 980 932  11 569   5 605 749 724 372 684 443 431 552 402 329  71]\n",
      "=========================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"==============VALIDACIÓN SIMPLE CON LENSES DATA 70%======================\")\n",
    "estrategia = ValidacionSimple(0.7)\n",
    "estrategia.creaParticiones(dataset)\n",
    "print(estrategia.particiones[0])\n",
    "print(\"=========================================================================\")\n",
    "print(\"==============VALIDACIÓN SIMPLE CON TIC-TAC-TOE DATA 80%=================\")\n",
    "estrategia2 = ValidacionSimple(0.8)\n",
    "estrategia2.creaParticiones(dataset2)\n",
    "print(estrategia2.particiones[0])\n",
    "print(\"=========================================================================\")\n",
    "print(\"==============VALIDACIÓN SIMPLE CON GERMAN DATA 75%======================\")\n",
    "estrategia3 = ValidacionSimple(0.75)\n",
    "estrategia3.creaParticiones(dataset3)\n",
    "print(estrategia3.particiones[0])\n",
    "print(\"=========================================================================\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Nuestra estrategia de <strong>validación cruzada</strong> consiste en dividir nuestro conjunto de datos en particiones de Train y Test como en validación simple, pero este proceso lo vamos a hacer K veces, para que todos los datos esten presenten en los dos subconjunto de datos para poder obtener una mejora a la hora de entrenar y clasificar.A continuación, mostraremos nuestra implementación de la validación cruzada; donde vamos a hacer K veces las divisiones del conunto de datos y si por algún motivo nuestra división de todos los subconjuntos no es perfecta balancearemos los datos sobrantes para poder entrenarlos y clasificarlos.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValidacionCruzada(EstrategiaParticionado):\n",
    "\n",
    "  # Crea particiones segun el metodo de validacion cruzada.\n",
    "  # El conjunto de entrenamiento se crea con las nfolds-1 particiones y el de test con la particion restante\n",
    "  # Esta funcion devuelve una lista de particiones (clase Particion)\n",
    "  # TODO: implementar\n",
    "\n",
    "  def __init__(self, k):\n",
    "    self.k = k\n",
    "    super().__init__(\"Validacion cruzada\")\n",
    "\n",
    "  def creaParticiones(self, datos, seed=None):\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    self.numeroParticiones = self.k\n",
    "\n",
    "    # Generamos una lista con todos los números de datos aleatorios\n",
    "    indicesAleatorios = np.random.permutation(int(datos.numDatos))\n",
    "\n",
    "    # Hallamos el tamaño de cada bloque\n",
    "    tamBloque = int(datos.numDatos / self.k)\n",
    "\n",
    "    datosSobran = datos.numDatos - (tamBloque * self.k)\n",
    "    count = 0\n",
    "    for i in range(self.k):\n",
    "\n",
    "      train = np.delete(indicesAleatorios, range(i * tamBloque, (i + 1) * tamBloque))\n",
    "      test = indicesAleatorios[i * tamBloque:(i + 1) * tamBloque]\n",
    "\n",
    "      # Caso en el que la cuenta es justa\n",
    "      if datosSobran == 0:\n",
    "        self.particiones.append(Particion(train, test))\n",
    "\n",
    "      # Contemplamos el caso de que la division para sacar el numero de subconjuntos no fuese entera\n",
    "      if datosSobran > 0:\n",
    "        count += 1\n",
    "        particionTest = np.append(test, train[(datos.numDatos - tamBloque) - i - 1])\n",
    "        particionTrain = np.delete(train, (datos.numDatos - tamBloque) - i - 1)\n",
    "        datosSobran -= 1\n",
    "        self.particiones.append(Particion(particionTrain, particionTest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>A continuación, mostraremos la ejecución de nuestra estrategia de partcionado validación simple o también llamada K-fold, con los diferentes conjuntos de datos y con diferentes K's. Vamos poder observar en la salida de nuestra celda que todos los valores van a estar una vez en la partición de Test.</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============VALIDACIÓN CRUZADA CON LENSES DATA K=5======================\n",
      "Train: [ 4 18  2 10 12 17  1  3 19 22 15  9 13  7 20 11 14  8 16]\n",
      "Test:  [21  0  6 23  5]\n",
      "Train: [21  0  6 23 12 17  1  3 19 22 15  9 13  7 20 11 14  8  5]\n",
      "Test:  [ 4 18  2 10 16]\n",
      "Train: [21  0  6 23  4 18  2 10 19 22 15  9 13  7 20 11 14 16  5]\n",
      "Test:  [12 17  1  3  8]\n",
      "Train: [21  0  6 23  4 18  2 10 12 17  1  3 13  7 20 11  8 16  5]\n",
      "Test:  [19 22 15  9 14]\n",
      "Train: [21  0  6 23  4 18  2 10 12 17  1  3 19 22 15  9 14  8 16  5]\n",
      "Test:  [13  7 20 11]\n",
      "==========================================================================\n",
      "==============VALIDACIÓN CRUZADA CON TIC-TAC-TOE DATA K=8=================\n",
      "Train: [132 488 938 140 910 888 431 770 603 837 118 710 605 931 627 261 730 157\n",
      " 560 722 242  28 816 460 684 893 491 614 448  59 813 393 894 882  82 185\n",
      " 511  73 555 745 534 619 142 646  41 581 957  17 558 514 858 337 110 709\n",
      " 257 926 480 415 473 589 219 724 494 287 592  75 795 826 123 101 384 908\n",
      " 411 553 723 557  40 155 935 392  27 818  37  12 364 238 228 610 896 530\n",
      " 639  36 704 674 153 812 517 676 432 344 267 336 609 349 380 454 911 653\n",
      " 345 295 600 736  10 504 482 310 106  67 423 842 593 241 459 912 929 430\n",
      " 290 309 732 853 934 370 373 183 543 834 666 129 612 915 872 456  23 259\n",
      " 740 633 308 906 203 417 468 715 752 374 114 758 830 819 243  97 855 941\n",
      " 561  79 263 195 594 224 895 851 266 866 713 331 656 932 914 754 429 539\n",
      "  61 642 117  92 161 404 917 638  76 125 522 802 426 223 922 892 154 210\n",
      " 650  74 208  19 173 768 474 737 281 422 181 519 586 315 230 630 400 467\n",
      " 660 121 873 394 441 779  45 104 524 418 705 442 766 797 279 821 807 124\n",
      " 325 942 839 268 449 179 362 275 576 503 843  14 606 447  81  33 793 927\n",
      " 365 209 306 703 848 218 377 356 617 406 611 664   1 874 285 545 166 171\n",
      " 338 408 450 862  63 902   0 342 398 777 568 270 877 648 192  72 347 227\n",
      "  99  85 712  42  66 277 343 178 632 167 518 564 440 925  43 180 744 346\n",
      " 773 320  55 898 201 532  62  44 501 580 421 885 445 823 591 149 951 333\n",
      " 628 376 850 572 695 247   7 174 128 810 637 759 903 311 300 652 686 462\n",
      "  94 383 280 667 655 801 107 647 662 206 385 717 217  65 293 461 368  98\n",
      " 329  20 177 437  89 233 840  95 571 186 833  11 547 623 691 184 492 122\n",
      " 497 936 739 584 947 222 565 741 316  29 901 846 836 930 815 175  24  93\n",
      " 954 475 369 481 340 297 327 794 672 526 680 334 470 825 687 464 323 143\n",
      " 678 120 904 764 103 950 498 523  38 196  71 831 861 213 649 355 286 643\n",
      " 899 274 502 479  21 434 945 785 483 946 750 312 782  16 251 112 784 292\n",
      " 108 725 820 640 271 328  48 528 301 755 563 515 670 389  77 100 413 747\n",
      " 489 215 538 868 863 602 693 513 757 469 887 844 559   5 828   4 711 731\n",
      " 139  60 221 330 296  30 318  13 920 102 214 720 679 552 607 205 551 585\n",
      " 738 350 596 841 134 284 244  49 476 273  26 829 956 232 147 401 420  56\n",
      "  52 288 156 234 357 127 150 654 395 391 353 824 669 884 250 322 471 294\n",
      " 570 806  50 587 407 618 778 299 326 886 948 698 897 225 682 172 651 424\n",
      " 169 663  87 435 566 144 569 688 644 305 814  25 889 845 191 760 645 443\n",
      " 141 119 786 402 890 535 706 204 865 163 780  46 943  86 307  22  83 692\n",
      " 550 226 787 507 827  18 136 933 412 446 466 615 542 115 701 661 880 269\n",
      " 554 762 694 748 620 168 928 145 937 427 258 416 399 352 742 939 588 379\n",
      " 567 188 486 856 487 133 838 358 923 197 237 556 811 202 508 182 282 537\n",
      " 822 598 371 506 726 455  57 671  39 808 626 719 541 472  68 771 953 900\n",
      " 319 419  90 878 864 397 574 721 756 113 246 414 405 578 727 668 520 579\n",
      " 697 707 444 527 160 871 255 151 264 549 510 189 152 700 577 436 734 410\n",
      " 396 207 193  58 249 378 496 949 248 262 749 735 876   6 879 131  53 690\n",
      "   3 516 624 216 891 629 382 763 265 875 239  31 403 272  64 867 359 940\n",
      " 478 425 919 728 582 849 463 477  70 260 635 765 729 135 452 525 601 798\n",
      " 521 776 235 769  78 673 767 716 366  88 783 817 761 805 677 146 847 883\n",
      " 595 465 870 321 675   2 790 641 387 909 604 165 137 116 548 276 253 636\n",
      " 211 499 354  54  51 860 314 439 428 622 529 505 245 743 540 162 803 781\n",
      " 339  96 212 835 198 613 372 774 381  80 130 164 699 303 485 657 751 944\n",
      " 509 458 109 952 924 138 905  34   9 313]\n",
      "Test:  [575 634 512 231   8 199 809 495 298 869  91 881 859 348 457 256 170 916\n",
      " 562 283 291 360 857  84 714 390 800 304 616 702 658 194 733 375 921 105\n",
      " 252 789 500 536 685 689 918 111 200 746 608 278 386 854 317 696 490 453\n",
      "  32 832 484 718 621 804 583 659 229 796 335  35  15 631 590 363 324 176\n",
      " 531  69 708 791 438 544 799 681 240 351 573 451 433 493  47 852 289 599\n",
      " 546 913 775 302 190 254 597 148 409 772 332 625 126 683 907 367 220 665\n",
      " 158 753 236 955 792 159 341 533 388 361 788 187]\n",
      "Train: [575 634 512 231   8 199 809 495 298 869  91 881 859 348 457 256 170 916\n",
      " 562 283 291 360 857  84 714 390 800 304 616 702 658 194 733 375 921 105\n",
      " 252 789 500 536 685 689 918 111 200 746 608 278 386 854 317 696 490 453\n",
      "  32 832 484 718 621 804 583 659 229 796 335  35  15 631 590 363 324 176\n",
      " 531  69 708 791 438 544 799 681 240 351 573 451 433 493  47 852 289 599\n",
      " 546 913 775 302 190 254 597 148 409 772 332 625 126 683 907 367 220 665\n",
      " 158 753 236 955 792 159 341 533 388 361 788 842 593 241 459 912 929 430\n",
      " 290 309 732 853 934 370 373 183 543 834 666 129 612 915 872 456  23 259\n",
      " 740 633 308 906 203 417 468 715 752 374 114 758 830 819 243  97 855 941\n",
      " 561  79 263 195 594 224 895 851 266 866 713 331 656 932 914 754 429 539\n",
      "  61 642 117  92 161 404 917 638  76 125 522 802 426 223 922 892 154 210\n",
      " 650  74 208  19 173 768 474 737 281 422 181 519 586 315 230 630 400 467\n",
      " 660 121 873 394 441 779  45 104 524 418 705 442 766 797 279 821 807 124\n",
      " 325 942 839 268 449 179 362 275 576 503 843  14 606 447  81  33 793 927\n",
      " 365 209 306 703 848 218 377 356 617 406 611 664   1 874 285 545 166 171\n",
      " 338 408 450 862  63 902   0 342 398 777 568 270 877 648 192  72 347 227\n",
      "  99  85 712  42  66 277 343 178 632 167 518 564 440 925  43 180 744 346\n",
      " 773 320  55 898 201 532  62  44 501 580 421 885 445 823 591 149 951 333\n",
      " 628 376 850 572 695 247   7 174 128 810 637 759 903 311 300 652 686 462\n",
      "  94 383 280 667 655 801 107 647 662 206 385 717 217  65 293 461 368  98\n",
      " 329  20 177 437  89 233 840  95 571 186 833  11 547 623 691 184 492 122\n",
      " 497 936 739 584 947 222 565 741 316  29 901 846 836 930 815 175  24  93\n",
      " 954 475 369 481 340 297 327 794 672 526 680 334 470 825 687 464 323 143\n",
      " 678 120 904 764 103 950 498 523  38 196  71 831 861 213 649 355 286 643\n",
      " 899 274 502 479  21 434 945 785 483 946 750 312 782  16 251 112 784 292\n",
      " 108 725 820 640 271 328  48 528 301 755 563 515 670 389  77 100 413 747\n",
      " 489 215 538 868 863 602 693 513 757 469 887 844 559   5 828   4 711 731\n",
      " 139  60 221 330 296  30 318  13 920 102 214 720 679 552 607 205 551 585\n",
      " 738 350 596 841 134 284 244  49 476 273  26 829 956 232 147 401 420  56\n",
      "  52 288 156 234 357 127 150 654 395 391 353 824 669 884 250 322 471 294\n",
      " 570 806  50 587 407 618 778 299 326 886 948 698 897 225 682 172 651 424\n",
      " 169 663  87 435 566 144 569 688 644 305 814  25 889 845 191 760 645 443\n",
      " 141 119 786 402 890 535 706 204 865 163 780  46 943  86 307  22  83 692\n",
      " 550 226 787 507 827  18 136 933 412 446 466 615 542 115 701 661 880 269\n",
      " 554 762 694 748 620 168 928 145 937 427 258 416 399 352 742 939 588 379\n",
      " 567 188 486 856 487 133 838 358 923 197 237 556 811 202 508 182 282 537\n",
      " 822 598 371 506 726 455  57 671  39 808 626 719 541 472  68 771 953 900\n",
      " 319 419  90 878 864 397 574 721 756 113 246 414 405 578 727 668 520 579\n",
      " 697 707 444 527 160 871 255 151 264 549 510 189 152 700 577 436 734 410\n",
      " 396 207 193  58 249 378 496 949 248 262 749 735 876   6 879 131  53 690\n",
      "   3 516 624 216 891 629 382 763 265 875 239  31 403 272  64 867 359 940\n",
      " 478 425 919 728 582 849 463 477  70 260 635 765 729 135 452 525 601 798\n",
      " 521 776 235 769  78 673 767 716 366  88 783 817 761 805 677 146 847 883\n",
      " 595 465 870 321 675   2 790 641 387 909 604 165 137 116 548 276 253 636\n",
      " 211 499 354  54  51 860 314 439 428 622 529 505 245 743 540 162 803 781\n",
      " 339  96 212 835 198 613 372 774 381  80 130 164 699 303 485 657 751 944\n",
      " 509 458 109 952 924 138 905  34   9 187]\n",
      "Test:  [132 488 938 140 910 888 431 770 603 837 118 710 605 931 627 261 730 157\n",
      " 560 722 242  28 816 460 684 893 491 614 448  59 813 393 894 882  82 185\n",
      " 511  73 555 745 534 619 142 646  41 581 957  17 558 514 858 337 110 709\n",
      " 257 926 480 415 473 589 219 724 494 287 592  75 795 826 123 101 384 908\n",
      " 411 553 723 557  40 155 935 392  27 818  37  12 364 238 228 610 896 530\n",
      " 639  36 704 674 153 812 517 676 432 344 267 336 609 349 380 454 911 653\n",
      " 345 295 600 736  10 504 482 310 106  67 423 313]\n",
      "Train: [575 634 512 231   8 199 809 495 298 869  91 881 859 348 457 256 170 916\n",
      " 562 283 291 360 857  84 714 390 800 304 616 702 658 194 733 375 921 105\n",
      " 252 789 500 536 685 689 918 111 200 746 608 278 386 854 317 696 490 453\n",
      "  32 832 484 718 621 804 583 659 229 796 335  35  15 631 590 363 324 176\n",
      " 531  69 708 791 438 544 799 681 240 351 573 451 433 493  47 852 289 599\n",
      " 546 913 775 302 190 254 597 148 409 772 332 625 126 683 907 367 220 665\n",
      " 158 753 236 955 792 159 341 533 388 361 788 132 488 938 140 910 888 431\n",
      " 770 603 837 118 710 605 931 627 261 730 157 560 722 242  28 816 460 684\n",
      " 893 491 614 448  59 813 393 894 882  82 185 511  73 555 745 534 619 142\n",
      " 646  41 581 957  17 558 514 858 337 110 709 257 926 480 415 473 589 219\n",
      " 724 494 287 592  75 795 826 123 101 384 908 411 553 723 557  40 155 935\n",
      " 392  27 818  37  12 364 238 228 610 896 530 639  36 704 674 153 812 517\n",
      " 676 432 344 267 336 609 349 380 454 911 653 345 295 600 736  10 504 482\n",
      " 310 106  67 423 449 179 362 275 576 503 843  14 606 447  81  33 793 927\n",
      " 365 209 306 703 848 218 377 356 617 406 611 664   1 874 285 545 166 171\n",
      " 338 408 450 862  63 902   0 342 398 777 568 270 877 648 192  72 347 227\n",
      "  99  85 712  42  66 277 343 178 632 167 518 564 440 925  43 180 744 346\n",
      " 773 320  55 898 201 532  62  44 501 580 421 885 445 823 591 149 951 333\n",
      " 628 376 850 572 695 247   7 174 128 810 637 759 903 311 300 652 686 462\n",
      "  94 383 280 667 655 801 107 647 662 206 385 717 217  65 293 461 368  98\n",
      " 329  20 177 437  89 233 840  95 571 186 833  11 547 623 691 184 492 122\n",
      " 497 936 739 584 947 222 565 741 316  29 901 846 836 930 815 175  24  93\n",
      " 954 475 369 481 340 297 327 794 672 526 680 334 470 825 687 464 323 143\n",
      " 678 120 904 764 103 950 498 523  38 196  71 831 861 213 649 355 286 643\n",
      " 899 274 502 479  21 434 945 785 483 946 750 312 782  16 251 112 784 292\n",
      " 108 725 820 640 271 328  48 528 301 755 563 515 670 389  77 100 413 747\n",
      " 489 215 538 868 863 602 693 513 757 469 887 844 559   5 828   4 711 731\n",
      " 139  60 221 330 296  30 318  13 920 102 214 720 679 552 607 205 551 585\n",
      " 738 350 596 841 134 284 244  49 476 273  26 829 956 232 147 401 420  56\n",
      "  52 288 156 234 357 127 150 654 395 391 353 824 669 884 250 322 471 294\n",
      " 570 806  50 587 407 618 778 299 326 886 948 698 897 225 682 172 651 424\n",
      " 169 663  87 435 566 144 569 688 644 305 814  25 889 845 191 760 645 443\n",
      " 141 119 786 402 890 535 706 204 865 163 780  46 943  86 307  22  83 692\n",
      " 550 226 787 507 827  18 136 933 412 446 466 615 542 115 701 661 880 269\n",
      " 554 762 694 748 620 168 928 145 937 427 258 416 399 352 742 939 588 379\n",
      " 567 188 486 856 487 133 838 358 923 197 237 556 811 202 508 182 282 537\n",
      " 822 598 371 506 726 455  57 671  39 808 626 719 541 472  68 771 953 900\n",
      " 319 419  90 878 864 397 574 721 756 113 246 414 405 578 727 668 520 579\n",
      " 697 707 444 527 160 871 255 151 264 549 510 189 152 700 577 436 734 410\n",
      " 396 207 193  58 249 378 496 949 248 262 749 735 876   6 879 131  53 690\n",
      "   3 516 624 216 891 629 382 763 265 875 239  31 403 272  64 867 359 940\n",
      " 478 425 919 728 582 849 463 477  70 260 635 765 729 135 452 525 601 798\n",
      " 521 776 235 769  78 673 767 716 366  88 783 817 761 805 677 146 847 883\n",
      " 595 465 870 321 675   2 790 641 387 909 604 165 137 116 548 276 253 636\n",
      " 211 499 354  54  51 860 314 439 428 622 529 505 245 743 540 162 803 781\n",
      " 339  96 212 835 198 613 372 774 381  80 130 164 699 303 485 657 751 944\n",
      " 509 458 109 952 924 138 905  34 313 187]\n",
      "Test:  [842 593 241 459 912 929 430 290 309 732 853 934 370 373 183 543 834 666\n",
      " 129 612 915 872 456  23 259 740 633 308 906 203 417 468 715 752 374 114\n",
      " 758 830 819 243  97 855 941 561  79 263 195 594 224 895 851 266 866 713\n",
      " 331 656 932 914 754 429 539  61 642 117  92 161 404 917 638  76 125 522\n",
      " 802 426 223 922 892 154 210 650  74 208  19 173 768 474 737 281 422 181\n",
      " 519 586 315 230 630 400 467 660 121 873 394 441 779  45 104 524 418 705\n",
      " 442 766 797 279 821 807 124 325 942 839 268   9]\n",
      "Train: [575 634 512 231   8 199 809 495 298 869  91 881 859 348 457 256 170 916\n",
      " 562 283 291 360 857  84 714 390 800 304 616 702 658 194 733 375 921 105\n",
      " 252 789 500 536 685 689 918 111 200 746 608 278 386 854 317 696 490 453\n",
      "  32 832 484 718 621 804 583 659 229 796 335  35  15 631 590 363 324 176\n",
      " 531  69 708 791 438 544 799 681 240 351 573 451 433 493  47 852 289 599\n",
      " 546 913 775 302 190 254 597 148 409 772 332 625 126 683 907 367 220 665\n",
      " 158 753 236 955 792 159 341 533 388 361 788 132 488 938 140 910 888 431\n",
      " 770 603 837 118 710 605 931 627 261 730 157 560 722 242  28 816 460 684\n",
      " 893 491 614 448  59 813 393 894 882  82 185 511  73 555 745 534 619 142\n",
      " 646  41 581 957  17 558 514 858 337 110 709 257 926 480 415 473 589 219\n",
      " 724 494 287 592  75 795 826 123 101 384 908 411 553 723 557  40 155 935\n",
      " 392  27 818  37  12 364 238 228 610 896 530 639  36 704 674 153 812 517\n",
      " 676 432 344 267 336 609 349 380 454 911 653 345 295 600 736  10 504 482\n",
      " 310 106  67 423 842 593 241 459 912 929 430 290 309 732 853 934 370 373\n",
      " 183 543 834 666 129 612 915 872 456  23 259 740 633 308 906 203 417 468\n",
      " 715 752 374 114 758 830 819 243  97 855 941 561  79 263 195 594 224 895\n",
      " 851 266 866 713 331 656 932 914 754 429 539  61 642 117  92 161 404 917\n",
      " 638  76 125 522 802 426 223 922 892 154 210 650  74 208  19 173 768 474\n",
      " 737 281 422 181 519 586 315 230 630 400 467 660 121 873 394 441 779  45\n",
      " 104 524 418 705 442 766 797 279 821 807 124 325 942 839 268 461 368  98\n",
      " 329  20 177 437  89 233 840  95 571 186 833  11 547 623 691 184 492 122\n",
      " 497 936 739 584 947 222 565 741 316  29 901 846 836 930 815 175  24  93\n",
      " 954 475 369 481 340 297 327 794 672 526 680 334 470 825 687 464 323 143\n",
      " 678 120 904 764 103 950 498 523  38 196  71 831 861 213 649 355 286 643\n",
      " 899 274 502 479  21 434 945 785 483 946 750 312 782  16 251 112 784 292\n",
      " 108 725 820 640 271 328  48 528 301 755 563 515 670 389  77 100 413 747\n",
      " 489 215 538 868 863 602 693 513 757 469 887 844 559   5 828   4 711 731\n",
      " 139  60 221 330 296  30 318  13 920 102 214 720 679 552 607 205 551 585\n",
      " 738 350 596 841 134 284 244  49 476 273  26 829 956 232 147 401 420  56\n",
      "  52 288 156 234 357 127 150 654 395 391 353 824 669 884 250 322 471 294\n",
      " 570 806  50 587 407 618 778 299 326 886 948 698 897 225 682 172 651 424\n",
      " 169 663  87 435 566 144 569 688 644 305 814  25 889 845 191 760 645 443\n",
      " 141 119 786 402 890 535 706 204 865 163 780  46 943  86 307  22  83 692\n",
      " 550 226 787 507 827  18 136 933 412 446 466 615 542 115 701 661 880 269\n",
      " 554 762 694 748 620 168 928 145 937 427 258 416 399 352 742 939 588 379\n",
      " 567 188 486 856 487 133 838 358 923 197 237 556 811 202 508 182 282 537\n",
      " 822 598 371 506 726 455  57 671  39 808 626 719 541 472  68 771 953 900\n",
      " 319 419  90 878 864 397 574 721 756 113 246 414 405 578 727 668 520 579\n",
      " 697 707 444 527 160 871 255 151 264 549 510 189 152 700 577 436 734 410\n",
      " 396 207 193  58 249 378 496 949 248 262 749 735 876   6 879 131  53 690\n",
      "   3 516 624 216 891 629 382 763 265 875 239  31 403 272  64 867 359 940\n",
      " 478 425 919 728 582 849 463 477  70 260 635 765 729 135 452 525 601 798\n",
      " 521 776 235 769  78 673 767 716 366  88 783 817 761 805 677 146 847 883\n",
      " 595 465 870 321 675   2 790 641 387 909 604 165 137 116 548 276 253 636\n",
      " 211 499 354  54  51 860 314 439 428 622 529 505 245 743 540 162 803 781\n",
      " 339  96 212 835 198 613 372 774 381  80 130 164 699 303 485 657 751 944\n",
      " 509 458 109 952 924 138 905   9 313 187]\n",
      "Test:  [449 179 362 275 576 503 843  14 606 447  81  33 793 927 365 209 306 703\n",
      " 848 218 377 356 617 406 611 664   1 874 285 545 166 171 338 408 450 862\n",
      "  63 902   0 342 398 777 568 270 877 648 192  72 347 227  99  85 712  42\n",
      "  66 277 343 178 632 167 518 564 440 925  43 180 744 346 773 320  55 898\n",
      " 201 532  62  44 501 580 421 885 445 823 591 149 951 333 628 376 850 572\n",
      " 695 247   7 174 128 810 637 759 903 311 300 652 686 462  94 383 280 667\n",
      " 655 801 107 647 662 206 385 717 217  65 293  34]\n",
      "Train: [575 634 512 231   8 199 809 495 298 869  91 881 859 348 457 256 170 916\n",
      " 562 283 291 360 857  84 714 390 800 304 616 702 658 194 733 375 921 105\n",
      " 252 789 500 536 685 689 918 111 200 746 608 278 386 854 317 696 490 453\n",
      "  32 832 484 718 621 804 583 659 229 796 335  35  15 631 590 363 324 176\n",
      " 531  69 708 791 438 544 799 681 240 351 573 451 433 493  47 852 289 599\n",
      " 546 913 775 302 190 254 597 148 409 772 332 625 126 683 907 367 220 665\n",
      " 158 753 236 955 792 159 341 533 388 361 788 132 488 938 140 910 888 431\n",
      " 770 603 837 118 710 605 931 627 261 730 157 560 722 242  28 816 460 684\n",
      " 893 491 614 448  59 813 393 894 882  82 185 511  73 555 745 534 619 142\n",
      " 646  41 581 957  17 558 514 858 337 110 709 257 926 480 415 473 589 219\n",
      " 724 494 287 592  75 795 826 123 101 384 908 411 553 723 557  40 155 935\n",
      " 392  27 818  37  12 364 238 228 610 896 530 639  36 704 674 153 812 517\n",
      " 676 432 344 267 336 609 349 380 454 911 653 345 295 600 736  10 504 482\n",
      " 310 106  67 423 842 593 241 459 912 929 430 290 309 732 853 934 370 373\n",
      " 183 543 834 666 129 612 915 872 456  23 259 740 633 308 906 203 417 468\n",
      " 715 752 374 114 758 830 819 243  97 855 941 561  79 263 195 594 224 895\n",
      " 851 266 866 713 331 656 932 914 754 429 539  61 642 117  92 161 404 917\n",
      " 638  76 125 522 802 426 223 922 892 154 210 650  74 208  19 173 768 474\n",
      " 737 281 422 181 519 586 315 230 630 400 467 660 121 873 394 441 779  45\n",
      " 104 524 418 705 442 766 797 279 821 807 124 325 942 839 268 449 179 362\n",
      " 275 576 503 843  14 606 447  81  33 793 927 365 209 306 703 848 218 377\n",
      " 356 617 406 611 664   1 874 285 545 166 171 338 408 450 862  63 902   0\n",
      " 342 398 777 568 270 877 648 192  72 347 227  99  85 712  42  66 277 343\n",
      " 178 632 167 518 564 440 925  43 180 744 346 773 320  55 898 201 532  62\n",
      "  44 501 580 421 885 445 823 591 149 951 333 628 376 850 572 695 247   7\n",
      " 174 128 810 637 759 903 311 300 652 686 462  94 383 280 667 655 801 107\n",
      " 647 662 206 385 717 217  65 293 757 469 887 844 559   5 828   4 711 731\n",
      " 139  60 221 330 296  30 318  13 920 102 214 720 679 552 607 205 551 585\n",
      " 738 350 596 841 134 284 244  49 476 273  26 829 956 232 147 401 420  56\n",
      "  52 288 156 234 357 127 150 654 395 391 353 824 669 884 250 322 471 294\n",
      " 570 806  50 587 407 618 778 299 326 886 948 698 897 225 682 172 651 424\n",
      " 169 663  87 435 566 144 569 688 644 305 814  25 889 845 191 760 645 443\n",
      " 141 119 786 402 890 535 706 204 865 163 780  46 943  86 307  22  83 692\n",
      " 550 226 787 507 827  18 136 933 412 446 466 615 542 115 701 661 880 269\n",
      " 554 762 694 748 620 168 928 145 937 427 258 416 399 352 742 939 588 379\n",
      " 567 188 486 856 487 133 838 358 923 197 237 556 811 202 508 182 282 537\n",
      " 822 598 371 506 726 455  57 671  39 808 626 719 541 472  68 771 953 900\n",
      " 319 419  90 878 864 397 574 721 756 113 246 414 405 578 727 668 520 579\n",
      " 697 707 444 527 160 871 255 151 264 549 510 189 152 700 577 436 734 410\n",
      " 396 207 193  58 249 378 496 949 248 262 749 735 876   6 879 131  53 690\n",
      "   3 516 624 216 891 629 382 763 265 875 239  31 403 272  64 867 359 940\n",
      " 478 425 919 728 582 849 463 477  70 260 635 765 729 135 452 525 601 798\n",
      " 521 776 235 769  78 673 767 716 366  88 783 817 761 805 677 146 847 883\n",
      " 595 465 870 321 675   2 790 641 387 909 604 165 137 116 548 276 253 636\n",
      " 211 499 354  54  51 860 314 439 428 622 529 505 245 743 540 162 803 781\n",
      " 339  96 212 835 198 613 372 774 381  80 130 164 699 303 485 657 751 944\n",
      " 509 458 109 952 924 138  34   9 313 187]\n",
      "Test:  [461 368  98 329  20 177 437  89 233 840  95 571 186 833  11 547 623 691\n",
      " 184 492 122 497 936 739 584 947 222 565 741 316  29 901 846 836 930 815\n",
      " 175  24  93 954 475 369 481 340 297 327 794 672 526 680 334 470 825 687\n",
      " 464 323 143 678 120 904 764 103 950 498 523  38 196  71 831 861 213 649\n",
      " 355 286 643 899 274 502 479  21 434 945 785 483 946 750 312 782  16 251\n",
      " 112 784 292 108 725 820 640 271 328  48 528 301 755 563 515 670 389  77\n",
      " 100 413 747 489 215 538 868 863 602 693 513 905]\n",
      "Train: [575 634 512 231   8 199 809 495 298 869  91 881 859 348 457 256 170 916\n",
      " 562 283 291 360 857  84 714 390 800 304 616 702 658 194 733 375 921 105\n",
      " 252 789 500 536 685 689 918 111 200 746 608 278 386 854 317 696 490 453\n",
      "  32 832 484 718 621 804 583 659 229 796 335  35  15 631 590 363 324 176\n",
      " 531  69 708 791 438 544 799 681 240 351 573 451 433 493  47 852 289 599\n",
      " 546 913 775 302 190 254 597 148 409 772 332 625 126 683 907 367 220 665\n",
      " 158 753 236 955 792 159 341 533 388 361 788 132 488 938 140 910 888 431\n",
      " 770 603 837 118 710 605 931 627 261 730 157 560 722 242  28 816 460 684\n",
      " 893 491 614 448  59 813 393 894 882  82 185 511  73 555 745 534 619 142\n",
      " 646  41 581 957  17 558 514 858 337 110 709 257 926 480 415 473 589 219\n",
      " 724 494 287 592  75 795 826 123 101 384 908 411 553 723 557  40 155 935\n",
      " 392  27 818  37  12 364 238 228 610 896 530 639  36 704 674 153 812 517\n",
      " 676 432 344 267 336 609 349 380 454 911 653 345 295 600 736  10 504 482\n",
      " 310 106  67 423 842 593 241 459 912 929 430 290 309 732 853 934 370 373\n",
      " 183 543 834 666 129 612 915 872 456  23 259 740 633 308 906 203 417 468\n",
      " 715 752 374 114 758 830 819 243  97 855 941 561  79 263 195 594 224 895\n",
      " 851 266 866 713 331 656 932 914 754 429 539  61 642 117  92 161 404 917\n",
      " 638  76 125 522 802 426 223 922 892 154 210 650  74 208  19 173 768 474\n",
      " 737 281 422 181 519 586 315 230 630 400 467 660 121 873 394 441 779  45\n",
      " 104 524 418 705 442 766 797 279 821 807 124 325 942 839 268 449 179 362\n",
      " 275 576 503 843  14 606 447  81  33 793 927 365 209 306 703 848 218 377\n",
      " 356 617 406 611 664   1 874 285 545 166 171 338 408 450 862  63 902   0\n",
      " 342 398 777 568 270 877 648 192  72 347 227  99  85 712  42  66 277 343\n",
      " 178 632 167 518 564 440 925  43 180 744 346 773 320  55 898 201 532  62\n",
      "  44 501 580 421 885 445 823 591 149 951 333 628 376 850 572 695 247   7\n",
      " 174 128 810 637 759 903 311 300 652 686 462  94 383 280 667 655 801 107\n",
      " 647 662 206 385 717 217  65 293 461 368  98 329  20 177 437  89 233 840\n",
      "  95 571 186 833  11 547 623 691 184 492 122 497 936 739 584 947 222 565\n",
      " 741 316  29 901 846 836 930 815 175  24  93 954 475 369 481 340 297 327\n",
      " 794 672 526 680 334 470 825 687 464 323 143 678 120 904 764 103 950 498\n",
      " 523  38 196  71 831 861 213 649 355 286 643 899 274 502 479  21 434 945\n",
      " 785 483 946 750 312 782  16 251 112 784 292 108 725 820 640 271 328  48\n",
      " 528 301 755 563 515 670 389  77 100 413 747 489 215 538 868 863 602 693\n",
      " 513 226 787 507 827  18 136 933 412 446 466 615 542 115 701 661 880 269\n",
      " 554 762 694 748 620 168 928 145 937 427 258 416 399 352 742 939 588 379\n",
      " 567 188 486 856 487 133 838 358 923 197 237 556 811 202 508 182 282 537\n",
      " 822 598 371 506 726 455  57 671  39 808 626 719 541 472  68 771 953 900\n",
      " 319 419  90 878 864 397 574 721 756 113 246 414 405 578 727 668 520 579\n",
      " 697 707 444 527 160 871 255 151 264 549 510 189 152 700 577 436 734 410\n",
      " 396 207 193  58 249 378 496 949 248 262 749 735 876   6 879 131  53 690\n",
      "   3 516 624 216 891 629 382 763 265 875 239  31 403 272  64 867 359 940\n",
      " 478 425 919 728 582 849 463 477  70 260 635 765 729 135 452 525 601 798\n",
      " 521 776 235 769  78 673 767 716 366  88 783 817 761 805 677 146 847 883\n",
      " 595 465 870 321 675   2 790 641 387 909 604 165 137 116 548 276 253 636\n",
      " 211 499 354  54  51 860 314 439 428 622 529 505 245 743 540 162 803 781\n",
      " 339  96 212 835 198 613 372 774 381  80 130 164 699 303 485 657 751 944\n",
      " 509 458 109 952 924 905  34   9 313 187]\n",
      "Test:  [757 469 887 844 559   5 828   4 711 731 139  60 221 330 296  30 318  13\n",
      " 920 102 214 720 679 552 607 205 551 585 738 350 596 841 134 284 244  49\n",
      " 476 273  26 829 956 232 147 401 420  56  52 288 156 234 357 127 150 654\n",
      " 395 391 353 824 669 884 250 322 471 294 570 806  50 587 407 618 778 299\n",
      " 326 886 948 698 897 225 682 172 651 424 169 663  87 435 566 144 569 688\n",
      " 644 305 814  25 889 845 191 760 645 443 141 119 786 402 890 535 706 204\n",
      " 865 163 780  46 943  86 307  22  83 692 550 138]\n",
      "Train: [575 634 512 231   8 199 809 495 298 869  91 881 859 348 457 256 170 916\n",
      " 562 283 291 360 857  84 714 390 800 304 616 702 658 194 733 375 921 105\n",
      " 252 789 500 536 685 689 918 111 200 746 608 278 386 854 317 696 490 453\n",
      "  32 832 484 718 621 804 583 659 229 796 335  35  15 631 590 363 324 176\n",
      " 531  69 708 791 438 544 799 681 240 351 573 451 433 493  47 852 289 599\n",
      " 546 913 775 302 190 254 597 148 409 772 332 625 126 683 907 367 220 665\n",
      " 158 753 236 955 792 159 341 533 388 361 788 132 488 938 140 910 888 431\n",
      " 770 603 837 118 710 605 931 627 261 730 157 560 722 242  28 816 460 684\n",
      " 893 491 614 448  59 813 393 894 882  82 185 511  73 555 745 534 619 142\n",
      " 646  41 581 957  17 558 514 858 337 110 709 257 926 480 415 473 589 219\n",
      " 724 494 287 592  75 795 826 123 101 384 908 411 553 723 557  40 155 935\n",
      " 392  27 818  37  12 364 238 228 610 896 530 639  36 704 674 153 812 517\n",
      " 676 432 344 267 336 609 349 380 454 911 653 345 295 600 736  10 504 482\n",
      " 310 106  67 423 842 593 241 459 912 929 430 290 309 732 853 934 370 373\n",
      " 183 543 834 666 129 612 915 872 456  23 259 740 633 308 906 203 417 468\n",
      " 715 752 374 114 758 830 819 243  97 855 941 561  79 263 195 594 224 895\n",
      " 851 266 866 713 331 656 932 914 754 429 539  61 642 117  92 161 404 917\n",
      " 638  76 125 522 802 426 223 922 892 154 210 650  74 208  19 173 768 474\n",
      " 737 281 422 181 519 586 315 230 630 400 467 660 121 873 394 441 779  45\n",
      " 104 524 418 705 442 766 797 279 821 807 124 325 942 839 268 449 179 362\n",
      " 275 576 503 843  14 606 447  81  33 793 927 365 209 306 703 848 218 377\n",
      " 356 617 406 611 664   1 874 285 545 166 171 338 408 450 862  63 902   0\n",
      " 342 398 777 568 270 877 648 192  72 347 227  99  85 712  42  66 277 343\n",
      " 178 632 167 518 564 440 925  43 180 744 346 773 320  55 898 201 532  62\n",
      "  44 501 580 421 885 445 823 591 149 951 333 628 376 850 572 695 247   7\n",
      " 174 128 810 637 759 903 311 300 652 686 462  94 383 280 667 655 801 107\n",
      " 647 662 206 385 717 217  65 293 461 368  98 329  20 177 437  89 233 840\n",
      "  95 571 186 833  11 547 623 691 184 492 122 497 936 739 584 947 222 565\n",
      " 741 316  29 901 846 836 930 815 175  24  93 954 475 369 481 340 297 327\n",
      " 794 672 526 680 334 470 825 687 464 323 143 678 120 904 764 103 950 498\n",
      " 523  38 196  71 831 861 213 649 355 286 643 899 274 502 479  21 434 945\n",
      " 785 483 946 750 312 782  16 251 112 784 292 108 725 820 640 271 328  48\n",
      " 528 301 755 563 515 670 389  77 100 413 747 489 215 538 868 863 602 693\n",
      " 513 757 469 887 844 559   5 828   4 711 731 139  60 221 330 296  30 318\n",
      "  13 920 102 214 720 679 552 607 205 551 585 738 350 596 841 134 284 244\n",
      "  49 476 273  26 829 956 232 147 401 420  56  52 288 156 234 357 127 150\n",
      " 654 395 391 353 824 669 884 250 322 471 294 570 806  50 587 407 618 778\n",
      " 299 326 886 948 698 897 225 682 172 651 424 169 663  87 435 566 144 569\n",
      " 688 644 305 814  25 889 845 191 760 645 443 141 119 786 402 890 535 706\n",
      " 204 865 163 780  46 943  86 307  22  83 692 550 876   6 879 131  53 690\n",
      "   3 516 624 216 891 629 382 763 265 875 239  31 403 272  64 867 359 940\n",
      " 478 425 919 728 582 849 463 477  70 260 635 765 729 135 452 525 601 798\n",
      " 521 776 235 769  78 673 767 716 366  88 783 817 761 805 677 146 847 883\n",
      " 595 465 870 321 675   2 790 641 387 909 604 165 137 116 548 276 253 636\n",
      " 211 499 354  54  51 860 314 439 428 622 529 505 245 743 540 162 803 781\n",
      " 339  96 212 835 198 613 372 774 381  80 130 164 699 303 485 657 751 944\n",
      " 509 458 109 952 924 138 905  34   9 313 187]\n",
      "Test:  [226 787 507 827  18 136 933 412 446 466 615 542 115 701 661 880 269 554\n",
      " 762 694 748 620 168 928 145 937 427 258 416 399 352 742 939 588 379 567\n",
      " 188 486 856 487 133 838 358 923 197 237 556 811 202 508 182 282 537 822\n",
      " 598 371 506 726 455  57 671  39 808 626 719 541 472  68 771 953 900 319\n",
      " 419  90 878 864 397 574 721 756 113 246 414 405 578 727 668 520 579 697\n",
      " 707 444 527 160 871 255 151 264 549 510 189 152 700 577 436 734 410 396\n",
      " 207 193  58 249 378 496 949 248 262 749 735]\n",
      "Train: [575 634 512 231   8 199 809 495 298 869  91 881 859 348 457 256 170 916\n",
      " 562 283 291 360 857  84 714 390 800 304 616 702 658 194 733 375 921 105\n",
      " 252 789 500 536 685 689 918 111 200 746 608 278 386 854 317 696 490 453\n",
      "  32 832 484 718 621 804 583 659 229 796 335  35  15 631 590 363 324 176\n",
      " 531  69 708 791 438 544 799 681 240 351 573 451 433 493  47 852 289 599\n",
      " 546 913 775 302 190 254 597 148 409 772 332 625 126 683 907 367 220 665\n",
      " 158 753 236 955 792 159 341 533 388 361 788 132 488 938 140 910 888 431\n",
      " 770 603 837 118 710 605 931 627 261 730 157 560 722 242  28 816 460 684\n",
      " 893 491 614 448  59 813 393 894 882  82 185 511  73 555 745 534 619 142\n",
      " 646  41 581 957  17 558 514 858 337 110 709 257 926 480 415 473 589 219\n",
      " 724 494 287 592  75 795 826 123 101 384 908 411 553 723 557  40 155 935\n",
      " 392  27 818  37  12 364 238 228 610 896 530 639  36 704 674 153 812 517\n",
      " 676 432 344 267 336 609 349 380 454 911 653 345 295 600 736  10 504 482\n",
      " 310 106  67 423 842 593 241 459 912 929 430 290 309 732 853 934 370 373\n",
      " 183 543 834 666 129 612 915 872 456  23 259 740 633 308 906 203 417 468\n",
      " 715 752 374 114 758 830 819 243  97 855 941 561  79 263 195 594 224 895\n",
      " 851 266 866 713 331 656 932 914 754 429 539  61 642 117  92 161 404 917\n",
      " 638  76 125 522 802 426 223 922 892 154 210 650  74 208  19 173 768 474\n",
      " 737 281 422 181 519 586 315 230 630 400 467 660 121 873 394 441 779  45\n",
      " 104 524 418 705 442 766 797 279 821 807 124 325 942 839 268 449 179 362\n",
      " 275 576 503 843  14 606 447  81  33 793 927 365 209 306 703 848 218 377\n",
      " 356 617 406 611 664   1 874 285 545 166 171 338 408 450 862  63 902   0\n",
      " 342 398 777 568 270 877 648 192  72 347 227  99  85 712  42  66 277 343\n",
      " 178 632 167 518 564 440 925  43 180 744 346 773 320  55 898 201 532  62\n",
      "  44 501 580 421 885 445 823 591 149 951 333 628 376 850 572 695 247   7\n",
      " 174 128 810 637 759 903 311 300 652 686 462  94 383 280 667 655 801 107\n",
      " 647 662 206 385 717 217  65 293 461 368  98 329  20 177 437  89 233 840\n",
      "  95 571 186 833  11 547 623 691 184 492 122 497 936 739 584 947 222 565\n",
      " 741 316  29 901 846 836 930 815 175  24  93 954 475 369 481 340 297 327\n",
      " 794 672 526 680 334 470 825 687 464 323 143 678 120 904 764 103 950 498\n",
      " 523  38 196  71 831 861 213 649 355 286 643 899 274 502 479  21 434 945\n",
      " 785 483 946 750 312 782  16 251 112 784 292 108 725 820 640 271 328  48\n",
      " 528 301 755 563 515 670 389  77 100 413 747 489 215 538 868 863 602 693\n",
      " 513 757 469 887 844 559   5 828   4 711 731 139  60 221 330 296  30 318\n",
      "  13 920 102 214 720 679 552 607 205 551 585 738 350 596 841 134 284 244\n",
      "  49 476 273  26 829 956 232 147 401 420  56  52 288 156 234 357 127 150\n",
      " 654 395 391 353 824 669 884 250 322 471 294 570 806  50 587 407 618 778\n",
      " 299 326 886 948 698 897 225 682 172 651 424 169 663  87 435 566 144 569\n",
      " 688 644 305 814  25 889 845 191 760 645 443 141 119 786 402 890 535 706\n",
      " 204 865 163 780  46 943  86 307  22  83 692 550 226 787 507 827  18 136\n",
      " 933 412 446 466 615 542 115 701 661 880 269 554 762 694 748 620 168 928\n",
      " 145 937 427 258 416 399 352 742 939 588 379 567 188 486 856 487 133 838\n",
      " 358 923 197 237 556 811 202 508 182 282 537 822 598 371 506 726 455  57\n",
      " 671  39 808 626 719 541 472  68 771 953 900 319 419  90 878 864 397 574\n",
      " 721 756 113 246 414 405 578 727 668 520 579 697 707 444 527 160 871 255\n",
      " 151 264 549 510 189 152 700 577 436 734 410 396 207 193  58 249 378 496\n",
      " 949 248 262 749 735 138 905  34   9 313 187]\n",
      "Test:  [876   6 879 131  53 690   3 516 624 216 891 629 382 763 265 875 239  31\n",
      " 403 272  64 867 359 940 478 425 919 728 582 849 463 477  70 260 635 765\n",
      " 729 135 452 525 601 798 521 776 235 769  78 673 767 716 366  88 783 817\n",
      " 761 805 677 146 847 883 595 465 870 321 675   2 790 641 387 909 604 165\n",
      " 137 116 548 276 253 636 211 499 354  54  51 860 314 439 428 622 529 505\n",
      " 245 743 540 162 803 781 339  96 212 835 198 613 372 774 381  80 130 164\n",
      " 699 303 485 657 751 944 509 458 109 952 924]\n",
      "=========================================================================\n",
      "==============VALIDACIÓN CRUZADA CON GERMAN DATA K=4=====================\n",
      "Train: [367 274 734 491 609 512 443 636 395 451 119 816 968 100 907 416 830 291\n",
      " 889 215  73 303 912 152  83 992 811 866 457 897 504 559 548 998 169  71\n",
      " 673 190 247 681 756 696 638 674 537 453 309 934 133 760 173 820 466  84\n",
      "   4 137 461 227 891 703 390 995 705 465  74 489 730 515 534 433 892 952\n",
      " 375 374   7 728 271 797 877 847 580 436 571 880 717 905  79 863 869 575\n",
      "  32 648 634 819 225 134 735 405 140 664  57 812 345 365 209  49 868 194\n",
      " 827 552 861 616 926 139 488 990 110 224 205 710 198 467 348   0 564 765\n",
      " 141 254 316 622  48 347 229 255 718 403 538  16 510 776 806 129 298 301\n",
      " 715 408 295 331 478 888 807 745 733 219 989  12 400 666 470 381 167 753\n",
      " 569 637 623 874 267 404 189 680 985 742 356 946 613 917 763  54 102 294\n",
      " 501 278 532 879  30 670 191 434 774 118 474 430 382 166 987  11  61 183\n",
      " 589 204 940 904 645 182 280 508 852 826 121 639 493 957 171 799  80 259\n",
      " 997 858 211 937  65 688 804 809 798 275 930 871  66 973 208 499 924 420\n",
      " 142 598 587 325 545 885 351 933 335 235 669 296 136  26 371 314 785 833\n",
      " 759 663 567  20 319 506 260 117 343 122 442 747 929 577 384 624  36 829\n",
      " 546 361 168 963 257 731 300 149 223 492 685 794 984 994 498 568   2 105\n",
      " 228 342 932  70 954 654 541 738 792 894 597 417 482 754 949 409 197 233\n",
      " 646 383 410 494 192 120 234 357  40 862  34 422 935 818 178 113 528 916\n",
      " 704 293 655 372 771 127 793 195 423 948 216 815 555 610 668 288 485 458\n",
      " 628 543 893 456 867 701 391 853 755 687 744 644 986 401 712 450 248 210\n",
      "  13 978 832 947 849 976 662  75 585 396 311 424 437 138 143 923 286 691\n",
      " 327 737 802  17 426 337 640 186 586 823 729 462 147 566 268 502 349 328\n",
      " 601 464 828 631 603 870 909 649 570 262 412  37 969 839  31 724  10 843\n",
      " 339 980 740 326 463 901 476 449 678 665 350 243 974 185 956 418 581 397\n",
      " 336 355 578 658 821 850 544 334 511 519 632  68 900 594 505 653 317 972\n",
      " 882 513 479 447 584 517   5 671 758  98 106  33 560 354 650  77 214 561\n",
      " 176 768  59 454 810 593 558 859  23 814 918 244 495 239 530 766 736 287\n",
      " 322   3 727  18 647 529 441 941 193  99 323 911 507 873 263 213 777 273\n",
      " 226 553 250 959 297 988 188 618 333  89  97 770 779 915 999 944 108 429\n",
      " 881 925 218 308 860 402 848  56 392 150 840  82 369 242 876 196 486 379\n",
      " 536 148 557 675 146 415 702 329 751 526 890 642 468 307 590 421 573 661\n",
      " 781  52 838 684 641 473 490 719 522 360 419 241 898 179 953 786 694 425\n",
      " 332 460 790  78 207 726 896  96 155 851 358 895 346 878 721 172 503 256\n",
      " 596 324 321 767 212 520 249 677 285  60 563 964 202 104 279 370 865 903\n",
      " 748 971  95 617 222 320 619 689 676  81 407  90 282  69 305 200  76   9\n",
      " 657 554  93 592  85 394 388  35  91 845 112 600 411 221 883 318 448 846\n",
      " 431 161 523 927 962 330 153 720 928 602 535 887 899 539  39 455 158 672\n",
      " 472 252 844 945 175 310 977 115  53 413 154 808 128 960 236 817 533 387\n",
      " 841 856 438 800 612 380 362 682 910 914 967 958 697  63 445 238 991 363\n",
      " 521 389 788 116 931 376 783  55 565 752 825 780  86 125 955 306 659  25\n",
      " 518  42 299 996 446 157 359 368 266 270 607 130 217 123 627 611 471 983\n",
      " 784 579 855 595 114 428   1 132 965 775  43 313]\n",
      "Test:  [708 500 439 315  62  67 272 246 707 289 550 583 961 509  15 312  64 245\n",
      " 834 652 378 398 683 290 230 757 109 982 556 284 304 414 170 938 201  27\n",
      " 625 469 399 572 338 366 698 749 722 706 604 341 608 302 725 795 920 483\n",
      "  45  22  46 240 773 124 373 805 732 551 711 251 547 540 741 831 177 875\n",
      "  24 791 181  72 591 943 709  19 103 232 406 922 822 789 621 340 951 385\n",
      " 480 497 842 265 813 156 452 700 527 746 614 283 629 975 531 605 135 837\n",
      " 950 913 966 778 264 562 481 824 660 352 750 713 936   6 635 261 801 542\n",
      " 643 107 854   8 514 163 626 633 772 516 180  41 377 908 764 884 630 386\n",
      " 160 364  87 203 606  38 651 690 582  92 151 942 165  51 699 432 353 144\n",
      " 620 524 174 435 835 762 237 145 459 872 588  88 979 919 281 269 886 692\n",
      "  58 599 695 258 231 292 939 993 796 743 206  21 615  94 739 440  44  14\n",
      " 199 761 679 159 716 162 393 253 723 484 803 769 576 164 921 902 184 981\n",
      " 686 276 787 714 344 667 101  28  50 187 857  47  29 277 487 220 477 549\n",
      " 131 864 574 782 126 970 693 656 427 906 444 525 496 475 836 111]\n",
      "Train: [708 500 439 315  62  67 272 246 707 289 550 583 961 509  15 312  64 245\n",
      " 834 652 378 398 683 290 230 757 109 982 556 284 304 414 170 938 201  27\n",
      " 625 469 399 572 338 366 698 749 722 706 604 341 608 302 725 795 920 483\n",
      "  45  22  46 240 773 124 373 805 732 551 711 251 547 540 741 831 177 875\n",
      "  24 791 181  72 591 943 709  19 103 232 406 922 822 789 621 340 951 385\n",
      " 480 497 842 265 813 156 452 700 527 746 614 283 629 975 531 605 135 837\n",
      " 950 913 966 778 264 562 481 824 660 352 750 713 936   6 635 261 801 542\n",
      " 643 107 854   8 514 163 626 633 772 516 180  41 377 908 764 884 630 386\n",
      " 160 364  87 203 606  38 651 690 582  92 151 942 165  51 699 432 353 144\n",
      " 620 524 174 435 835 762 237 145 459 872 588  88 979 919 281 269 886 692\n",
      "  58 599 695 258 231 292 939 993 796 743 206  21 615  94 739 440  44  14\n",
      " 199 761 679 159 716 162 393 253 723 484 803 769 576 164 921 902 184 981\n",
      " 686 276 787 714 344 667 101  28  50 187 857  47  29 277 487 220 477 549\n",
      " 131 864 574 782 126 970 693 656 427 906 444 525 496 475 836 111 785 833\n",
      " 759 663 567  20 319 506 260 117 343 122 442 747 929 577 384 624  36 829\n",
      " 546 361 168 963 257 731 300 149 223 492 685 794 984 994 498 568   2 105\n",
      " 228 342 932  70 954 654 541 738 792 894 597 417 482 754 949 409 197 233\n",
      " 646 383 410 494 192 120 234 357  40 862  34 422 935 818 178 113 528 916\n",
      " 704 293 655 372 771 127 793 195 423 948 216 815 555 610 668 288 485 458\n",
      " 628 543 893 456 867 701 391 853 755 687 744 644 986 401 712 450 248 210\n",
      "  13 978 832 947 849 976 662  75 585 396 311 424 437 138 143 923 286 691\n",
      " 327 737 802  17 426 337 640 186 586 823 729 462 147 566 268 502 349 328\n",
      " 601 464 828 631 603 870 909 649 570 262 412  37 969 839  31 724  10 843\n",
      " 339 980 740 326 463 901 476 449 678 665 350 243 974 185 956 418 581 397\n",
      " 336 355 578 658 821 850 544 334 511 519 632  68 900 594 505 653 317 972\n",
      " 882 513 479 447 584 517   5 671 758  98 106  33 560 354 650  77 214 561\n",
      " 176 768  59 454 810 593 558 859  23 814 918 244 495 239 530 766 736 287\n",
      " 322   3 727  18 647 529 441 941 193  99 323 911 507 873 263 213 777 273\n",
      " 226 553 250 959 297 988 188 618 333  89  97 770 779 915 999 944 108 429\n",
      " 881 925 218 308 860 402 848  56 392 150 840  82 369 242 876 196 486 379\n",
      " 536 148 557 675 146 415 702 329 751 526 890 642 468 307 590 421 573 661\n",
      " 781  52 838 684 641 473 490 719 522 360 419 241 898 179 953 786 694 425\n",
      " 332 460 790  78 207 726 896  96 155 851 358 895 346 878 721 172 503 256\n",
      " 596 324 321 767 212 520 249 677 285  60 563 964 202 104 279 370 865 903\n",
      " 748 971  95 617 222 320 619 689 676  81 407  90 282  69 305 200  76   9\n",
      " 657 554  93 592  85 394 388  35  91 845 112 600 411 221 883 318 448 846\n",
      " 431 161 523 927 962 330 153 720 928 602 535 887 899 539  39 455 158 672\n",
      " 472 252 844 945 175 310 977 115  53 413 154 808 128 960 236 817 533 387\n",
      " 841 856 438 800 612 380 362 682 910 914 967 958 697  63 445 238 991 363\n",
      " 521 389 788 116 931 376 783  55 565 752 825 780  86 125 955 306 659  25\n",
      " 518  42 299 996 446 157 359 368 266 270 607 130 217 123 627 611 471 983\n",
      " 784 579 855 595 114 428   1 132 965 775  43 313]\n",
      "Test:  [367 274 734 491 609 512 443 636 395 451 119 816 968 100 907 416 830 291\n",
      " 889 215  73 303 912 152  83 992 811 866 457 897 504 559 548 998 169  71\n",
      " 673 190 247 681 756 696 638 674 537 453 309 934 133 760 173 820 466  84\n",
      "   4 137 461 227 891 703 390 995 705 465  74 489 730 515 534 433 892 952\n",
      " 375 374   7 728 271 797 877 847 580 436 571 880 717 905  79 863 869 575\n",
      "  32 648 634 819 225 134 735 405 140 664  57 812 345 365 209  49 868 194\n",
      " 827 552 861 616 926 139 488 990 110 224 205 710 198 467 348   0 564 765\n",
      " 141 254 316 622  48 347 229 255 718 403 538  16 510 776 806 129 298 301\n",
      " 715 408 295 331 478 888 807 745 733 219 989  12 400 666 470 381 167 753\n",
      " 569 637 623 874 267 404 189 680 985 742 356 946 613 917 763  54 102 294\n",
      " 501 278 532 879  30 670 191 434 774 118 474 430 382 166 987  11  61 183\n",
      " 589 204 940 904 645 182 280 508 852 826 121 639 493 957 171 799  80 259\n",
      " 997 858 211 937  65 688 804 809 798 275 930 871  66 973 208 499 924 420\n",
      " 142 598 587 325 545 885 351 933 335 235 669 296 136  26 371 314]\n",
      "Train: [708 500 439 315  62  67 272 246 707 289 550 583 961 509  15 312  64 245\n",
      " 834 652 378 398 683 290 230 757 109 982 556 284 304 414 170 938 201  27\n",
      " 625 469 399 572 338 366 698 749 722 706 604 341 608 302 725 795 920 483\n",
      "  45  22  46 240 773 124 373 805 732 551 711 251 547 540 741 831 177 875\n",
      "  24 791 181  72 591 943 709  19 103 232 406 922 822 789 621 340 951 385\n",
      " 480 497 842 265 813 156 452 700 527 746 614 283 629 975 531 605 135 837\n",
      " 950 913 966 778 264 562 481 824 660 352 750 713 936   6 635 261 801 542\n",
      " 643 107 854   8 514 163 626 633 772 516 180  41 377 908 764 884 630 386\n",
      " 160 364  87 203 606  38 651 690 582  92 151 942 165  51 699 432 353 144\n",
      " 620 524 174 435 835 762 237 145 459 872 588  88 979 919 281 269 886 692\n",
      "  58 599 695 258 231 292 939 993 796 743 206  21 615  94 739 440  44  14\n",
      " 199 761 679 159 716 162 393 253 723 484 803 769 576 164 921 902 184 981\n",
      " 686 276 787 714 344 667 101  28  50 187 857  47  29 277 487 220 477 549\n",
      " 131 864 574 782 126 970 693 656 427 906 444 525 496 475 836 111 367 274\n",
      " 734 491 609 512 443 636 395 451 119 816 968 100 907 416 830 291 889 215\n",
      "  73 303 912 152  83 992 811 866 457 897 504 559 548 998 169  71 673 190\n",
      " 247 681 756 696 638 674 537 453 309 934 133 760 173 820 466  84   4 137\n",
      " 461 227 891 703 390 995 705 465  74 489 730 515 534 433 892 952 375 374\n",
      "   7 728 271 797 877 847 580 436 571 880 717 905  79 863 869 575  32 648\n",
      " 634 819 225 134 735 405 140 664  57 812 345 365 209  49 868 194 827 552\n",
      " 861 616 926 139 488 990 110 224 205 710 198 467 348   0 564 765 141 254\n",
      " 316 622  48 347 229 255 718 403 538  16 510 776 806 129 298 301 715 408\n",
      " 295 331 478 888 807 745 733 219 989  12 400 666 470 381 167 753 569 637\n",
      " 623 874 267 404 189 680 985 742 356 946 613 917 763  54 102 294 501 278\n",
      " 532 879  30 670 191 434 774 118 474 430 382 166 987  11  61 183 589 204\n",
      " 940 904 645 182 280 508 852 826 121 639 493 957 171 799  80 259 997 858\n",
      " 211 937  65 688 804 809 798 275 930 871  66 973 208 499 924 420 142 598\n",
      " 587 325 545 885 351 933 335 235 669 296 136  26 371 314 263 213 777 273\n",
      " 226 553 250 959 297 988 188 618 333  89  97 770 779 915 999 944 108 429\n",
      " 881 925 218 308 860 402 848  56 392 150 840  82 369 242 876 196 486 379\n",
      " 536 148 557 675 146 415 702 329 751 526 890 642 468 307 590 421 573 661\n",
      " 781  52 838 684 641 473 490 719 522 360 419 241 898 179 953 786 694 425\n",
      " 332 460 790  78 207 726 896  96 155 851 358 895 346 878 721 172 503 256\n",
      " 596 324 321 767 212 520 249 677 285  60 563 964 202 104 279 370 865 903\n",
      " 748 971  95 617 222 320 619 689 676  81 407  90 282  69 305 200  76   9\n",
      " 657 554  93 592  85 394 388  35  91 845 112 600 411 221 883 318 448 846\n",
      " 431 161 523 927 962 330 153 720 928 602 535 887 899 539  39 455 158 672\n",
      " 472 252 844 945 175 310 977 115  53 413 154 808 128 960 236 817 533 387\n",
      " 841 856 438 800 612 380 362 682 910 914 967 958 697  63 445 238 991 363\n",
      " 521 389 788 116 931 376 783  55 565 752 825 780  86 125 955 306 659  25\n",
      " 518  42 299 996 446 157 359 368 266 270 607 130 217 123 627 611 471 983\n",
      " 784 579 855 595 114 428   1 132 965 775  43 313]\n",
      "Test:  [785 833 759 663 567  20 319 506 260 117 343 122 442 747 929 577 384 624\n",
      "  36 829 546 361 168 963 257 731 300 149 223 492 685 794 984 994 498 568\n",
      "   2 105 228 342 932  70 954 654 541 738 792 894 597 417 482 754 949 409\n",
      " 197 233 646 383 410 494 192 120 234 357  40 862  34 422 935 818 178 113\n",
      " 528 916 704 293 655 372 771 127 793 195 423 948 216 815 555 610 668 288\n",
      " 485 458 628 543 893 456 867 701 391 853 755 687 744 644 986 401 712 450\n",
      " 248 210  13 978 832 947 849 976 662  75 585 396 311 424 437 138 143 923\n",
      " 286 691 327 737 802  17 426 337 640 186 586 823 729 462 147 566 268 502\n",
      " 349 328 601 464 828 631 603 870 909 649 570 262 412  37 969 839  31 724\n",
      "  10 843 339 980 740 326 463 901 476 449 678 665 350 243 974 185 956 418\n",
      " 581 397 336 355 578 658 821 850 544 334 511 519 632  68 900 594 505 653\n",
      " 317 972 882 513 479 447 584 517   5 671 758  98 106  33 560 354 650  77\n",
      " 214 561 176 768  59 454 810 593 558 859  23 814 918 244 495 239 530 766\n",
      " 736 287 322   3 727  18 647 529 441 941 193  99 323 911 507 873]\n",
      "Train: [708 500 439 315  62  67 272 246 707 289 550 583 961 509  15 312  64 245\n",
      " 834 652 378 398 683 290 230 757 109 982 556 284 304 414 170 938 201  27\n",
      " 625 469 399 572 338 366 698 749 722 706 604 341 608 302 725 795 920 483\n",
      "  45  22  46 240 773 124 373 805 732 551 711 251 547 540 741 831 177 875\n",
      "  24 791 181  72 591 943 709  19 103 232 406 922 822 789 621 340 951 385\n",
      " 480 497 842 265 813 156 452 700 527 746 614 283 629 975 531 605 135 837\n",
      " 950 913 966 778 264 562 481 824 660 352 750 713 936   6 635 261 801 542\n",
      " 643 107 854   8 514 163 626 633 772 516 180  41 377 908 764 884 630 386\n",
      " 160 364  87 203 606  38 651 690 582  92 151 942 165  51 699 432 353 144\n",
      " 620 524 174 435 835 762 237 145 459 872 588  88 979 919 281 269 886 692\n",
      "  58 599 695 258 231 292 939 993 796 743 206  21 615  94 739 440  44  14\n",
      " 199 761 679 159 716 162 393 253 723 484 803 769 576 164 921 902 184 981\n",
      " 686 276 787 714 344 667 101  28  50 187 857  47  29 277 487 220 477 549\n",
      " 131 864 574 782 126 970 693 656 427 906 444 525 496 475 836 111 367 274\n",
      " 734 491 609 512 443 636 395 451 119 816 968 100 907 416 830 291 889 215\n",
      "  73 303 912 152  83 992 811 866 457 897 504 559 548 998 169  71 673 190\n",
      " 247 681 756 696 638 674 537 453 309 934 133 760 173 820 466  84   4 137\n",
      " 461 227 891 703 390 995 705 465  74 489 730 515 534 433 892 952 375 374\n",
      "   7 728 271 797 877 847 580 436 571 880 717 905  79 863 869 575  32 648\n",
      " 634 819 225 134 735 405 140 664  57 812 345 365 209  49 868 194 827 552\n",
      " 861 616 926 139 488 990 110 224 205 710 198 467 348   0 564 765 141 254\n",
      " 316 622  48 347 229 255 718 403 538  16 510 776 806 129 298 301 715 408\n",
      " 295 331 478 888 807 745 733 219 989  12 400 666 470 381 167 753 569 637\n",
      " 623 874 267 404 189 680 985 742 356 946 613 917 763  54 102 294 501 278\n",
      " 532 879  30 670 191 434 774 118 474 430 382 166 987  11  61 183 589 204\n",
      " 940 904 645 182 280 508 852 826 121 639 493 957 171 799  80 259 997 858\n",
      " 211 937  65 688 804 809 798 275 930 871  66 973 208 499 924 420 142 598\n",
      " 587 325 545 885 351 933 335 235 669 296 136  26 371 314 785 833 759 663\n",
      " 567  20 319 506 260 117 343 122 442 747 929 577 384 624  36 829 546 361\n",
      " 168 963 257 731 300 149 223 492 685 794 984 994 498 568   2 105 228 342\n",
      " 932  70 954 654 541 738 792 894 597 417 482 754 949 409 197 233 646 383\n",
      " 410 494 192 120 234 357  40 862  34 422 935 818 178 113 528 916 704 293\n",
      " 655 372 771 127 793 195 423 948 216 815 555 610 668 288 485 458 628 543\n",
      " 893 456 867 701 391 853 755 687 744 644 986 401 712 450 248 210  13 978\n",
      " 832 947 849 976 662  75 585 396 311 424 437 138 143 923 286 691 327 737\n",
      " 802  17 426 337 640 186 586 823 729 462 147 566 268 502 349 328 601 464\n",
      " 828 631 603 870 909 649 570 262 412  37 969 839  31 724  10 843 339 980\n",
      " 740 326 463 901 476 449 678 665 350 243 974 185 956 418 581 397 336 355\n",
      " 578 658 821 850 544 334 511 519 632  68 900 594 505 653 317 972 882 513\n",
      " 479 447 584 517   5 671 758  98 106  33 560 354 650  77 214 561 176 768\n",
      "  59 454 810 593 558 859  23 814 918 244 495 239 530 766 736 287 322   3\n",
      " 727  18 647 529 441 941 193  99 323 911 507 873]\n",
      "Test:  [263 213 777 273 226 553 250 959 297 988 188 618 333  89  97 770 779 915\n",
      " 999 944 108 429 881 925 218 308 860 402 848  56 392 150 840  82 369 242\n",
      " 876 196 486 379 536 148 557 675 146 415 702 329 751 526 890 642 468 307\n",
      " 590 421 573 661 781  52 838 684 641 473 490 719 522 360 419 241 898 179\n",
      " 953 786 694 425 332 460 790  78 207 726 896  96 155 851 358 895 346 878\n",
      " 721 172 503 256 596 324 321 767 212 520 249 677 285  60 563 964 202 104\n",
      " 279 370 865 903 748 971  95 617 222 320 619 689 676  81 407  90 282  69\n",
      " 305 200  76   9 657 554  93 592  85 394 388  35  91 845 112 600 411 221\n",
      " 883 318 448 846 431 161 523 927 962 330 153 720 928 602 535 887 899 539\n",
      "  39 455 158 672 472 252 844 945 175 310 977 115  53 413 154 808 128 960\n",
      " 236 817 533 387 841 856 438 800 612 380 362 682 910 914 967 958 697  63\n",
      " 445 238 991 363 521 389 788 116 931 376 783  55 565 752 825 780  86 125\n",
      " 955 306 659  25 518  42 299 996 446 157 359 368 266 270 607 130 217 123\n",
      " 627 611 471 983 784 579 855 595 114 428   1 132 965 775  43 313]\n",
      "=========================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"==============VALIDACIÓN CRUZADA CON LENSES DATA K=5======================\")\n",
    "estrategia1 = ValidacionCruzada(5)\n",
    "estrategia1.creaParticiones(dataset)\n",
    "for particion in estrategia1.particiones:\n",
    "    print(particion)\n",
    "print(\"==========================================================================\")\n",
    "print(\"==============VALIDACIÓN CRUZADA CON TIC-TAC-TOE DATA K=8=================\")\n",
    "estrategia21 = ValidacionCruzada(8)\n",
    "estrategia21.creaParticiones(dataset2)\n",
    "for particion in estrategia21.particiones:\n",
    "    print(particion)\n",
    "print(\"=========================================================================\")\n",
    "print(\"==============VALIDACIÓN CRUZADA CON GERMAN DATA K=4=====================\")\n",
    "estrategia31 = ValidacionCruzada(4)\n",
    "estrategia31.creaParticiones(dataset3)\n",
    "for particion in estrategia31.particiones:\n",
    "    print(particion)\n",
    "print(\"=========================================================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Apartado 2: Naive-Bayes</h3>\n",
    "<p>Es un clasificador de datos que se basa en la regla de Bayes. Donde primero vamos a dividir el conjunto de datos y poder particionarlo en dos subconunto de datos con validación simple o en varios subconjunto de datos con validación cruzada. En el código que mostramos a continuación es lo que van hacer todos los clasificadores que podemos implementar. Los métodos de entrenamiento y clasifica cada clasificador lo hace el clasificador debido a que los métodos que van a utilizar son únicos y los tenemos que implementar en las clases especificas de cada clasificador.</p>\n",
    "<p>El método <strong>error</strong> va a comprobar los errores que ha obtenido nuestro clasificador, para ver el error que hemos obtenido se hace comparando la última columna de nuestra matriz de datos con la predicción que hemos obtenido en el método de clasifica del clasificador. Si es dintinto la predicción con la última columna de los datos le sumamos uno y lo vamos a dividir entre el número de lineas que tiene el subconjunto de datos Test.</p>\n",
    "<p>El método <strong>validación</strong> lo que va hacer es realizar los métodos de entrenamiento, clasifica y calcula error del clasificador seguido sin ninguna interrupcion. El método va a comprobar si le estan pasando validación simple o cruzada con el número de partciones que tiene. Si el tamaño es igual a 1 va a llamar a los métodos mencionados anteriormente y va a devolver el error que ha obtenido tras el entrenamiento y la clasificación. Si el tamaño es mayor a 1 hacemos un bucle que cubra todas las partciones para entrenarlas, clasificarlas y obtener su error, despues de la realización de esos métodos vamos a hacer la media aritmetica de todos los errores obtenidos con las diferentes particiones que tenemos en nuestra estrategia.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Clasificador:\n",
    "  # Clase abstracta\n",
    "  __metaclass__ = ABCMeta\n",
    "\n",
    "  # Metodos abstractos que se implementan en casa clasificador concreto\n",
    "  @abstractmethod\n",
    "  # TODO: esta funcion debe ser implementada en cada clasificador concreto\n",
    "  # datosTrain: matriz numpy con los datos de entrenamiento\n",
    "  # atributosDiscretos: array bool con la indicatriz de los atributos nominales\n",
    "  # diccionario: array de diccionarios de la estructura Datos utilizados para la codificacion de variables discretas\n",
    "  def entrenamiento(self, datos, datosTrain, atributosDiscretos, diccionario):\n",
    "    pass\n",
    "\n",
    "  @abstractmethod\n",
    "  # TODO: esta funcion debe ser implementada en cada clasificador concreto\n",
    "  # devuelve un numpy array con las predicciones\n",
    "  def clasifica(self, datosTest, atributosDiscretos, diccionario):\n",
    "    pass\n",
    "\n",
    "  # Obtiene el numero de aciertos y errores para calcular la tasa de fallo\n",
    "  # TODO: implementar\n",
    "  def error(self, datos, pred):\n",
    "    # Aqui se compara la prediccion (pred) con las clases reales y se calcula el error\n",
    "    i = 0\n",
    "    real = datos[:, -1]\n",
    "    error = 0\n",
    "    for i in range(len(real)):\n",
    "      if real[i] != pred[i]:\n",
    "        error += 1\n",
    "    err = (error) / (len(real) + 0.0)\n",
    "    return err\n",
    "\n",
    "  # Realiza una clasificacion utilizando una estrategia de particionado determinada\n",
    "  # TODO: implementar esta funcion\n",
    "  def validacion(self, particionado: object, dataset: object, clasificador: object, seed: object = None) -> object:\n",
    "\n",
    "    # Creamos las particiones siguiendo la estrategia llamando a particionado.creaParticiones\n",
    "    # - Para validacion cruzada: en el bucle hasta nv entrenamos el clasificador con la particion de train i\n",
    "    # y obtenemos el error en la particion de test i\n",
    "    # - Para validacion simple (hold-out): entrenamos el clasificador con la particion de train\n",
    "    # y obtenemos el error en la particion test. Otra opci�n es repetir la validaci�n simple un n�mero especificado de veces, obteniendo en cada una un error. Finalmente se calcular�a la media.\n",
    "    errores = 0\n",
    "    # particionado.creaParticiones(dataset, seed)\n",
    "    # Comprobamos si es por validación cruzada o simple, por la longitud de la lista de particiones\n",
    "\n",
    "    particionado.creaParticiones(dataset)\n",
    "\n",
    "    # Validación Simple\n",
    "    if len(particionado.particiones) == 1:\n",
    "      clasificador.entrenamiento(dataset, particionado.particiones[0].indicesTrain)\n",
    "      pred = clasificador.clasifica(dataset, particionado.particiones[0].indicesTest)\n",
    "      ret = self.error(dataset.extraeDatos(particionado.particiones[0].indicesTest), pred)\n",
    "      if ret > 0:\n",
    "        return ret\n",
    "      else:\n",
    "        return 0\n",
    "\n",
    "    # Validación Cruzada\n",
    "    else:\n",
    "      for particion in particionado.particiones:\n",
    "        clasificador.entrenamiento(dataset, particion.indicesTrain)\n",
    "        pred = clasificador.clasifica(dataset, particion.indicesTest)\n",
    "        ret = self.error(dataset.extraeDatos(particion.indicesTest), pred)\n",
    "        errores += ret\n",
    "      error = errores / len(particionado.particiones)\n",
    "\n",
    "      # Devolucion de la media de los errores\n",
    "      return error\n",
    "\n",
    "  def matrizConfusion(self, dataset, datosTest, prediccion):\n",
    "\n",
    "    # Calculamos la matriz de confusion utlizando sk-learn. Solo se calcula en el caso de que la clasificacion sea binaria.\n",
    "    testData = dataset.extraeDatos(datosTest)\n",
    "    clase_real = testData[:, -1]\n",
    "\n",
    "    matriz = confusion_matrix(prediccion, clase_real)\n",
    "\n",
    "    # La funcion ravel() devuelve todas las estadisticas relacionadas con la matriz de confusion\n",
    "    tn, fp, fn, tp = matriz.ravel()\n",
    "\n",
    "\n",
    "    # Calculamos las tasas extraídas de la matriz de confusión\n",
    "    tpr = tp / (tp + fn)\n",
    "    fpr = fp / (fp + fn)\n",
    "\n",
    "    self.lista_tpr.append(tpr)\n",
    "    self.lista_fpr.append(fpr)\n",
    "\n",
    "    return matriz\n",
    "\n",
    "  def curvaROC(self):\n",
    "\n",
    "    x = np.linspace(0, 1, 100)\n",
    "    plt.plot(x, x, c='blue')\n",
    "    for i in range(len(self.lista_fpr)):\n",
    "        plt.plot(self.lista_fpr[i],self.lista_tpr[i],'ro')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Implementación del Clasificador Naive-Bayes</h3>\n",
    "<p>En la siguiente celda vamos a poder observar el codigo de entrenamiento y clasifica del clasificador de Naive-Bayes, a continuación explicaremos brevemente el funcionamiento de cada uno de los métodos especificos del clasificador. En este claisificador podemos aplicar la regla de Laplace, que es si obtenemos un cero en algunas de las mátrices de conteos de los datos, es decir, a la hora de calcular P(D|H), tendremos que sumar 1 a todas las celdas de conteos de esa P(D|H).</p>\n",
    "<p>El método <strong>entrenamiento</strong>, lo primero que hace este método es obtener las probabilidades a priori de las clases que hay en el subconjunto de Test. Al calcular esas probabbilades las introducimos en un diccionario para poder utilizarlas más tarde. Ahora vamos a calcular la P(D|H)que se va a calcular de diferentes maneras para datos continuos o discretos\n",
    "    <ol>\n",
    "        <li><strong>Atributos discretos</strong>: se calcula haciendo los conteos de las veces que sale la P(D|H) en el subconjunto de datos Train.</li>\n",
    "        <li><strong>Atributos continuos</strong>: se calcula la media y desviación tipica del subconjunto de datos Train.</li>\n",
    "    </ol>\n",
    "Todo esto se mete en una matriz que tiene los diferentes datos y las diferentes clases del conjunto de datos, donde esa matriz la vamos a utilizar para calcular las probabilidades a posteriori con todos las datos que tiene las matrices que hemos creado.</p>\n",
    "<p>El método <strong> clasifica</strong> dependiendo de los datos que vamos obteniendo del subconjunto de datos de Test, y vamos a obtener las diferentes probabilidades de las distintas clases que tenemos el problema si nos llega ese dato. Esas probabilidades las guardamos en una lista para luego multiplicarlas por los a priori y poder coger la clase que de más probabilidad para guardarla en la lista de las predicciones de nuestro clasificador Naive-Bayes y así despues obtener el error que hemos obtenenido. Este método tambien clasifica de manera distinta los atributos discretos y continuos:\n",
    "    <ol>\n",
    "        <li><strong>Atributos discretos</strong>: se calcula obteniendo el número que hay en la matriz de probabilidades a posteriori que hemos creado anteriormente.</li>\n",
    "        <li><strong>Atributos continuos</strong>: se calcula haciendo la ecuación de la distribución normal.\n",
    "   </ol>\n",
    " </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClasificadorNaiveBayes(Clasificador):\n",
    "\n",
    "  def __init__(self, laplace):\n",
    "    self.laplace = laplace\n",
    "    self.lista_fpr = []\n",
    "    self.lista_tpr = []\n",
    "\n",
    "  def entrenamiento(self, dataset, datosTrain):\n",
    "\n",
    "    # Cargamos todos los datos de la clase del dataset desde la matriz de datos\n",
    "    clasesTrain = dataset.extraeDatos(datosTrain)\n",
    "    self.numClases = clasesTrain[:, -1]\n",
    "\n",
    "    # Contamos las apariciones de cada uno para luego calcular la probabilidad a priori de cada clase\n",
    "    counter = Counter(self.numClases)\n",
    "\n",
    "    # Calculamos la probabilidad de la clase y lo metemos en un diccionario ordenado segun el numero\n",
    "    # correspondiente a cada clase asignado en el diccionario\n",
    "    self.dictPrioris = {}\n",
    "    for k in counter:\n",
    "      k = int(k)\n",
    "      counter[k] = counter[k] / len(self.numClases)\n",
    "      self.dictPrioris[k] = counter[k]\n",
    "\n",
    "    # Aqui ordenamos el diccionario para que esten en el mismo orden de como extraemos los datos del dataset\n",
    "    self.dictPrioris = SortedDict(self.dictPrioris)\n",
    "\n",
    "    # Calcular tablas de probabilidades del entrenamiento. Tenemos que calcular por cada atributo una cuenta\n",
    "    # de las apariciones en cada clase\n",
    "    # Creamos una lista de matrices, donde vamos almacenar todos los datos que hemos obtenido en los datos de Test\n",
    "    self.posteriori = np.zeros(len(dataset.nombreAtributos) - 1, dtype=object)\n",
    "\n",
    "    # Recorremos todos los datos de la matriz sin llegar a la clase\n",
    "    for i in range(len(dataset.nombreAtributos) - 1):\n",
    "\n",
    "      # Si el dato que obtenemos es Nominal haremos el recuento de todas las veces que sale la P(D|H)\n",
    "      if dataset.nominalAtributos[i] == True:\n",
    "\n",
    "        # Creamos una matriz de tamaño X: Número de Atributos menos la clase Y: Número de clases\n",
    "        post = np.zeros((len(dataset.listaDicts[i]), len(dataset.listaDicts[-1])))\n",
    "\n",
    "        # Aqui contamos todos las datos que queremos del datos Train para construir la matriz de entrenamiento\n",
    "        for c in range(len(dataset.listaDicts[-1])):\n",
    "          datosEnt = dataset.extraeDatos(datosTrain)\n",
    "          dat = datosEnt[:, i]\n",
    "          repes = Counter(dat[datosEnt[:, -1] == c])\n",
    "          for r in repes:\n",
    "            post[int(r), c] = repes[r]\n",
    "          if self.laplace == True:\n",
    "            self.posteriori[i] = post + 1\n",
    "          else:\n",
    "            self.posteriori[i] = post\n",
    "\n",
    "      # Si el dato es Continuo obtendremos la media y la desviación tipica de la clase\n",
    "      else:\n",
    "\n",
    "        # Creamos una matriz de X: Los datos de Media y Desivación típica Y: Número de clases\n",
    "        post = np.zeros((2, len(dataset.listaDicts[-1])))\n",
    "\n",
    "        # Aqui obtenemos la media y desviación tipica de cada clase, despues de tener los datos de entrenamiento\n",
    "        for c in range(len(dataset.listaDicts[-1])):\n",
    "          datosEnt = dataset.extraeDatos(datosTrain)\n",
    "          dat = datosEnt[:, i]\n",
    "          datos = dat[datosEnt[:, -1] == c]\n",
    "          post[0][c] = np.mean(datos)\n",
    "          post[1][c] = np.std(datos)\n",
    "        self.posteriori[i] = post\n",
    "\n",
    "\n",
    "    # Calculamos los valores de los posteriori de todos las tablas anteriores\n",
    "    for i in range(len(dataset.listaDicts) - 1):\n",
    "      if dataset.nominalAtributos[i] == True:\n",
    "        self.posteriori[i] /= sum(self.posteriori[i])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "  def clasifica(self, dataset, datosTest):\n",
    "    acum_probs = 1\n",
    "    self.prediccion = []\n",
    "    datTest = dataset.extraeDatos(datosTest)\n",
    "\n",
    "    # Ahora vamos a estudiar la probabilidad de la clase con los datos obtenidos en el entrenamiento\n",
    "    # Recorremos todos las datos de la matriz de los datos Test\n",
    "    for dato in datTest:\n",
    "      mapa = []\n",
    "      # Aqui obtenemos los prioris de cada clase para poder obtener la probabilidad de cada una\n",
    "      for clase in range(len(self.dictPrioris)):\n",
    "        listaVerosimilitudes = []\n",
    "        # Aqui obtenemos cada valor posteriori de nuestro entrenamiento de los datos, es decir, P(D|H)\n",
    "        for atributo in range(len(self.posteriori)):\n",
    "          if dataset.nominalAtributos[atributo] == True:\n",
    "            prob = self.posteriori[atributo][int(dato[atributo])][clase]\n",
    "            listaVerosimilitudes.append(prob)\n",
    "\n",
    "          # Aqui obtenemos la probabilidad de los atibutos continuos\n",
    "          else:\n",
    "            # Hacemos la formula de la distribucion normal\n",
    "            exp1 = 1 / (self.posteriori[atributo][1][clase] * math.sqrt(2 * math.pi))\n",
    "            exp2 = np.power((dato[atributo] - self.posteriori[atributo][0][clase]), 2)\n",
    "            exp3 = np.power(self.posteriori[atributo][1][clase], 2)\n",
    "            exp4 = exp2 / exp3\n",
    "            exp4 = math.exp((-1 / 2) * exp4)\n",
    "            prob = exp1 * exp4\n",
    "            listaVerosimilitudes.append(prob)\n",
    "\n",
    "        for verosimilitud in listaVerosimilitudes:\n",
    "          acum_probs *= verosimilitud\n",
    "        acum_probs *= self.dictPrioris.get(clase)\n",
    "        mapa.append(acum_probs)\n",
    "        acum_probs = 1\n",
    "\n",
    "      # Aqui obtenemos la predicción de mayor probabilidad y la guardamos en nuestra lista de predicciones\n",
    "      self.prediccion.append(np.argmax(mapa))\n",
    "\n",
    "\n",
    "    # Devolvemos la lista con la predicción de nuestro clasifica\n",
    "    return self.prediccion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>A continuación, vamos a mostrar una ejecución del claisificador de Naive-Bayes con las diferentes validaciones y los diferentes conjuntos de datos que tenemos, al final de esto mostraremos la probabilidad que hemos obtenido</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============NAIVE-BAYES CON LENSES DATA ======================\n",
      "==============CON LAPLACE Y VALIDACION SIMPLE===================\n",
      "ERROR OBTENIDO: 0.375\n",
      "==============CON LAPLACE Y VALIDACION CRUZADA==================\n",
      "ERROR OBTENIDO: 0.3375000000000001\n",
      "==============SIN LAPLACE Y VALIDACION SIMPLE===================\n",
      "ERROR OBTENIDO: 0.25\n",
      "==============SIN LAPLACE Y VALIDACION CRUZADA==================\n",
      "ERROR OBTENIDO: 0.3360000000000001\n",
      "================================================================\n",
      "==============NAIVE-BAYES CON TIC-TAC-TOE DATA==================\n",
      "==============CON LAPLACE Y VALIDACION SIMPLE===================\n",
      "ERROR OBTENIDO: 0.2916666666666667\n",
      "==============CON LAPLACE Y VALIDACION CRUZADA==================\n",
      "ERROR OBTENIDO: 0.3019498424369748\n",
      "==============SIN LAPLACE Y VALIDACION SIMPLE===================\n",
      "ERROR OBTENIDO: 0.2916666666666667\n",
      "==============SIN LAPLACE Y VALIDACION CRUZADA==================\n",
      "ERROR OBTENIDO: 0.3025280112044818\n",
      "================================================================\n",
      "==============NAIVE-BAYES CON GERMAN DATA ======================\n",
      "==============CON LAPLACE Y VALIDACION SIMPLE===================\n",
      "ERROR OBTENIDO: 0.276\n",
      "==============CON LAPLACE Y VALIDACION CRUZADA==================\n",
      "ERROR OBTENIDO: 0.25275\n",
      "==============SIN LAPLACE Y VALIDACION SIMPLE===================\n",
      "ERROR OBTENIDO: 0.284\n",
      "==============SIN LAPLACE Y VALIDACION CRUZADA==================\n",
      "ERROR OBTENIDO: 0.251\n",
      "================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"==============NAIVE-BAYES CON LENSES DATA ======================\")\n",
    "print(\"==============CON LAPLACE Y VALIDACION SIMPLE===================\")\n",
    "nb = ClasificadorNaiveBayes(True)\n",
    "error = nb.validacion(estrategia,dataset,nb)\n",
    "print(\"ERROR OBTENIDO:\",error)\n",
    "print(\"==============CON LAPLACE Y VALIDACION CRUZADA==================\")\n",
    "nb = ClasificadorNaiveBayes(True)\n",
    "error = nb.validacion(estrategia1,dataset,nb)\n",
    "print(\"ERROR OBTENIDO:\",error)\n",
    "print(\"==============SIN LAPLACE Y VALIDACION SIMPLE===================\")\n",
    "nb = ClasificadorNaiveBayes(False)\n",
    "error = nb.validacion(estrategia,dataset,nb)\n",
    "print(\"ERROR OBTENIDO:\",error)\n",
    "print(\"==============SIN LAPLACE Y VALIDACION CRUZADA==================\")\n",
    "nb = ClasificadorNaiveBayes(False)\n",
    "error = nb.validacion(estrategia1,dataset,nb)\n",
    "print(\"ERROR OBTENIDO:\",error)\n",
    "print(\"================================================================\")\n",
    "print(\"==============NAIVE-BAYES CON TIC-TAC-TOE DATA==================\")\n",
    "print(\"==============CON LAPLACE Y VALIDACION SIMPLE===================\")\n",
    "nb = ClasificadorNaiveBayes(True)\n",
    "error = nb.validacion(estrategia2,dataset2,nb)\n",
    "print(\"ERROR OBTENIDO:\",error)\n",
    "print(\"==============CON LAPLACE Y VALIDACION CRUZADA==================\")\n",
    "nb = ClasificadorNaiveBayes(True)\n",
    "error = nb.validacion(estrategia21,dataset2,nb)\n",
    "print(\"ERROR OBTENIDO:\",error)\n",
    "print(\"==============SIN LAPLACE Y VALIDACION SIMPLE===================\")\n",
    "nb = ClasificadorNaiveBayes(False)\n",
    "error = nb.validacion(estrategia2,dataset2,nb)\n",
    "print(\"ERROR OBTENIDO:\",error)\n",
    "print(\"==============SIN LAPLACE Y VALIDACION CRUZADA==================\")\n",
    "nb = ClasificadorNaiveBayes(False)\n",
    "error = nb.validacion(estrategia21,dataset2,nb)\n",
    "print(\"ERROR OBTENIDO:\",error)\n",
    "print(\"================================================================\")\n",
    "print(\"==============NAIVE-BAYES CON GERMAN DATA ======================\")\n",
    "print(\"==============CON LAPLACE Y VALIDACION SIMPLE===================\")\n",
    "nb = ClasificadorNaiveBayes(True)\n",
    "error = nb.validacion(estrategia3,dataset3,nb)\n",
    "print(\"ERROR OBTENIDO:\",error)\n",
    "print(\"==============CON LAPLACE Y VALIDACION CRUZADA==================\")\n",
    "nb = ClasificadorNaiveBayes(True)\n",
    "error = nb.validacion(estrategia31,dataset3,nb)\n",
    "print(\"ERROR OBTENIDO:\",error)\n",
    "print(\"==============SIN LAPLACE Y VALIDACION SIMPLE===================\")\n",
    "nb = ClasificadorNaiveBayes(False)\n",
    "error = nb.validacion(estrategia3,dataset3,nb)\n",
    "print(\"ERROR OBTENIDO:\",error)\n",
    "print(\"==============SIN LAPLACE Y VALIDACION CRUZADA==================\")\n",
    "nb = ClasificadorNaiveBayes(False)\n",
    "error = nb.validacion(estrategia31,dataset3,nb)\n",
    "print(\"ERROR OBTENIDO:\",error)\n",
    "print(\"================================================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Apartado 3: Scikit-Learn</h3>\n",
    "<p>Es una libreria de python que nos proporciona diferentes implementaciones del clasificador de Naive-Bayes. A continuación, vamos a explicar lo que hace cada método que hemos implementado en la celda de abajo con la libreria de Scikit-Learn.</p>\n",
    "<p>El método <strong>validacion_simple_sklearn</strong> donde le introducimos por parametro el conjunto de datos del cual queremos hacer la validación y el porcentaje que queremos tener del subconjunto de entrenamiento. La función de sklearn nos va a devolver los subconjuntos de datos de entrenamiento y de clasificación.</p>\n",
    "<p>El método <strong>validacion_cruzada_sklearn</strong> donde le introducimos por parametro el conjunto de datos del cual queremos hacer la validación y el número de partciones que vamos a hacer del conjunto de datos. El método nos va a devolver la lista de particiones que hemos obtenido con el método de sklearn.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold, cross_val_predict\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from EstrategiaParticionado import Particion\n",
    "def validacion_simple_sklearn(dataset, porcentaje):\n",
    "\n",
    "    # Matriz con los atributos\n",
    "    X = dataset.datos[:, :-1]\n",
    "\n",
    "    # Array con las clases\n",
    "    y = dataset.datos[:, -1]\n",
    "\n",
    "    # Realizamos la divison en train-test, X_train es la partición sobre la que se va a entrenar e X_test sobre la que se va a clasificar\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=porcentaje, test_size=1 - porcentaje, shuffle=True)\n",
    "\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "def validacion_cruzada_sklearn(dataset, k):\n",
    "\n",
    "    # Matriz con los atributos\n",
    "    X = dataset.datos[:, :-1]\n",
    "\n",
    "    # Array con las clases\n",
    "    y = dataset.datos[:, -1]\n",
    "\n",
    "    kf = KFold(n_splits=k, shuffle=True)\n",
    "\n",
    "    particiones = []\n",
    "\n",
    "    for train_index, test_index in kf.split(X,y):\n",
    "        particiones.append(Particion(train_index,test_index))\n",
    "\n",
    "    return particiones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============SKLEARN VALIDACIÓN SIMPLE 70% LENSES DATA==================\n",
      "TRAIN:\n",
      " [[1. 0. 1. 1.]\n",
      " [1. 0. 1. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [1. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 1. 1. 1.]\n",
      " [0. 1. 1. 1.]\n",
      " [0. 0. 1. 0.]\n",
      " [2. 1. 1. 0.]\n",
      " [2. 0. 0. 0.]\n",
      " [0. 0. 1. 1.]\n",
      " [2. 1. 0. 1.]\n",
      " [2. 0. 0. 1.]\n",
      " [0. 1. 1. 0.]\n",
      " [0. 1. 0. 1.]\n",
      " [2. 1. 0. 0.]]\n",
      "TEST:\n",
      " [[1. 1. 0. 1.]\n",
      " [2. 1. 1. 1.]\n",
      " [2. 0. 1. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 0. 1.]\n",
      " [2. 0. 1. 1.]\n",
      " [1. 1. 1. 0.]\n",
      " [0. 0. 0. 0.]]\n",
      "==============SKLEARN VALIDACIÓN CRUZADA K=5 LENSES DATA=================\n",
      "Train: [ 0  2  3  4  5  6  7  8  9 10 12 13 14 16 17 18 19 20 23]\n",
      "Test:  [ 1 11 15 21 22]\n",
      "Train: [ 0  1  2  3  4  6  7  8 10 11 12 15 16 18 19 20 21 22 23]\n",
      "Test:  [ 5  9 13 14 17]\n",
      "Train: [ 0  1  4  5  6  8  9 10 11 12 13 14 15 17 18 19 20 21 22]\n",
      "Test:  [ 2  3  7 16 23]\n",
      "Train: [ 0  1  2  3  4  5  7  9 11 13 14 15 16 17 19 20 21 22 23]\n",
      "Test:  [ 6  8 10 12 18]\n",
      "Train: [ 1  2  3  5  6  7  8  9 10 11 12 13 14 15 16 17 18 21 22 23]\n",
      "Test:  [ 0  4 19 20]\n",
      "=========================================================================\n",
      "==============SKLEARN VALIDACIÓN SIMPLE 80% TIC-TAC-TOE DATA=============\n",
      "TRAIN:\n",
      " [[0. 2. 0. ... 0. 2. 1.]\n",
      " [0. 2. 1. ... 1. 2. 2.]\n",
      " [0. 1. 0. ... 2. 2. 2.]\n",
      " ...\n",
      " [1. 2. 0. ... 1. 0. 2.]\n",
      " [1. 2. 1. ... 2. 2. 2.]\n",
      " [0. 2. 1. ... 1. 0. 0.]]\n",
      "TEST:\n",
      " [[0. 0. 2. ... 0. 2. 2.]\n",
      " [0. 2. 1. ... 1. 2. 0.]\n",
      " [0. 2. 1. ... 2. 2. 0.]\n",
      " ...\n",
      " [2. 1. 2. ... 0. 1. 0.]\n",
      " [0. 1. 0. ... 2. 2. 2.]\n",
      " [2. 1. 0. ... 2. 0. 0.]]\n",
      "==============SKLEARN VALIDACIÓN CRUZADA K=8 TIC-TAC-TOE DATA============\n",
      "Train: [  0   1   2   3   5   6   7   8   9  10  11  13  14  15  16  17  18  19\n",
      "  20  21  23  24  25  26  27  28  30  32  33  34  35  36  37  38  39  42\n",
      "  43  44  45  46  47  48  49  50  51  52  53  54  55  56  57  58  59  60\n",
      "  61  62  63  64  65  66  67  68  69  70  71  72  73  75  77  78  79  80\n",
      "  81  82  83  84  86  87  88  89  90  91  92  93  94  95  96  97  98 100\n",
      " 101 102 103 104 106 107 108 109 110 111 112 113 114 115 116 117 118 119\n",
      " 120 121 122 123 124 125 126 127 129 130 131 132 134 135 136 137 138 139\n",
      " 140 141 142 143 144 145 146 147 150 151 152 153 154 156 157 158 159 160\n",
      " 161 162 163 164 165 166 167 168 169 170 171 172 173 174 176 178 179 180\n",
      " 181 182 184 185 186 187 188 189 190 191 192 193 194 195 196 198 200 201\n",
      " 202 203 204 205 209 210 211 212 213 214 215 216 217 219 220 221 222 224\n",
      " 225 229 230 232 233 234 235 237 238 239 240 241 242 243 244 245 246 247\n",
      " 248 249 252 254 255 256 257 258 259 260 261 262 263 264 266 267 268 269\n",
      " 270 271 272 273 274 275 276 277 278 279 281 282 284 285 286 287 288 290\n",
      " 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 308 309\n",
      " 310 311 312 313 314 315 316 317 318 319 321 322 323 324 325 326 327 328\n",
      " 329 330 331 332 333 334 335 336 339 340 341 342 343 344 345 346 349 350\n",
      " 351 352 353 354 355 356 357 359 360 361 362 363 364 365 368 369 370 371\n",
      " 372 373 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390\n",
      " 391 392 393 394 395 396 397 399 400 402 403 404 405 407 408 409 410 411\n",
      " 412 413 414 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430\n",
      " 431 433 434 435 436 437 438 440 441 442 443 444 445 446 447 448 449 450\n",
      " 451 452 453 454 456 457 458 459 460 461 462 463 464 465 466 467 468 469\n",
      " 470 473 474 475 476 477 479 480 481 482 485 486 487 488 489 490 491 492\n",
      " 493 494 495 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511\n",
      " 512 513 514 515 516 517 519 521 522 523 524 525 527 528 529 530 531 533\n",
      " 534 535 537 538 539 540 541 544 545 546 548 549 550 551 552 553 554 555\n",
      " 556 557 558 559 561 562 563 564 565 566 567 569 570 572 573 574 575 576\n",
      " 577 578 579 580 581 582 583 585 586 587 588 589 590 591 592 593 594 596\n",
      " 597 598 599 600 601 602 604 605 606 607 609 611 612 613 615 616 617 618\n",
      " 619 620 621 623 624 625 627 628 630 631 632 633 634 635 636 637 638 639\n",
      " 640 641 642 644 646 647 648 649 650 651 652 653 654 655 656 657 658 659\n",
      " 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677\n",
      " 678 679 680 681 682 683 684 685 686 687 689 691 692 693 695 696 697 698\n",
      " 699 700 702 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718\n",
      " 719 720 721 722 723 724 725 726 727 729 730 731 732 733 734 735 736 737\n",
      " 738 739 740 741 743 744 745 746 747 748 749 750 751 752 753 754 755 756\n",
      " 757 758 759 760 761 762 764 765 766 767 769 770 772 773 774 775 776 777\n",
      " 778 779 780 781 782 783 784 785 786 787 788 789 790 792 793 794 795 796\n",
      " 799 800 801 802 804 806 807 808 810 811 813 814 815 816 817 818 819 820\n",
      " 821 822 823 824 825 826 827 828 829 830 831 832 833 835 838 839 840 841\n",
      " 842 843 844 845 846 848 849 851 852 853 854 855 856 858 859 860 861 862\n",
      " 863 864 865 867 868 870 871 872 873 875 876 877 878 879 880 881 882 883\n",
      " 885 886 887 888 889 890 891 892 894 895 896 897 898 899 901 902 903 904\n",
      " 905 906 907 908 909 910 911 912 913 914 915 916 917 919 920 921 922 923\n",
      " 925 926 927 929 931 932 933 934 935 936 939 940 941 942 943 944 945 947\n",
      " 948 949 950 951 952 953 954 955 956 957]\n",
      "Test:  [  4  12  22  29  31  40  41  74  76  85  99 105 128 133 148 149 155 175\n",
      " 177 183 197 199 206 207 208 218 223 226 227 228 231 236 250 251 253 265\n",
      " 280 283 289 307 320 337 338 347 348 358 366 367 374 398 401 406 415 432\n",
      " 439 455 471 472 478 483 484 496 518 520 526 532 536 542 543 547 560 568\n",
      " 571 584 595 603 608 610 614 622 626 629 643 645 688 690 694 701 703 728\n",
      " 742 763 768 771 791 797 798 803 805 809 812 834 836 837 847 850 857 866\n",
      " 869 874 884 893 900 918 924 928 930 937 938 946]\n",
      "Train: [  2   3   4   5   6   7   9  10  11  12  16  17  18  19  20  22  23  24\n",
      "  25  26  29  30  31  32  34  36  37  38  39  40  41  45  47  48  49  50\n",
      "  51  52  54  55  57  58  59  60  61  62  63  64  65  66  68  69  70  71\n",
      "  72  73  74  75  76  77  78  80  81  82  84  85  86  89  90  91  92  93\n",
      "  95  96  97  98  99 100 101 103 104 105 106 107 108 109 110 113 114 115\n",
      " 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133\n",
      " 134 135 136 137 138 139 142 143 144 145 146 147 148 149 151 152 153 155\n",
      " 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173\n",
      " 174 175 176 177 179 180 181 182 183 184 185 186 187 189 190 191 192 193\n",
      " 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211\n",
      " 212 213 214 215 216 217 218 219 220 222 223 224 225 226 227 228 229 231\n",
      " 232 233 234 235 236 238 239 240 241 243 245 246 247 248 249 250 251 252\n",
      " 253 254 255 257 258 259 260 261 262 263 265 266 267 268 269 270 271 272\n",
      " 273 274 276 277 278 279 280 281 282 283 284 285 287 288 289 290 291 293\n",
      " 295 296 297 298 299 300 301 302 303 304 306 307 308 309 310 312 313 314\n",
      " 315 316 317 318 319 320 321 323 324 325 326 327 328 329 330 331 332 333\n",
      " 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351\n",
      " 352 354 356 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372\n",
      " 373 374 375 376 377 379 381 382 383 384 385 386 387 388 389 390 391 392\n",
      " 393 394 395 397 398 400 401 403 404 406 407 408 409 410 412 413 414 415\n",
      " 416 417 418 420 421 422 423 424 425 427 428 429 430 431 432 433 434 435\n",
      " 436 437 438 439 440 442 443 444 445 446 447 448 450 451 452 453 454 455\n",
      " 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473\n",
      " 474 475 476 477 478 479 480 481 482 483 484 486 487 488 489 490 491 492\n",
      " 494 496 497 498 499 500 501 502 503 504 505 506 509 510 511 512 513 515\n",
      " 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533\n",
      " 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 551 552\n",
      " 553 554 555 556 557 559 560 561 563 564 565 567 568 570 571 572 573 574\n",
      " 575 576 577 579 580 581 582 583 584 585 586 587 588 589 590 591 592 594\n",
      " 595 596 597 598 599 600 602 603 604 605 607 608 609 610 612 613 614 615\n",
      " 616 617 618 619 620 622 623 625 626 627 628 629 630 631 632 633 634 635\n",
      " 637 638 640 642 643 644 645 647 648 649 650 651 652 653 654 655 656 657\n",
      " 658 659 660 661 662 663 665 666 667 668 669 670 672 673 677 678 679 680\n",
      " 681 682 683 686 687 688 689 690 691 692 693 694 695 696 698 699 700 701\n",
      " 702 703 704 705 706 707 708 709 710 711 712 713 715 716 717 719 720 721\n",
      " 722 723 724 725 726 727 728 729 731 732 733 734 735 736 737 738 740 741\n",
      " 742 743 744 748 749 750 751 752 753 754 755 756 758 760 761 762 763 764\n",
      " 765 766 767 768 769 770 771 775 776 777 778 779 780 781 782 783 784 785\n",
      " 786 787 788 789 790 791 793 794 795 796 797 798 799 801 802 803 805 806\n",
      " 807 808 809 810 812 813 814 815 816 817 818 819 820 822 824 826 828 829\n",
      " 830 831 832 833 834 835 836 837 838 839 840 841 842 843 844 845 846 847\n",
      " 848 849 850 851 852 854 855 856 857 858 859 860 861 862 863 864 865 866\n",
      " 867 868 869 870 871 872 873 874 875 876 877 878 879 880 881 882 883 884\n",
      " 885 886 887 888 889 890 891 892 893 894 895 896 897 898 899 900 901 902\n",
      " 903 905 906 907 909 910 911 912 913 914 915 916 918 919 921 922 923 924\n",
      " 925 926 927 928 929 930 932 933 934 935 936 937 938 939 940 941 942 943\n",
      " 945 946 949 950 951 952 953 954 955 957]\n",
      "Test:  [  0   1   8  13  14  15  21  27  28  33  35  42  43  44  46  53  56  67\n",
      "  79  83  87  88  94 102 111 112 140 141 150 154 178 188 221 230 237 242\n",
      " 244 256 264 275 286 292 294 305 311 322 353 355 357 378 380 396 399 402\n",
      " 405 411 419 426 441 449 485 493 495 507 508 514 550 558 562 566 569 578\n",
      " 593 601 606 611 621 624 636 639 641 646 664 671 674 675 676 684 685 697\n",
      " 714 718 730 739 745 746 747 757 759 772 773 774 792 800 804 811 821 823\n",
      " 825 827 853 904 908 917 920 931 944 947 948 956]\n",
      "Train: [  0   1   2   3   4   5   6   7   8   9  10  12  13  14  15  16  17  18\n",
      "  19  21  22  23  24  25  27  28  29  30  31  32  33  34  35  37  38  39\n",
      "  40  41  42  43  44  45  46  47  48  50  51  52  53  55  56  57  58  59\n",
      "  60  61  62  63  64  65  67  69  70  71  72  73  74  75  76  77  78  79\n",
      "  80  81  82  83  84  85  86  87  88  89  90  91  92  93  94  95  96  97\n",
      "  98  99 100 101 102 103 104 105 107 108 109 110 111 112 114 115 117 118\n",
      " 119 120 121 122 123 124 125 126 127 128 130 131 132 133 135 137 138 139\n",
      " 140 141 142 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158\n",
      " 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177\n",
      " 178 179 182 183 184 185 186 187 188 189 192 193 194 197 198 199 200 201\n",
      " 202 203 204 205 206 207 208 209 210 211 213 214 215 216 217 218 219 220\n",
      " 221 222 223 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239\n",
      " 240 241 242 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258\n",
      " 259 260 261 262 263 264 265 266 267 270 271 272 273 274 275 276 277 278\n",
      " 279 280 281 282 283 285 286 287 289 290 291 292 293 294 295 296 297 299\n",
      " 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 316 317 318\n",
      " 319 320 321 322 324 325 326 327 328 329 330 332 333 334 335 337 338 339\n",
      " 340 342 343 345 346 347 348 349 350 351 353 354 355 356 357 358 359 360\n",
      " 361 363 364 365 366 367 369 370 372 373 374 377 378 379 380 381 383 384\n",
      " 385 386 387 389 390 392 393 394 395 396 397 398 399 400 401 402 403 404\n",
      " 405 406 407 408 409 410 411 413 414 415 416 417 418 419 420 421 422 423\n",
      " 425 426 427 428 430 431 432 434 437 438 439 440 441 442 443 444 445 446\n",
      " 447 448 449 450 452 453 454 455 456 457 458 460 461 463 464 465 466 467\n",
      " 468 469 471 472 473 475 478 479 480 481 483 484 485 486 487 488 489 490\n",
      " 491 492 493 494 495 496 497 499 500 501 502 503 504 505 506 507 508 509\n",
      " 510 511 512 513 514 515 516 517 518 519 520 522 523 524 525 526 527 528\n",
      " 530 531 532 534 535 536 538 541 542 543 545 546 547 548 549 550 551 553\n",
      " 554 555 556 557 558 559 560 561 562 564 565 566 567 568 569 570 571 572\n",
      " 574 575 576 577 578 580 581 582 583 584 585 586 588 589 590 592 593 594\n",
      " 595 596 597 598 599 600 601 603 605 606 607 608 609 610 611 612 613 614\n",
      " 615 616 618 620 621 622 623 624 625 626 628 629 630 631 632 633 635 636\n",
      " 637 638 639 640 641 642 643 644 645 646 647 648 650 651 652 653 654 655\n",
      " 656 657 658 659 660 661 662 663 664 667 668 669 671 672 674 675 676 677\n",
      " 678 680 681 683 684 685 686 687 688 689 690 691 692 694 695 697 698 699\n",
      " 700 701 703 704 705 706 707 708 709 710 713 714 716 717 718 719 720 721\n",
      " 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739\n",
      " 740 741 742 743 744 745 746 747 748 749 751 752 754 755 756 757 758 759\n",
      " 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777\n",
      " 778 780 782 784 785 787 788 789 790 791 792 793 794 797 798 799 800 801\n",
      " 802 803 804 805 807 809 810 811 812 813 814 815 816 818 819 820 821 822\n",
      " 823 824 825 826 827 828 829 830 832 834 835 836 837 839 840 841 842 844\n",
      " 845 846 847 848 849 850 852 853 854 855 857 859 860 861 862 863 866 867\n",
      " 868 869 870 871 872 873 874 875 876 877 878 879 880 881 882 883 884 885\n",
      " 887 888 890 891 892 893 894 895 896 898 899 900 902 904 906 907 908 909\n",
      " 910 911 912 913 914 915 917 918 919 920 921 922 923 924 925 926 927 928\n",
      " 929 930 931 932 933 934 935 936 937 938 939 940 941 942 943 944 945 946\n",
      " 947 948 949 951 952 953 954 955 956 957]\n",
      "Test:  [ 11  20  26  36  49  54  66  68 106 113 116 129 134 136 143 159 180 181\n",
      " 190 191 195 196 212 224 243 268 269 284 288 298 315 323 331 336 341 344\n",
      " 352 362 368 371 375 376 382 388 391 412 424 429 433 435 436 451 459 462\n",
      " 470 474 476 477 482 498 521 529 533 537 539 540 544 552 563 573 579 587\n",
      " 591 602 604 617 619 627 634 649 665 666 670 673 679 682 693 696 702 711\n",
      " 712 715 750 753 779 781 783 786 795 796 806 808 817 831 833 838 843 851\n",
      " 856 858 864 865 886 889 897 901 903 905 916 950]\n",
      "Train: [  0   1   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18\n",
      "  20  21  22  23  24  25  26  27  28  29  31  32  33  34  35  36  37  38\n",
      "  39  40  41  42  43  44  45  46  47  48  49  50  53  54  55  56  57  59\n",
      "  60  62  63  65  66  67  68  69  70  71  72  74  76  77  78  79  80  81\n",
      "  82  83  84  85  86  87  88  89  90  91  93  94  96  97  98  99 100 102\n",
      " 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 120 121 122\n",
      " 123 124 125 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141\n",
      " 142 143 144 145 146 148 149 150 151 153 154 155 157 158 159 160 163 164\n",
      " 165 166 167 168 171 172 173 174 175 176 177 178 180 181 182 183 184 185\n",
      " 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203\n",
      " 204 205 206 207 208 209 210 211 212 214 215 216 217 218 219 220 221 222\n",
      " 223 224 226 227 228 229 230 231 232 233 234 235 236 237 239 240 241 242\n",
      " 243 244 245 246 247 249 250 251 252 253 254 255 256 257 258 259 260 261\n",
      " 263 264 265 266 268 269 270 271 272 273 274 275 276 277 278 279 280 281\n",
      " 282 283 284 285 286 287 288 289 290 291 292 294 296 297 298 300 301 302\n",
      " 303 304 305 307 308 309 310 311 312 313 314 315 317 318 319 320 322 323\n",
      " 324 326 327 328 329 330 331 332 333 334 335 336 337 338 339 341 342 343\n",
      " 344 345 346 347 348 349 350 351 352 353 355 356 357 358 359 360 362 363\n",
      " 364 365 366 367 368 369 370 371 372 374 375 376 377 378 379 380 381 382\n",
      " 383 385 387 388 389 391 392 395 396 398 399 400 401 402 403 404 405 406\n",
      " 407 408 410 411 412 413 415 416 419 420 421 422 424 425 426 427 428 429\n",
      " 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448\n",
      " 449 450 451 453 455 456 457 458 459 460 461 462 463 464 465 466 467 470\n",
      " 471 472 474 475 476 477 478 480 481 482 483 484 485 486 487 489 490 492\n",
      " 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510\n",
      " 511 512 514 516 517 518 519 520 521 522 524 526 527 528 529 530 532 533\n",
      " 534 535 536 537 538 539 540 541 542 543 544 545 546 547 550 551 552 553\n",
      " 554 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572\n",
      " 573 575 576 577 578 579 581 582 584 586 587 590 591 592 593 595 596 597\n",
      " 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 614 615 616\n",
      " 617 618 619 620 621 622 624 625 626 627 628 629 631 632 633 634 635 636\n",
      " 639 640 641 642 643 644 645 646 647 649 651 652 653 654 655 656 660 661\n",
      " 663 664 665 666 667 668 669 670 671 673 674 675 676 677 678 679 680 681\n",
      " 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699\n",
      " 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717\n",
      " 718 719 720 721 722 726 728 729 730 731 732 734 735 736 737 739 740 741\n",
      " 742 743 745 746 747 748 750 751 752 753 754 755 757 759 760 761 762 763\n",
      " 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781\n",
      " 783 784 785 786 787 788 790 791 792 793 794 795 796 797 798 799 800 803\n",
      " 804 805 806 807 808 809 811 812 813 814 816 817 819 821 822 823 824 825\n",
      " 827 828 829 830 831 832 833 834 835 836 837 838 839 840 843 844 845 847\n",
      " 848 849 850 851 852 853 854 855 856 857 858 859 860 861 862 863 864 865\n",
      " 866 867 868 869 870 871 872 873 874 875 876 877 878 879 881 884 885 886\n",
      " 887 888 889 890 891 893 895 896 897 898 899 900 901 903 904 905 906 907\n",
      " 908 909 910 911 912 913 914 916 917 918 920 921 922 923 924 925 926 927\n",
      " 928 929 930 931 932 933 934 935 936 937 938 939 940 941 943 944 945 946\n",
      " 947 948 949 950 951 952 953 954 955 956]\n",
      "Test:  [  2  19  30  51  52  58  61  64  73  75  92  95 101 103 119 126 147 152\n",
      " 156 161 162 169 170 179 213 225 238 248 262 267 293 295 299 306 316 321\n",
      " 325 340 354 361 373 384 386 390 393 394 397 409 414 417 418 423 430 452\n",
      " 454 468 469 473 479 488 491 513 515 523 525 531 548 549 555 574 580 583\n",
      " 585 588 589 594 613 623 630 637 638 648 650 657 658 659 662 672 723 724\n",
      " 725 727 733 738 744 749 756 758 782 789 801 802 810 815 818 820 826 841\n",
      " 842 846 880 882 883 892 894 902 915 919 942 957]\n",
      "Train: [  0   1   2   4   5   6   7   8  11  12  13  14  15  17  18  19  20  21\n",
      "  22  23  25  26  27  28  29  30  31  32  33  35  36  37  38  40  41  42\n",
      "  43  44  46  48  49  50  51  52  53  54  55  56  57  58  59  60  61  62\n",
      "  63  64  65  66  67  68  70  72  73  74  75  76  77  78  79  80  81  82\n",
      "  83  85  86  87  88  89  91  92  94  95  97  99 100 101 102 103 104 105\n",
      " 106 107 110 111 112 113 115 116 117 118 119 120 121 122 123 124 125 126\n",
      " 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144\n",
      " 146 147 148 149 150 151 152 153 154 155 156 157 159 160 161 162 163 164\n",
      " 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182\n",
      " 183 184 186 187 188 189 190 191 193 194 195 196 197 198 199 200 201 203\n",
      " 204 205 206 207 208 209 210 212 213 214 215 217 218 219 220 221 222 223\n",
      " 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 241 242\n",
      " 243 244 245 247 248 249 250 251 253 255 256 257 258 259 260 261 262 263\n",
      " 264 265 266 267 268 269 270 271 272 274 275 277 278 279 280 281 283 284\n",
      " 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302\n",
      " 304 305 306 307 308 309 310 311 312 315 316 317 318 319 320 321 322 323\n",
      " 325 326 327 329 331 333 335 336 337 338 339 340 341 342 343 344 345 347\n",
      " 348 350 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367\n",
      " 368 371 373 374 375 376 378 379 380 381 382 383 384 385 386 387 388 389\n",
      " 390 391 392 393 394 395 396 397 398 399 400 401 402 404 405 406 408 409\n",
      " 410 411 412 414 415 416 417 418 419 420 421 422 423 424 425 426 427 429\n",
      " 430 431 432 433 435 436 437 439 440 441 444 445 446 447 449 451 452 454\n",
      " 455 456 457 458 459 460 461 462 463 465 466 468 469 470 471 472 473 474\n",
      " 475 476 477 478 479 480 481 482 483 484 485 486 488 490 491 492 493 495\n",
      " 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513\n",
      " 514 515 517 518 519 520 521 522 523 524 525 526 529 531 532 533 535 536\n",
      " 537 539 540 542 543 544 545 546 547 548 549 550 552 553 554 555 556 557\n",
      " 558 560 561 562 563 565 566 567 568 569 570 571 573 574 575 576 577 578\n",
      " 579 580 581 582 583 584 585 587 588 589 590 591 592 593 594 595 596 597\n",
      " 598 599 600 601 602 603 604 605 606 608 609 610 611 612 613 614 615 616\n",
      " 617 618 619 621 622 623 624 625 626 627 629 630 631 632 634 635 636 637\n",
      " 638 639 640 641 642 643 644 645 646 648 649 650 651 652 653 654 656 657\n",
      " 658 659 660 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676\n",
      " 677 678 679 680 681 682 683 684 685 687 688 689 690 692 693 694 696 697\n",
      " 698 701 702 703 704 705 706 707 708 710 711 712 714 715 716 717 718 719\n",
      " 720 721 723 724 725 726 727 728 730 731 732 733 734 735 736 738 739 741\n",
      " 742 744 745 746 747 748 749 750 751 752 753 754 756 757 758 759 761 762\n",
      " 763 764 765 766 768 771 772 773 774 776 777 779 781 782 783 784 785 786\n",
      " 787 788 789 790 791 792 794 795 796 797 798 800 801 802 803 804 805 806\n",
      " 807 808 809 810 811 812 813 815 816 817 818 820 821 822 823 824 825 826\n",
      " 827 828 829 830 831 832 833 834 835 836 837 838 839 840 841 842 843 845\n",
      " 846 847 848 849 850 851 852 853 854 855 856 857 858 859 860 861 862 863\n",
      " 864 865 866 867 868 869 871 872 873 874 875 880 881 882 883 884 885 886\n",
      " 887 888 889 891 892 893 894 895 896 897 898 899 900 901 902 903 904 905\n",
      " 906 908 909 910 911 912 914 915 916 917 918 919 920 921 923 924 926 927\n",
      " 928 929 930 931 933 934 935 936 937 938 939 940 942 943 944 945 946 947\n",
      " 948 949 950 951 952 953 954 955 956 957]\n",
      "Test:  [  3   9  10  16  24  34  39  45  47  69  71  84  90  93  96  98 108 109\n",
      " 114 145 158 185 192 202 211 216 240 246 252 254 273 276 282 303 313 314\n",
      " 324 328 330 332 334 346 349 351 369 370 372 377 403 407 413 428 434 438\n",
      " 442 443 448 450 453 464 467 487 489 494 516 527 528 530 534 538 541 551\n",
      " 559 564 572 586 607 620 628 633 647 655 661 686 691 695 699 700 709 713\n",
      " 722 729 737 740 743 755 760 767 769 770 775 778 780 793 799 814 819 844\n",
      " 870 876 877 878 879 890 907 913 922 925 932 941]\n",
      "Train: [  0   1   2   3   4   5   7   8   9  10  11  12  13  14  15  16  17  19\n",
      "  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36  39\n",
      "  40  41  42  43  44  45  46  47  49  50  51  52  53  54  55  56  58  59\n",
      "  61  62  63  64  66  67  68  69  70  71  73  74  75  76  77  78  79  80\n",
      "  81  82  83  84  85  87  88  90  92  93  94  95  96  98  99 101 102 103\n",
      " 105 106 107 108 109 111 112 113 114 115 116 117 119 121 124 126 127 128\n",
      " 129 132 133 134 135 136 138 140 141 143 144 145 146 147 148 149 150 151\n",
      " 152 153 154 155 156 158 159 161 162 164 166 167 169 170 172 174 175 176\n",
      " 177 178 179 180 181 182 183 184 185 187 188 189 190 191 192 194 195 196\n",
      " 197 198 199 200 201 202 205 206 207 208 209 210 211 212 213 215 216 217\n",
      " 218 219 221 222 223 224 225 226 227 228 230 231 232 233 234 235 236 237\n",
      " 238 240 241 242 243 244 245 246 247 248 250 251 252 253 254 255 256 259\n",
      " 260 261 262 264 265 266 267 268 269 270 271 272 273 274 275 276 278 279\n",
      " 280 281 282 283 284 285 286 287 288 289 290 292 293 294 295 296 297 298\n",
      " 299 300 301 303 305 306 307 308 309 311 312 313 314 315 316 317 318 319\n",
      " 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337\n",
      " 338 339 340 341 344 346 347 348 349 350 351 352 353 354 355 356 357 358\n",
      " 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376\n",
      " 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394\n",
      " 395 396 397 398 399 401 402 403 404 405 406 407 408 409 411 412 413 414\n",
      " 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432\n",
      " 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450\n",
      " 451 452 453 454 455 456 457 458 459 460 462 463 464 465 467 468 469 470\n",
      " 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 487 488 489\n",
      " 490 491 493 494 495 496 497 498 499 500 503 504 506 507 508 509 510 511\n",
      " 513 514 515 516 518 519 520 521 522 523 524 525 526 527 528 529 530 531\n",
      " 532 533 534 535 536 537 538 539 540 541 542 543 544 546 547 548 549 550\n",
      " 551 552 553 554 555 557 558 559 560 562 563 564 565 566 567 568 569 570\n",
      " 571 572 573 574 575 578 579 580 581 582 583 584 585 586 587 588 589 591\n",
      " 593 594 595 597 600 601 602 603 604 606 607 608 610 611 612 613 614 616\n",
      " 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634\n",
      " 635 636 637 638 639 641 642 643 644 645 646 647 648 649 650 651 652 654\n",
      " 655 656 657 658 659 661 662 664 665 666 667 669 670 671 672 673 674 675\n",
      " 676 678 679 680 681 682 683 684 685 686 688 689 690 691 692 693 694 695\n",
      " 696 697 698 699 700 701 702 703 704 705 706 707 709 710 711 712 713 714\n",
      " 715 718 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 737\n",
      " 738 739 740 742 743 744 745 746 747 748 749 750 751 753 754 755 756 757\n",
      " 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775\n",
      " 777 778 779 780 781 782 783 784 785 786 789 790 791 792 793 795 796 797\n",
      " 798 799 800 801 802 803 804 805 806 808 809 810 811 812 813 814 815 817\n",
      " 818 819 820 821 823 824 825 826 827 828 829 830 831 833 834 835 836 837\n",
      " 838 840 841 842 843 844 846 847 849 850 851 852 853 854 855 856 857 858\n",
      " 859 860 864 865 866 867 868 869 870 871 872 873 874 876 877 878 879 880\n",
      " 881 882 883 884 886 887 889 890 892 893 894 895 896 897 898 900 901 902\n",
      " 903 904 905 907 908 912 913 914 915 916 917 918 919 920 922 923 924 925\n",
      " 926 927 928 930 931 932 933 934 935 936 937 938 939 941 942 943 944 945\n",
      " 946 947 948 949 950 951 953 954 956 957]\n",
      "Test:  [  6  18  37  38  48  57  60  65  72  86  89  91  97 100 104 110 118 120\n",
      " 122 123 125 130 131 137 139 142 157 160 163 165 168 171 173 186 193 203\n",
      " 204 214 220 229 239 249 257 258 263 277 291 302 304 310 342 343 345 400\n",
      " 410 461 466 486 492 501 502 505 512 517 545 556 561 576 577 590 592 596\n",
      " 598 599 605 609 615 640 653 660 663 668 677 687 708 716 717 719 720 736\n",
      " 741 752 776 787 788 794 807 816 822 832 839 845 848 861 862 863 875 885\n",
      " 888 891 899 906 909 910 911 921 929 940 952 955]\n",
      "Train: [  0   1   2   3   4   6   7   8   9  10  11  12  13  14  15  16  17  18\n",
      "  19  20  21  22  23  24  26  27  28  29  30  31  32  33  34  35  36  37\n",
      "  38  39  40  41  42  43  44  45  46  47  48  49  51  52  53  54  55  56\n",
      "  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71  72  73  74\n",
      "  75  76  77  78  79  83  84  85  86  87  88  89  90  91  92  93  94  95\n",
      "  96  97  98  99 100 101 102 103 104 105 106 107 108 109 110 111 112 113\n",
      " 114 116 117 118 119 120 121 122 123 124 125 126 128 129 130 131 132 133\n",
      " 134 136 137 139 140 141 142 143 145 147 148 149 150 152 153 154 155 156\n",
      " 157 158 159 160 161 162 163 165 168 169 170 171 172 173 174 175 176 177\n",
      " 178 179 180 181 183 184 185 186 188 189 190 191 192 193 195 196 197 198\n",
      " 199 201 202 203 204 206 207 208 211 212 213 214 216 218 220 221 223 224\n",
      " 225 226 227 228 229 230 231 232 234 235 236 237 238 239 240 241 242 243\n",
      " 244 246 247 248 249 250 251 252 253 254 256 257 258 259 260 262 263 264\n",
      " 265 266 267 268 269 272 273 274 275 276 277 278 279 280 282 283 284 285\n",
      " 286 287 288 289 290 291 292 293 294 295 298 299 302 303 304 305 306 307\n",
      " 308 309 310 311 312 313 314 315 316 318 320 321 322 323 324 325 327 328\n",
      " 330 331 332 334 335 336 337 338 340 341 342 343 344 345 346 347 348 349\n",
      " 351 352 353 354 355 357 358 360 361 362 364 365 366 367 368 369 370 371\n",
      " 372 373 374 375 376 377 378 379 380 381 382 383 384 386 388 389 390 391\n",
      " 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 409 410\n",
      " 411 412 413 414 415 416 417 418 419 420 422 423 424 426 428 429 430 432\n",
      " 433 434 435 436 437 438 439 440 441 442 443 445 446 448 449 450 451 452\n",
      " 453 454 455 457 458 459 461 462 464 465 466 467 468 469 470 471 472 473\n",
      " 474 475 476 477 478 479 480 482 483 484 485 486 487 488 489 491 492 493\n",
      " 494 495 496 497 498 501 502 503 505 507 508 512 513 514 515 516 517 518\n",
      " 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537\n",
      " 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 554 555 556\n",
      " 557 558 559 560 561 562 563 564 565 566 568 569 571 572 573 574 576 577\n",
      " 578 579 580 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596\n",
      " 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614\n",
      " 615 617 618 619 620 621 622 623 624 626 627 628 629 630 631 633 634 636\n",
      " 637 638 639 640 641 643 645 646 647 648 649 650 652 653 654 655 657 658\n",
      " 659 660 661 662 663 664 665 666 668 670 671 672 673 674 675 676 677 679\n",
      " 682 683 684 685 686 687 688 690 691 692 693 694 695 696 697 698 699 700\n",
      " 701 702 703 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720\n",
      " 721 722 723 724 725 727 728 729 730 731 733 736 737 738 739 740 741 742\n",
      " 743 744 745 746 747 749 750 752 753 754 755 756 757 758 759 760 761 763\n",
      " 764 765 766 767 768 769 770 771 772 773 774 775 776 778 779 780 781 782\n",
      " 783 784 785 786 787 788 789 790 791 792 793 794 795 796 797 798 799 800\n",
      " 801 802 803 804 805 806 807 808 809 810 811 812 813 814 815 816 817 818\n",
      " 819 820 821 822 823 824 825 826 827 831 832 833 834 836 837 838 839 841\n",
      " 842 843 844 845 846 847 848 850 851 853 854 856 857 858 859 860 861 862\n",
      " 863 864 865 866 868 869 870 873 874 875 876 877 878 879 880 881 882 883\n",
      " 884 885 886 888 889 890 891 892 893 894 895 896 897 899 900 901 902 903\n",
      " 904 905 906 907 908 909 910 911 912 913 914 915 916 917 918 919 920 921\n",
      " 922 924 925 926 928 929 930 931 932 933 937 938 939 940 941 942 943 944\n",
      " 945 946 947 948 949 950 952 954 955 956 957]\n",
      "Test:  [  5  25  50  80  81  82 115 127 135 138 144 146 151 164 166 167 182 187\n",
      " 194 200 205 209 210 215 217 219 222 233 245 255 261 270 271 281 296 297\n",
      " 300 301 317 319 326 329 333 339 350 356 359 363 385 387 408 421 425 427\n",
      " 431 444 447 456 460 463 481 490 499 500 504 506 509 510 511 519 553 567\n",
      " 570 575 581 616 625 632 635 642 644 651 656 667 669 678 680 681 689 704\n",
      " 705 726 732 734 735 748 751 762 777 828 829 830 835 840 849 852 855 867\n",
      " 871 872 887 898 923 927 934 935 936 951 953]\n",
      "Train: [  0   1   2   3   4   5   6   8   9  10  11  12  13  14  15  16  18  19\n",
      "  20  21  22  24  25  26  27  28  29  30  31  33  34  35  36  37  38  39\n",
      "  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54  56  57  58\n",
      "  60  61  64  65  66  67  68  69  71  72  73  74  75  76  79  80  81  82\n",
      "  83  84  85  86  87  88  89  90  91  92  93  94  95  96  97  98  99 100\n",
      " 101 102 103 104 105 106 108 109 110 111 112 113 114 115 116 118 119 120\n",
      " 122 123 125 126 127 128 129 130 131 133 134 135 136 137 138 139 140 141\n",
      " 142 143 144 145 146 147 148 149 150 151 152 154 155 156 157 158 159 160\n",
      " 161 162 163 164 165 166 167 168 169 170 171 173 175 177 178 179 180 181\n",
      " 182 183 185 186 187 188 190 191 192 193 194 195 196 197 199 200 202 203\n",
      " 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221\n",
      " 222 223 224 225 226 227 228 229 230 231 233 236 237 238 239 240 242 243\n",
      " 244 245 246 248 249 250 251 252 253 254 255 256 257 258 261 262 263 264\n",
      " 265 267 268 269 270 271 273 275 276 277 280 281 282 283 284 286 288 289\n",
      " 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 310\n",
      " 311 313 314 315 316 317 319 320 321 322 323 324 325 326 328 329 330 331\n",
      " 332 333 334 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350\n",
      " 351 352 353 354 355 356 357 358 359 361 362 363 366 367 368 369 370 371\n",
      " 372 373 374 375 376 377 378 380 382 384 385 386 387 388 390 391 393 394\n",
      " 396 397 398 399 400 401 402 403 405 406 407 408 409 410 411 412 413 414\n",
      " 415 417 418 419 421 423 424 425 426 427 428 429 430 431 432 433 434 435\n",
      " 436 438 439 441 442 443 444 447 448 449 450 451 452 453 454 455 456 459\n",
      " 460 461 462 463 464 466 467 468 469 470 471 472 473 474 476 477 478 479\n",
      " 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 498 499\n",
      " 500 501 502 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518\n",
      " 519 520 521 523 525 526 527 528 529 530 531 532 533 534 536 537 538 539\n",
      " 540 541 542 543 544 545 547 548 549 550 551 552 553 555 556 558 559 560\n",
      " 561 562 563 564 566 567 568 569 570 571 572 573 574 575 576 577 578 579\n",
      " 580 581 583 584 585 586 587 588 589 590 591 592 593 594 595 596 598 599\n",
      " 601 602 603 604 605 606 607 608 609 610 611 613 614 615 616 617 619 620\n",
      " 621 622 623 624 625 626 627 628 629 630 632 633 634 635 636 637 638 639\n",
      " 640 641 642 643 644 645 646 647 648 649 650 651 653 655 656 657 658 659\n",
      " 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677\n",
      " 678 679 680 681 682 684 685 686 687 688 689 690 691 693 694 695 696 697\n",
      " 699 700 701 702 703 704 705 708 709 711 712 713 714 715 716 717 718 719\n",
      " 720 722 723 724 725 726 727 728 729 730 732 733 734 735 736 737 738 739\n",
      " 740 741 742 743 744 745 746 747 748 749 750 751 752 753 755 756 757 758\n",
      " 759 760 762 763 767 768 769 770 771 772 773 774 775 776 777 778 779 780\n",
      " 781 782 783 786 787 788 789 791 792 793 794 795 796 797 798 799 800 801\n",
      " 802 803 804 805 806 807 808 809 810 811 812 814 815 816 817 818 819 820\n",
      " 821 822 823 825 826 827 828 829 830 831 832 833 834 835 836 837 838 839\n",
      " 840 841 842 843 844 845 846 847 848 849 850 851 852 853 855 856 857 858\n",
      " 861 862 863 864 865 866 867 869 870 871 872 874 875 876 877 878 879 880\n",
      " 882 883 884 885 886 887 888 889 890 891 892 893 894 897 898 899 900 901\n",
      " 902 903 904 905 906 907 908 909 910 911 913 915 916 917 918 919 920 921\n",
      " 922 923 924 925 927 928 929 930 931 932 934 935 936 937 938 940 941 942\n",
      " 944 946 947 948 950 951 952 953 955 956 957]\n",
      "Test:  [  7  17  23  32  55  59  62  63  70  77  78 107 117 121 124 132 153 172\n",
      " 174 176 184 189 198 201 232 234 235 241 247 259 260 266 272 274 278 279\n",
      " 285 287 290 308 309 312 318 327 335 360 364 365 379 381 383 389 392 395\n",
      " 404 416 420 422 437 440 445 446 457 458 465 475 480 497 503 522 524 535\n",
      " 546 554 557 565 582 597 600 612 618 631 652 654 683 692 698 706 707 710\n",
      " 721 731 754 761 764 765 766 784 785 790 813 824 854 859 860 868 873 881\n",
      " 895 896 912 914 926 933 939 943 945 949 954]\n",
      "=========================================================================\n",
      "==============SKLEARN VALIDACIÓN SIMPLE 75% GERMAN DATA==================\n",
      "TRAIN:\n",
      " [[ 1. 15.  2. ...  1.  0.  0.]\n",
      " [ 3. 24.  4. ...  1.  0.  0.]\n",
      " [ 3.  6.  3. ...  1.  0.  0.]\n",
      " ...\n",
      " [ 0. 12.  4. ...  1.  0.  1.]\n",
      " [ 1. 18.  4. ...  1.  0.  0.]\n",
      " [ 3. 12.  2. ...  1.  1.  0.]]\n",
      "TEST:\n",
      " [[ 0. 18.  4. ...  1.  1.  0.]\n",
      " [ 2. 12.  2. ...  1.  0.  0.]\n",
      " [ 0. 12.  2. ...  2.  1.  0.]\n",
      " ...\n",
      " [ 0. 36.  4. ...  1.  1.  0.]\n",
      " [ 0.  6.  4. ...  1.  0.  1.]\n",
      " [ 3. 10.  2. ...  2.  0.  1.]]\n",
      "==============SKLEARN VALIDACIÓN CRUZADA K=4 GERMAN DATA=================\n",
      "Train: [  0   1   2   3   5   6   7   8   9  11  12  13  14  16  17  18  19  20\n",
      "  22  23  24  25  26  27  28  29  30  31  32  34  37  39  40  41  45  46\n",
      "  48  49  50  51  52  53  54  55  56  57  58  59  60  61  62  63  64  65\n",
      "  66  67  68  69  70  71  72  74  75  77  78  79  80  82  84  85  86  87\n",
      "  88  91  92  93  94  96  97 101 102 103 104 105 110 112 114 115 116 117\n",
      " 118 119 120 121 123 124 126 127 128 129 130 131 132 133 134 135 137 138\n",
      " 141 142 143 144 145 147 148 149 150 151 152 154 156 157 158 159 160 161\n",
      " 162 163 164 165 166 167 169 171 172 173 174 175 176 177 178 179 182 184\n",
      " 186 188 190 191 192 194 198 199 200 201 202 203 204 205 206 207 208 210\n",
      " 211 212 214 216 217 218 219 220 222 223 224 225 226 227 228 229 230 233\n",
      " 234 235 236 237 238 240 242 243 244 247 248 252 254 255 256 257 259 261\n",
      " 262 263 264 266 267 268 269 271 272 274 275 276 277 279 280 282 283 284\n",
      " 285 287 288 289 290 293 295 296 298 299 300 301 302 304 305 306 307 310\n",
      " 311 312 314 315 316 317 319 320 321 322 323 324 325 326 328 330 332 333\n",
      " 335 336 337 338 339 340 341 342 343 344 345 346 347 349 350 351 352 353\n",
      " 354 355 357 358 359 360 361 363 364 365 367 369 370 371 372 373 374 375\n",
      " 376 378 379 380 381 382 383 384 385 386 387 388 389 390 391 393 394 395\n",
      " 396 398 399 401 402 404 407 410 411 412 413 414 415 416 418 419 420 421\n",
      " 423 425 426 429 430 431 433 434 435 437 438 439 440 441 442 443 444 446\n",
      " 447 448 449 450 452 453 454 455 456 457 459 460 461 463 464 465 466 468\n",
      " 469 470 471 472 473 474 475 476 477 478 479 480 483 485 487 488 490 493\n",
      " 494 495 496 497 498 499 502 503 504 505 509 510 511 512 513 515 516 517\n",
      " 518 520 521 522 523 526 527 530 531 532 533 534 535 537 541 544 545 546\n",
      " 547 548 549 550 552 553 555 556 557 559 560 561 562 564 565 566 567 568\n",
      " 569 570 571 574 576 577 579 581 582 583 584 586 587 588 590 591 592 593\n",
      " 595 597 599 604 605 606 608 609 610 611 615 616 617 618 619 620 622 624\n",
      " 625 626 627 628 629 630 631 632 633 635 637 640 642 643 644 645 646 647\n",
      " 649 650 652 653 654 656 657 658 659 660 661 664 665 666 667 668 670 672\n",
      " 673 674 675 677 679 680 681 682 683 684 685 686 687 688 689 690 694 696\n",
      " 697 698 699 701 704 706 707 708 709 710 711 712 713 714 715 716 718 719\n",
      " 721 722 724 725 726 728 729 730 731 732 733 734 735 736 737 738 739 740\n",
      " 741 742 743 744 745 746 747 748 749 751 753 754 755 756 757 759 760 761\n",
      " 762 763 764 765 766 767 768 769 770 771 773 774 775 777 779 781 782 783\n",
      " 784 785 788 789 790 792 793 794 796 801 802 803 804 805 806 807 808 809\n",
      " 810 811 812 813 814 816 817 818 820 822 823 824 825 829 830 831 832 833\n",
      " 834 835 836 838 839 840 841 844 846 847 848 851 852 853 854 855 857 858\n",
      " 860 861 862 863 864 865 866 868 869 870 871 872 874 875 876 877 879 880\n",
      " 881 882 883 887 888 889 890 891 892 894 897 898 899 900 903 904 905 906\n",
      " 907 908 909 910 911 913 914 915 916 918 919 920 922 923 924 925 926 927\n",
      " 928 929 930 931 932 933 934 935 936 937 938 939 940 942 943 944 945 947\n",
      " 948 951 952 953 955 956 957 960 961 962 963 965 966 968 969 970 971 976\n",
      " 977 979 980 981 983 985 986 987 988 990 993 996]\n",
      "Test:  [  4  10  15  21  33  35  36  38  42  43  44  47  73  76  81  83  89  90\n",
      "  95  98  99 100 106 107 108 109 111 113 122 125 136 139 140 146 153 155\n",
      " 168 170 180 181 183 185 187 189 193 195 196 197 209 213 215 221 231 232\n",
      " 239 241 245 246 249 250 251 253 258 260 265 270 273 278 281 286 291 292\n",
      " 294 297 303 308 309 313 318 327 329 331 334 348 356 362 366 368 377 392\n",
      " 397 400 403 405 406 408 409 417 422 424 427 428 432 436 445 451 458 462\n",
      " 467 481 482 484 486 489 491 492 500 501 506 507 508 514 519 524 525 528\n",
      " 529 536 538 539 540 542 543 551 554 558 563 572 573 575 578 580 585 589\n",
      " 594 596 598 600 601 602 603 607 612 613 614 621 623 634 636 638 639 641\n",
      " 648 651 655 662 663 669 671 676 678 691 692 693 695 700 702 703 705 717\n",
      " 720 723 727 750 752 758 772 776 778 780 786 787 791 795 797 798 799 800\n",
      " 815 819 821 826 827 828 837 842 843 845 849 850 856 859 867 873 878 884\n",
      " 885 886 893 895 896 901 902 912 917 921 941 946 949 950 954 958 959 964\n",
      " 967 972 973 974 975 978 982 984 989 991 992 994 995 997 998 999]\n",
      "Train: [  1   4   5   6   7   8   9  10  11  13  15  16  17  19  20  21  23  24\n",
      "  26  27  28  29  30  31  32  33  34  35  36  37  38  39  40  42  43  44\n",
      "  45  47  49  51  53  54  57  58  59  61  64  66  68  70  73  74  75  76\n",
      "  78  79  80  81  83  85  87  89  90  91  92  95  96  97  98  99 100 101\n",
      " 103 106 107 108 109 111 112 113 114 115 117 118 119 120 122 125 126 127\n",
      " 128 129 130 132 133 135 136 137 138 139 140 141 143 144 146 148 151 152\n",
      " 153 154 155 156 160 161 163 164 165 166 168 169 170 171 172 173 174 177\n",
      " 178 179 180 181 183 184 185 186 187 188 189 190 191 192 193 194 195 196\n",
      " 197 202 203 204 205 206 207 208 209 210 211 212 213 215 217 218 219 221\n",
      " 222 223 224 226 227 229 230 231 232 233 234 236 239 240 241 243 244 245\n",
      " 246 247 248 249 250 251 252 253 257 258 260 261 262 263 264 265 266 267\n",
      " 268 269 270 272 273 274 276 277 278 281 282 283 284 285 286 287 288 289\n",
      " 291 292 293 294 295 296 297 298 299 301 303 304 305 306 307 308 309 310\n",
      " 311 312 313 314 315 316 317 318 319 321 322 324 326 327 328 329 330 331\n",
      " 332 333 334 335 337 338 343 344 345 346 347 348 350 351 352 353 355 356\n",
      " 359 360 361 362 363 364 365 366 367 368 370 371 372 377 378 379 381 384\n",
      " 385 386 387 388 391 392 393 394 395 396 397 398 399 400 401 402 403 404\n",
      " 405 406 407 408 409 411 413 414 417 422 423 424 425 426 427 428 429 432\n",
      " 434 435 436 437 438 443 444 445 446 447 448 449 451 453 454 455 457 458\n",
      " 460 461 462 463 464 465 467 468 469 470 471 473 474 477 478 479 480 481\n",
      " 482 483 484 485 486 489 490 491 492 493 496 498 499 500 501 502 504 505\n",
      " 506 507 508 509 511 514 515 516 517 518 519 524 525 526 527 528 529 530\n",
      " 532 533 535 536 538 539 540 542 543 544 545 547 549 550 551 554 555 556\n",
      " 557 558 559 560 561 562 563 565 566 568 569 571 572 573 575 577 578 580\n",
      " 581 583 585 586 588 589 590 591 592 593 594 595 596 597 598 599 600 601\n",
      " 602 603 604 605 607 608 610 612 613 614 615 617 618 619 621 622 623 624\n",
      " 628 632 633 634 635 636 637 638 639 640 641 642 643 645 646 647 648 649\n",
      " 650 651 652 653 655 656 658 659 660 662 663 665 666 667 668 669 670 671\n",
      " 672 673 675 676 677 678 682 684 687 690 691 692 693 694 695 696 697 698\n",
      " 699 700 702 703 705 706 709 711 712 713 714 717 718 719 720 722 723 725\n",
      " 727 729 730 731 733 734 735 736 737 738 740 741 742 744 745 746 747 748\n",
      " 749 750 752 755 756 757 758 759 760 762 765 768 769 770 771 772 773 774\n",
      " 776 777 778 779 780 781 783 784 785 786 787 789 790 791 793 795 797 798\n",
      " 799 800 802 803 804 805 808 810 811 813 815 816 818 819 821 823 824 825\n",
      " 826 827 828 831 834 835 837 838 839 840 842 843 844 845 846 848 849 850\n",
      " 852 853 855 856 858 859 861 862 863 867 868 869 870 872 873 874 876 877\n",
      " 878 880 881 883 884 885 886 887 888 889 890 892 893 894 895 896 897 898\n",
      " 899 900 901 902 903 905 906 907 908 909 910 912 913 914 915 916 917 918\n",
      " 920 921 924 925 926 928 929 930 932 933 934 935 937 938 940 941 942 944\n",
      " 946 947 948 949 950 951 952 954 955 957 958 959 960 961 963 964 965 966\n",
      " 967 968 969 970 972 973 974 975 976 977 978 979 980 981 982 983 984 985\n",
      " 986 987 989 990 991 992 993 994 995 997 998 999]\n",
      "Test:  [  0   2   3  12  14  18  22  25  41  46  48  50  52  55  56  60  62  63\n",
      "  65  67  69  71  72  77  82  84  86  88  93  94 102 104 105 110 116 121\n",
      " 123 124 131 134 142 145 147 149 150 157 158 159 162 167 175 176 182 198\n",
      " 199 200 201 214 216 220 225 228 235 237 238 242 254 255 256 259 271 275\n",
      " 279 280 290 300 302 320 323 325 336 339 340 341 342 349 354 357 358 369\n",
      " 373 374 375 376 380 382 383 389 390 410 412 415 416 418 419 420 421 430\n",
      " 431 433 439 440 441 442 450 452 456 459 466 472 475 476 487 488 494 495\n",
      " 497 503 510 512 513 520 521 522 523 531 534 537 541 546 548 552 553 564\n",
      " 567 570 574 576 579 582 584 587 606 609 611 616 620 625 626 627 629 630\n",
      " 631 644 654 657 661 664 674 679 680 681 683 685 686 688 689 701 704 707\n",
      " 708 710 715 716 721 724 726 728 732 739 743 751 753 754 761 763 764 766\n",
      " 767 775 782 788 792 794 796 801 806 807 809 812 814 817 820 822 829 830\n",
      " 832 833 836 841 847 851 854 857 860 864 865 866 871 875 879 882 891 904\n",
      " 911 919 922 923 927 931 936 939 943 945 953 956 962 971 988 996]\n",
      "Train: [  0   2   3   4   6   7  10  12  13  14  15  18  19  21  22  23  24  25\n",
      "  27  29  30  33  34  35  36  37  38  39  40  41  42  43  44  46  47  48\n",
      "  49  50  52  54  55  56  58  59  60  62  63  65  66  67  69  71  72  73\n",
      "  75  76  77  78  79  80  81  82  83  84  86  87  88  89  90  91  93  94\n",
      "  95  98  99 100 102 103 104 105 106 107 108 109 110 111 112 113 115 116\n",
      " 117 118 120 121 122 123 124 125 126 130 131 132 134 136 137 138 139 140\n",
      " 142 143 145 146 147 148 149 150 151 153 154 155 156 157 158 159 160 161\n",
      " 162 167 168 170 173 175 176 179 180 181 182 183 185 186 187 188 189 190\n",
      " 191 192 193 194 195 196 197 198 199 200 201 202 206 209 212 213 214 215\n",
      " 216 218 220 221 224 225 227 228 231 232 235 236 237 238 239 240 241 242\n",
      " 244 245 246 247 249 250 251 252 253 254 255 256 257 258 259 260 261 262\n",
      " 265 267 268 269 270 271 273 274 275 278 279 280 281 283 285 286 288 290\n",
      " 291 292 294 295 297 298 299 300 302 303 304 306 307 308 309 313 314 315\n",
      " 316 318 320 321 322 323 325 326 327 329 330 331 333 334 335 336 339 340\n",
      " 341 342 345 346 348 349 351 354 355 356 357 358 362 365 366 368 369 372\n",
      " 373 374 375 376 377 378 380 382 383 384 389 390 392 393 394 396 397 400\n",
      " 403 404 405 406 408 409 410 411 412 415 416 417 418 419 420 421 422 424\n",
      " 425 426 427 428 430 431 432 433 434 435 436 438 439 440 441 442 444 445\n",
      " 446 450 451 452 454 455 456 458 459 460 461 462 463 464 465 466 467 469\n",
      " 470 472 474 475 476 480 481 482 483 484 486 487 488 489 490 491 492 493\n",
      " 494 495 497 500 501 503 504 505 506 507 508 510 511 512 513 514 516 517\n",
      " 519 520 521 522 523 524 525 527 528 529 531 533 534 536 537 538 539 540\n",
      " 541 542 543 545 546 548 550 551 552 553 554 555 556 558 559 563 564 566\n",
      " 567 570 571 572 573 574 575 576 578 579 580 581 582 583 584 585 586 587\n",
      " 588 589 593 594 595 596 597 598 600 601 602 603 604 605 606 607 609 611\n",
      " 612 613 614 615 616 617 619 620 621 622 623 624 625 626 627 629 630 631\n",
      " 632 633 634 636 637 638 639 641 644 646 647 648 649 650 651 654 655 657\n",
      " 658 659 661 662 663 664 667 669 670 671 673 674 675 676 678 679 680 681\n",
      " 682 683 684 685 686 687 688 689 690 691 692 693 694 695 698 699 700 701\n",
      " 702 703 704 705 707 708 709 710 712 713 715 716 717 718 719 720 721 723\n",
      " 724 725 726 727 728 731 732 735 736 737 738 739 740 743 744 746 747 748\n",
      " 750 751 752 753 754 756 757 758 760 761 763 764 765 766 767 769 771 772\n",
      " 773 775 776 778 779 780 781 782 783 786 787 788 789 790 791 792 794 795\n",
      " 796 797 798 799 800 801 803 804 805 806 807 808 809 810 811 812 814 815\n",
      " 816 817 818 819 820 821 822 823 826 827 828 829 830 832 833 834 836 837\n",
      " 838 839 840 841 842 843 845 846 847 848 849 850 851 854 856 857 858 859\n",
      " 860 861 862 864 865 866 867 868 871 873 875 878 879 882 884 885 886 889\n",
      " 890 891 892 893 895 896 898 899 900 901 902 903 904 905 907 908 911 912\n",
      " 913 916 917 918 919 920 921 922 923 925 926 927 929 931 932 933 936 937\n",
      " 938 939 940 941 942 943 945 946 949 950 951 953 954 955 956 958 959 961\n",
      " 962 963 964 965 966 967 971 972 973 974 975 976 977 978 979 980 982 983\n",
      " 984 988 989 990 991 992 994 995 996 997 998 999]\n",
      "Test:  [  1   5   8   9  11  16  17  20  26  28  31  32  45  51  53  57  61  64\n",
      "  68  70  74  85  92  96  97 101 114 119 127 128 129 133 135 141 144 152\n",
      " 163 164 165 166 169 171 172 174 177 178 184 203 204 205 207 208 210 211\n",
      " 217 219 222 223 226 229 230 233 234 243 248 263 264 266 272 276 277 282\n",
      " 284 287 289 293 296 301 305 310 311 312 317 319 324 328 332 337 338 343\n",
      " 344 347 350 352 353 359 360 361 363 364 367 370 371 379 381 385 386 387\n",
      " 388 391 395 398 399 401 402 407 413 414 423 429 437 443 447 448 449 453\n",
      " 457 468 471 473 477 478 479 485 496 498 499 502 509 515 518 526 530 532\n",
      " 535 544 547 549 557 560 561 562 565 568 569 577 590 591 592 599 608 610\n",
      " 618 628 635 640 642 643 645 652 653 656 660 665 666 668 672 677 696 697\n",
      " 706 711 714 722 729 730 733 734 741 742 745 749 755 759 762 768 770 774\n",
      " 777 784 785 793 802 813 824 825 831 835 844 852 853 855 863 869 870 872\n",
      " 874 876 877 880 881 883 887 888 894 897 906 909 910 914 915 924 928 930\n",
      " 934 935 944 947 948 952 957 960 968 969 970 981 985 986 987 993]\n",
      "Train: [  0   1   2   3   4   5   8   9  10  11  12  14  15  16  17  18  20  21\n",
      "  22  25  26  28  31  32  33  35  36  38  41  42  43  44  45  46  47  48\n",
      "  50  51  52  53  55  56  57  60  61  62  63  64  65  67  68  69  70  71\n",
      "  72  73  74  76  77  81  82  83  84  85  86  88  89  90  92  93  94  95\n",
      "  96  97  98  99 100 101 102 104 105 106 107 108 109 110 111 113 114 116\n",
      " 119 121 122 123 124 125 127 128 129 131 133 134 135 136 139 140 141 142\n",
      " 144 145 146 147 149 150 152 153 155 157 158 159 162 163 164 165 166 167\n",
      " 168 169 170 171 172 174 175 176 177 178 180 181 182 183 184 185 187 189\n",
      " 193 195 196 197 198 199 200 201 203 204 205 207 208 209 210 211 213 214\n",
      " 215 216 217 219 220 221 222 223 225 226 228 229 230 231 232 233 234 235\n",
      " 237 238 239 241 242 243 245 246 248 249 250 251 253 254 255 256 258 259\n",
      " 260 263 264 265 266 270 271 272 273 275 276 277 278 279 280 281 282 284\n",
      " 286 287 289 290 291 292 293 294 296 297 300 301 302 303 305 308 309 310\n",
      " 311 312 313 317 318 319 320 323 324 325 327 328 329 331 332 334 336 337\n",
      " 338 339 340 341 342 343 344 347 348 349 350 352 353 354 356 357 358 359\n",
      " 360 361 362 363 364 366 367 368 369 370 371 373 374 375 376 377 379 380\n",
      " 381 382 383 385 386 387 388 389 390 391 392 395 397 398 399 400 401 402\n",
      " 403 405 406 407 408 409 410 412 413 414 415 416 417 418 419 420 421 422\n",
      " 423 424 427 428 429 430 431 432 433 436 437 439 440 441 442 443 445 447\n",
      " 448 449 450 451 452 453 456 457 458 459 462 466 467 468 471 472 473 475\n",
      " 476 477 478 479 481 482 484 485 486 487 488 489 491 492 494 495 496 497\n",
      " 498 499 500 501 502 503 506 507 508 509 510 512 513 514 515 518 519 520\n",
      " 521 522 523 524 525 526 528 529 530 531 532 534 535 536 537 538 539 540\n",
      " 541 542 543 544 546 547 548 549 551 552 553 554 557 558 560 561 562 563\n",
      " 564 565 567 568 569 570 572 573 574 575 576 577 578 579 580 582 584 585\n",
      " 587 589 590 591 592 594 596 598 599 600 601 602 603 606 607 608 609 610\n",
      " 611 612 613 614 616 618 620 621 623 625 626 627 628 629 630 631 634 635\n",
      " 636 638 639 640 641 642 643 644 645 648 651 652 653 654 655 656 657 660\n",
      " 661 662 663 664 665 666 668 669 671 672 674 676 677 678 679 680 681 683\n",
      " 685 686 688 689 691 692 693 695 696 697 700 701 702 703 704 705 706 707\n",
      " 708 710 711 714 715 716 717 720 721 722 723 724 726 727 728 729 730 732\n",
      " 733 734 739 741 742 743 745 749 750 751 752 753 754 755 758 759 761 762\n",
      " 763 764 766 767 768 770 772 774 775 776 777 778 780 782 784 785 786 787\n",
      " 788 791 792 793 794 795 796 797 798 799 800 801 802 806 807 809 812 813\n",
      " 814 815 817 819 820 821 822 824 825 826 827 828 829 830 831 832 833 835\n",
      " 836 837 841 842 843 844 845 847 849 850 851 852 853 854 855 856 857 859\n",
      " 860 863 864 865 866 867 869 870 871 872 873 874 875 876 877 878 879 880\n",
      " 881 882 883 884 885 886 887 888 891 893 894 895 896 897 901 902 904 906\n",
      " 909 910 911 912 914 915 917 919 921 922 923 924 927 928 930 931 934 935\n",
      " 936 939 941 943 944 945 946 947 948 949 950 952 953 954 956 957 958 959\n",
      " 960 962 964 967 968 969 970 971 972 973 974 975 978 981 982 984 985 986\n",
      " 987 988 989 991 992 993 994 995 996 997 998 999]\n",
      "Test:  [  6   7  13  19  23  24  27  29  30  34  37  39  40  49  54  58  59  66\n",
      "  75  78  79  80  87  91 103 112 115 117 118 120 126 130 132 137 138 143\n",
      " 148 151 154 156 160 161 173 179 186 188 190 191 192 194 202 206 212 218\n",
      " 224 227 236 240 244 247 252 257 261 262 267 268 269 274 283 285 288 295\n",
      " 298 299 304 306 307 314 315 316 321 322 326 330 333 335 345 346 351 355\n",
      " 365 372 378 384 393 394 396 404 411 425 426 434 435 438 444 446 454 455\n",
      " 460 461 463 464 465 469 470 474 480 483 490 493 504 505 511 516 517 527\n",
      " 533 545 550 555 556 559 566 571 581 583 586 588 593 595 597 604 605 615\n",
      " 617 619 622 624 632 633 637 646 647 649 650 658 659 667 670 673 675 682\n",
      " 684 687 690 694 698 699 709 712 713 718 719 725 731 735 736 737 738 740\n",
      " 744 746 747 748 756 757 760 765 769 771 773 779 781 783 789 790 803 804\n",
      " 805 808 810 811 816 818 823 834 838 839 840 846 848 858 861 862 868 889\n",
      " 890 892 898 899 900 903 905 907 908 913 916 918 920 925 926 929 932 933\n",
      " 937 938 940 942 951 955 961 963 965 966 976 977 979 980 983 990]\n",
      "=========================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"==============SKLEARN VALIDACIÓN SIMPLE 70% LENSES DATA==================\")\n",
    "x_train, x_test, y_train, y_test = validacion_simple_sklearn(dataset, 0.7)\n",
    "print(\"TRAIN:\\n\", x_train)\n",
    "print(\"TEST:\\n\", x_test)\n",
    "print(\"==============SKLEARN VALIDACIÓN CRUZADA K=5 LENSES DATA=================\")\n",
    "particiones = validacion_cruzada_sklearn(dataset,5)\n",
    "for particion in particiones:\n",
    "    print(particion)\n",
    "print(\"=========================================================================\")\n",
    "print(\"==============SKLEARN VALIDACIÓN SIMPLE 80% TIC-TAC-TOE DATA=============\")\n",
    "x_train1, x_test1, y_train1, y_test1 = validacion_simple_sklearn(dataset2, 0.8)\n",
    "print(\"TRAIN:\\n\", x_train1)\n",
    "print(\"TEST:\\n\", x_test1)\n",
    "print(\"==============SKLEARN VALIDACIÓN CRUZADA K=8 TIC-TAC-TOE DATA============\")\n",
    "particiones = validacion_cruzada_sklearn(dataset2,8)\n",
    "for particion in particiones:\n",
    "    print(particion)\n",
    "print(\"=========================================================================\")\n",
    "print(\"==============SKLEARN VALIDACIÓN SIMPLE 75% GERMAN DATA==================\")\n",
    "x_train2, x_test2, y_train2, y_test2 = validacion_simple_sklearn(dataset3, 0.75)\n",
    "print(\"TRAIN:\\n\", x_train2)\n",
    "print(\"TEST:\\n\", x_test2)\n",
    "print(\"==============SKLEARN VALIDACIÓN CRUZADA K=4 GERMAN DATA=================\")\n",
    "particiones = validacion_cruzada_sklearn(dataset3,4)\n",
    "for particion in particiones:\n",
    "    print(particion)\n",
    "print(\"=========================================================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Clasificador Naive-Bayes con Sklearn</h3>\n",
    "<p>Ahora vamos a realizar pruebas con la implementación que nos da sklearn del clasificador de Naive-Bayes, anteriormente hemos realizado las diferentes estrategias de particionado, por lo tanto, ahora solo nos falta introducir esos subconjuntos a los métodos que hemos creado para tener la implementacion de sklearn y que la libreria se encargue de hacer el entrenamiento y la clasificación del conjunto de datos. A continuación, explicaremos brevemente cada uno de los métodos que hemos creado.</p>\n",
    "<p>El método <strong>nb_sklearn</strong>: en este método introducimos como parametro el subconjunto de datos de entrenamiento y de clasificacion, el tipo que queremos calcular y si se va a utilizar la regla de Laplace, donde el tipo va estar predefinido a Multinominal y laplace va estar definido a True. Este método va a devolver la predicción de las clases.</p>\n",
    "<p>El método <strong>error</strong>: este método va a calcular los errores que vamos a obtener el pporcentaje de error que hemos obtenido con el clasificador Naive-Bayes.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validacion_simple_sklearn(dataset, porcentaje):\n",
    "\n",
    "    # Matriz con los atributos\n",
    "    X = dataset.datos[:, :-1]\n",
    "\n",
    "    # Array con las clases\n",
    "    y = dataset.datos[:, -1]\n",
    "\n",
    "    # Realizamos la divison en train-test, X_train es la partición sobre la que se va a entrenar e X_test sobre la que se va a clasificar\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=porcentaje, test_size=1 - porcentaje, shuffle=True)\n",
    "\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "def validacion_cruzada_sklearn(dataset, k):\n",
    "\n",
    "    # Matriz con los atributos\n",
    "    X = dataset.datos[:, :-1]\n",
    "\n",
    "    # Array con las clases\n",
    "    y = dataset.datos[:, -1]\n",
    "\n",
    "    kf = KFold(n_splits=k, shuffle=True)\n",
    "\n",
    "    particiones = []\n",
    "\n",
    "    for train_index, test_index in kf.split(X,y):\n",
    "        particiones.append(Particion(train_index,test_index))\n",
    "\n",
    "    return particiones\n",
    "\n",
    "def nb_sklearn(x_train, y_train, x_test, tipo=\"Multinomial\", laplace=True):\n",
    "\n",
    "    if tipo == \"Gaussian\":\n",
    "        if laplace == True:\n",
    "            clf = GaussianNB(alpha=1.0)\n",
    "        else:\n",
    "            clf = GaussianNB()\n",
    "\n",
    "    elif tipo == \"Multinomial\":\n",
    "        if laplace == True:\n",
    "            clf = MultinomialNB(alpha=1.0, fit_prior = True, class_prior = None)\n",
    "        else:\n",
    "            clf = MultinomialNB(fit_prior=True, class_prior=False)\n",
    "    else:\n",
    "        print(\"Error, clasificador no valido. Utilizar GaussianNB o MultinomialNB\")\n",
    "        return\n",
    "\n",
    "    # Entrenamos el modelo\n",
    "    clf.fit(x_train, y_train)\n",
    "\n",
    "    # Clasificacion\n",
    "    prediccion = clf.predict(x_test)\n",
    "\n",
    "    return prediccion\n",
    "\n",
    "def nb_sklearn_validacion_cruzada(x_train, y_train, k):\n",
    "\n",
    "    clf = MultinomialNB(alpha = 1.0, fit_prior = True, class_prior = None)\n",
    "\n",
    "    error = cross_val_score(clf, x_train, y_train, cv = k)\n",
    "\n",
    "    return error\n",
    "\n",
    "\n",
    "\n",
    "def error(clases_predichas, clases_reales):\n",
    "\n",
    "    return 1 - np.sum(np.equal(clases_predichas, clases_reales)) / len(clases_predichas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============SKLEARN NAIVE-BAYES LENSES DATA==================\n",
      "==============CON LAPLACE Y VALIDACION SIMPLE==================\n",
      "ERROR OBTENIDO: 1.0\n",
      "==============SIN LAPLACE Y VALIDACION SIMPLE==================\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "object of type 'bool' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-4fcd5cd9dddf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"==============SIN LAPLACE Y VALIDACION SIMPLE==================\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidacion_simple_sklearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.7\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnb_sklearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Multinomial\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0merrornb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ERROR OBTENIDO:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrornb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-38-1f4318f6bd35>\u001b[0m in \u001b[0;36mnb_sklearn\u001b[0;34m(x_train, y_train, x_test, tipo, laplace)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;31m# Entrenamos el modelo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;31m# Clasificacion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/naive_bayes.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    614\u001b[0m         \u001b[0malpha\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_alpha\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    615\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_feature_log_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 616\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_class_log_prior\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclass_prior\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_prior\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    617\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/naive_bayes.py\u001b[0m in \u001b[0;36m_update_class_log_prior\u001b[0;34m(self, class_prior)\u001b[0m\n\u001b[1;32m    456\u001b[0m         \u001b[0mn_classes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    457\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mclass_prior\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 458\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclass_prior\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mn_classes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    459\u001b[0m                 raise ValueError(\"Number of priors must match number of\"\n\u001b[1;32m    460\u001b[0m                                  \" classes.\")\n",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'bool' has no len()"
     ]
    }
   ],
   "source": [
    "print(\"==============SKLEARN NAIVE-BAYES LENSES DATA==================\")\n",
    "print(\"==============CON LAPLACE Y VALIDACION SIMPLE==================\")\n",
    "x_train, x_test, y_train, y_test = validacion_simple_sklearn(dataset, 0.7)\n",
    "pred = nb_sklearn(x_train, y_train, x_test)\n",
    "errornb = error(pred, x_test[:,-1])\n",
    "print(\"ERROR OBTENIDO:\", errornb)\n",
    "print(\"==============SIN LAPLACE Y VALIDACION SIMPLE==================\")\n",
    "x_train, x_test, y_train, y_test = validacion_simple_sklearn(dataset, 0.7)\n",
    "pred = nb_sklearn(x_train, y_train, x_test, \"Multinomial\",False)\n",
    "errornb = error(pred, x_test[:,-1])\n",
    "print(\"ERROR OBTENIDO:\", errornb)\n",
    "print(\"==============SKLEARN NAIVE-BAYES TIC-TAC-TOE DATA=============\")\n",
    "print(\"==============CON LAPLACE Y VALIDACION SIMPLE==================\")\n",
    "x_train2, x_test2, y_train2, y_test2 = validacion_simple_sklearn(dataset2, 0.8)\n",
    "pred = nb_sklearn(x_train2, y_train2, x_test2)\n",
    "errornb2 = error(pred, x_test2[:,-1])\n",
    "print(\"ERROR OBTENIDO:\", errornb)\n",
    "print(\"==============SIN LAPLACE Y VALIDACION SIMPLE==================\")\n",
    "pred = nb_sklearn(x_train2, y_train2, x_test2, \"Multinomial\",False)\n",
    "errornb2 = error(pred, x_test2[:,-1])\n",
    "print(\"ERROR OBTENIDO:\", errornb)\n",
    "print(\"==============SKLEARN NAIVE-BAYES GERMAN DATA=============\")\n",
    "print(\"==============CON LAPLACE Y VALIDACION SIMPLE==================\")\n",
    "x_train3, x_test3, y_train3, y_test3 = validacion_simple_sklearn(dataset3, 0.75)\n",
    "pred = nb_sklearn(x_train3, y_train3, x_test3)\n",
    "errornb2 = error(pred, x_test3[:,-1])\n",
    "print(\"ERROR OBTENIDO:\", errornb)\n",
    "print(\"==============SIN LAPLACE Y VALIDACION SIMPLE==================\")\n",
    "pred = nb_sklearn(x_train3, y_train3, x_test3, \"Multinomial\",False)\n",
    "errornb2 = error(pred, x_test3[:,-1])\n",
    "print(\"ERROR OBTENIDO:\", errornb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Apartado 4:Evaluación de hipótesis mediante Análisis ROC</h3>\n",
    "<p>La curva ROC es una representación gráfica de la sensibilidad a la especifidad de un clasificador, en esta práctica este análisis lo vamos a realizar del clasificador implementado que es Naive-Bayes. En este gráfico se representan los verdaderos postivos frente a los falsos positivos.</p>\n",
    "<p>Es una herramienta que nos proporciona la selección de modelos más óptimos y descartar los menos óptimos.</p>\n",
    "<p>A continuación, mostraremos la implementación que hemos realizado en para crear este análisis ROC:</p>\n",
    "<ol>\n",
    "    <li>El primer paso es crear la <strong> matriz de confusión</strong>, donde esta matriz la hemos utilizado con un método de la libreria de sklearn que nos dibuja la matriz de confusion para los datos que queremos del conjunto de datos. Despues de haber creado la matriz calculamos los valores de verdaderos positivos, falsos positivos, falsos negativos y verdaderos negativos y, por último, calculamos las tasas de la matriz de confusion y las guardamos en una lista. </li>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Vamos a mostrar a continuación una ejecución del anterior codigo para verlo con diferentes datasets y diferentes predicciones.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "matrizConfusion() missing 1 required positional argument: 'prediccion'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-51-fcebc4065ffa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mnb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mentrenamiento\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparticion\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindicesTrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasifica\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mparticion\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindicesTest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mmatriz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmatrizConfusion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparticion\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindicesTest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatriz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: matrizConfusion() missing 1 required positional argument: 'prediccion'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ol>\n",
    "2. En segundo lugar, debemos sacar la gráfica de la curva ROC. Esta gráfica lo sacamos con la libreria pyplot, mas concretamente, con matplotlib. Donde en el eje Y pondremos los valores TPR y en el eje de las X pondremos los valores de FPR.\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> A contnuación, mostraremos todas las curvas ROC para todos los conjuntos de datos que tenemos y asi poder ver si nuestro clasificador es óptimo para este tipo de datos.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
