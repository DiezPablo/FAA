{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>PRÁCTICA 1 - FUNDAMENTOS DE APRENDIZAJE AUTOMÁTICO</h1>\n",
    "<h3>Realizada la práctica por:<br/>\n",
    "    <ol>\n",
    "    -Pablo Díez del Pozo<br/>\n",
    "    -Alejandro Alcalá Álvarez\n",
    "    </ol>\n",
    " </h3>\n",
    "<h3>Grupo: 1461</h3>\n",
    "<h3>Pareja: 01</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/python3\n",
    "# -*- coding: utf-8 -*-"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Importaciones necesarias para la ejecucion del código</h3>\n",
    "<p>Podemos observar todos los import necesarios que tenemos que realizar para que la ejecución de nuestro codigo funcione a la perfección, a continuación, explicaremos cada uno de los imports y para que son necesarios:</p>\n",
    "<ol>\n",
    "    <li>Random: se utiliza para hacer las secuencias de índices aleatorios para las particiones de entrenamiento y de clasificación.\n",
    "    <li>Math: se utiliza para hacer la distribución normal para los atributos que sean continuos y asi poder calcular su probabilidad.\n",
    "    <li>Numpy: Es la libreria mas utilizada en esta práctica, debido a que almacenamos los datos en una matriz numpy y guardamos las probabilidades posterioris de los atributos en un array de matrices de numpy.\n",
    "    <li>ABC: se utiliza para haces clases y métodos abstractos.\n",
    "    <li>Datos: se utiliza para importar toda la funcionalidad de nuestro modulo Datos.\n",
    "    <li>Collections: se utiliza para contabilizar las probabilidades condicionadas y para ver cuantas clases hay en el fichero\n",
    "    <li> SortedDict: se utiliza para ordenar el diccionario que creamos con las probabilidades a priori de cada clase\n",
    "    <li>Sklearn: se utiliza para hacer el tercer apartado de esta práctica, donde nos da una implementación del algoritmo de Naive-Bayes\n",
    "    <li>Pyplot: se utiliza en el último apartado de la práctica, donde nos da una implementación para pintar la curva ROC.\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "from abc import ABCMeta, abstractmethod\n",
    "from Datos import Datos\n",
    "from collections import Counter\n",
    "from sortedcontainers import SortedDict\n",
    "from sklearn.metrics import confusion_matrix, auc\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Obtener los datos de los Distintos Dataset</h3>\n",
    "<p>Aqui vamos a poder observar como vamos a codificar los datos que nos dan en un fichero a una matriz Numpy para poder tratar los datos para poder entrenarlos y clasificarlos con Naive-Bayes</p>\n",
    "<p>Vamos a ver como llamando a la clase Datos y que en su constructor le ponemos la ruta del fichero se crea la matriz numpy de los datos, pero a demás de esa matriz también guardamos información necesaria para poder entrenarlos y clasificarlos correctamente. Por ejemplo, guardamos si los atributos son continuos o discretos.</p>\n",
    "<p>A continuación, vamos a mostrar una ejecución para cada uno de los conjuntos de datos que nos dan para hacer Naive-Bayes. En la celda de abajo vereis la ejecución.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'lenses.data'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-e2e5429300b3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDatos\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'lenses.data'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdataset2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDatos\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'tic-tac-toe.data'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdataset3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDatos\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'german.data'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"==============MATRIZ NUMPY DEL CONJUNTO DE DATOS LENSES=====================\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Code/Repos_Git/FAA/Practica1/Codigo/Datos.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, nombreFichero)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnombreFichero\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnombreFichero\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m             \u001b[0;31m# Guardamos el numero de datos que contiene el DataSet y esta en la primera linea\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumDatos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'lenses.data'"
     ]
    }
   ],
   "source": [
    "dataset = Datos('../Datasets/lenses.data')\n",
    "dataset2 = Datos('../Datasets/tic-tac-toe.data')\n",
    "dataset3 = Datos('../Datasets/german.data')\n",
    "print(\"==============MATRIZ NUMPY DEL CONJUNTO DE DATOS LENSES=====================\")\n",
    "print(dataset.datos)\n",
    "print(\"============================================================================\")\n",
    "print(\"==============MATRIZ NUMPY DEL CONJUNTO DE DATOS TIC-TAC-TOE================\")\n",
    "print(dataset2.datos)\n",
    "print(\"============================================================================\")\n",
    "print(\"==============MATRIZ NUMPY DEL CONJUNTO DE DATOS GERMAN=====================\")\n",
    "print(dataset3.datos)\n",
    "print(\"============================================================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Apartado 1: Estrategia de Particionado</h3>\n",
    "<p>En este apartado vamos a probar las dos estrategias de particionado de los datos que hemos tenido que implementar en esta práctica, las cuales son:</p>\n",
    "    <ol>\n",
    "        <p>- Validación Simple.</p>\n",
    "        <p>- Validación Cruzada.</p>\n",
    "    </ol>\n",
    "<p>Nuestra estrategia de <strong>validación simple</strong> consiste en meterle un porcentaje por el cual queremos dividir el conjunto de datos en dos subconjuntos de datos, donde uno lo vamos a utilizar para entrenar y el otro lo vamos a utilizar para hacer la predicción con nuestro clasificador. En la celda de abajo mostraremos el código necesario para poder realizar correctamente la validacion simple.</p>\n",
    "<p>Como podemos observar en el código de abajo de validación simple, lo que hacemos es que ponemos una semilla a random y decimos que el numero de particiones va a ser uno. A continuación, haremos un permutacion de numeros aleatorios entre el 0  y el número de datos que hay en el fichero. Por ultimo, lo que hacemos es que le creamos la partición que va a tener en su interior los dos subconjuntos de Train y Test. En esa permutación lo multiplicamos por el porcentaje que le hemos dado nosotros para crear los dos suboconjuntos.</p>\n",
    "<p>Debajo de esta celda vamos a comprobar en como funciona  la validación simple con diferentes porcentajes para obtener el subconjunto de datos de Train y Test, en los diferentes conjuntos de datos </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Particion():\n",
    "\n",
    "  # Esta clase mantiene la lista de �ndices de Train y Test para cada partici�n del conjunto de particiones\n",
    "  def __init__(self, train=[], test=[]):\n",
    "    self.indicesTrain = train\n",
    "    self.indicesTest = test\n",
    "\n",
    "  def __str__(self):\n",
    "    return \"Train: {}\\nTest:  {}\".format(str(self.indicesTrain), str(self.indicesTest))\n",
    "\n",
    "class EstrategiaParticionado:\n",
    "  # Clase abstracta\n",
    "  __metaclass__ = ABCMeta\n",
    "\n",
    "  # Lista de las particiones\n",
    "  def __init__(self, nombre=\"\"):\n",
    "    self.nombreEstrategia = nombre\n",
    "    self.numeroParticiones = 0\n",
    "    self.particiones = []\n",
    "\n",
    "  # Atributos: deben rellenarse adecuadamente para cada estrategia concreta: nombreEstrategia, numeroParticiones, listaParticiones. Se pasan en el constructor\n",
    "\n",
    "  @abstractmethod\n",
    "  # TODO: esta funcion deben ser implementadas en cada estrategia concreta\n",
    "  def creaParticiones(self, datos, seed=None):\n",
    "    pass\n",
    "\n",
    "class ValidacionSimple(EstrategiaParticionado):\n",
    "\n",
    "  def __init__(self, porcentaje):\n",
    "    self.porcentaje = porcentaje\n",
    "    super().__init__(\"Validacion simple\")\n",
    "\n",
    "  # Crea particiones segun el metodo tradicional de division de los datos segun el porcentaje deseado.\n",
    "  # Devuelve una lista de particiones (clase Particion)\n",
    "  # TODO: implementar\n",
    "  def creaParticiones(self, datos, seed=None):\n",
    "\n",
    "    np.random.seed(seed)\n",
    "    self.numeroParticiones = 1\n",
    "\n",
    "    # Generamos una lista con todos los números de datos aleatorios\n",
    "    indicesAleatorios = np.random.permutation(int(datos.numDatos))\n",
    "\n",
    "    # Creamos la particion, en funcion del porcentaje especificado\n",
    "    self.particiones = [Particion(indicesAleatorios[:int(datos.numDatos * self.porcentaje)],\n",
    "                                  indicesAleatorios[int(datos.numDatos * self.porcentaje):])]\n",
    "\n",
    "    return self.particiones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============VALIDACIÓN SIMPLE CON LENSES DATA 70%======================\n",
      "Train: [16 10  9  2 15 12  0 19 21 18  8 14 17 20  5 13]\n",
      "Test:  [ 4  6 23  7 11  1 22  3]\n",
      "=========================================================================\n",
      "==============VALIDACIÓN SIMPLE CON TIC-TAC-TOE DATA 80%=================\n",
      "Train: [713 709 512  11 151 849 170  92 848 634 546 541 228 646 815  77 808 300\n",
      " 755 415 356 623 113 747 447 519 359 337 242 577 215  19 565 319 765 742\n",
      "  24 921 509  69 270 605 431 779 539 121 443 276  81 564 609 551 908  66\n",
      " 444 593 864 107 626 875  68   4  22 554 213 791 837 925 597 789 381 774\n",
      " 391 553 910 945 318 737 718 268 255 312 693 922 446 749 743 138  18 132\n",
      " 940 248 252 205 378 827 197 153 122 292 536 349 869 338  85 724  59 404\n",
      " 736 668 911 410 102 283 137 585 559 836 374 408 220 610 806 181 285 863\n",
      " 899 452 231 535 217 664 572 596 239 439 917 352 189 244 426 900 229 372\n",
      " 824  58 871  95 147 934 484 560 418  15 281 459 652 632 200 116 540 385\n",
      " 136 587 952 772 735 732 275 253 298 876 858 493 199 708 436 580 636 335\n",
      " 767 555 903 430   0 207 234 494 428 211 307 637 445 733 575 398 648 630\n",
      " 621 230 550  13  61 568 861 855 183 505 129 526 936 714 615 127 119 842\n",
      " 817 190 168 383 548 805 480 457 760 467  31 267 672  20 468 510 286 503\n",
      "  41  87 873 684 865 859 515 353 288 322 162 854 376 937 143  37 603 373\n",
      " 442 722 935 562 174 739 843 188  16 946 290 819 669 697 807 357  10 128\n",
      " 880 938 194 665 358 111 291 246 534 327 393  98 545 112 822 289 380 612\n",
      " 834 695 898 277 904 456 663 225 487 578 874 948 845 670 696 108 152 795\n",
      " 715 557 310 386 513 891 274 770 666 753 758 542 533  44 204 678 700 171\n",
      " 628 633 354 699   6 412 905 346 788 671 511 474 202 627 131 237 660 434\n",
      " 490 429 517 606  52 496 655 245  70 825 475 169 390 254 182 483 613 214\n",
      " 388 287 347 622 764 590  53 208 832 329 608 109  29 918 717 601 506 682\n",
      " 667 953 901  51 740 221 463 954 643  30 296 469  36 336 588 140 872 653\n",
      " 449 222 683  12 106 160 453 427 339 192 828 420 123 691 712 830 607 531\n",
      " 949 651 481 777 325 800  42 595 520 883 611 167 881 887 324 201  83 569\n",
      " 656 219 470 193   1 780 787 115  96 957 810 746 730 631 425 233 649  76\n",
      " 355 502 110 802 756 156  56 460  97 657 797 504 674 265 943 295 763 305\n",
      "  67 266 273 644 716 403 262 236  26 702 154 838 688 794 710 750 719 705\n",
      " 206 311 704 438 424  73 552  90  63 811 433 259 416 498 524 703 853 529\n",
      " 249 584 328 570 619 257 117 417   2 818 345 141 232 226 689 846 278 680\n",
      " 851 841 105 537 920 912 931 847 486 768 191 692 888 142 184 833 348 482\n",
      " 786 150 813 180 133 726 620 882 707 216 450 399 479 321 212 525 523 685\n",
      "  33 582 294 507  38 500 530 440 371 124 306 867 752 675 284 441 100 315\n",
      " 516  28 821 120  34 157 741 384 387 224 661 161 269 604 472 499 928 662\n",
      " 165 342 340 734 878 164  74 731 299 422 676 941 297 146 301 432 893 521\n",
      " 159 723 635   8 785 320 870  55  82 396 363 271 176 411 514 558 720 485\n",
      " 240 947 862 501  89 203 831 258 489 784 279   3 454  72 223 263 896 455\n",
      " 331 379 382  93 148 868 302 914  23  25 243 125  88 906 618 574  65 333\n",
      " 177 856 173   5  94 877 491 826 155 955 448 437 369 462 916 586 599 332\n",
      " 282 473 725 375 686 933 885 913 762 323 130  46 341 907 812 751  79 326\n",
      " 942 857 409  35 950 461 792 543 639 198 711 314  60 729 308  49 538 690\n",
      " 179 260  40 400 844 698 591 895 466 783 149 773 567  64 414 839 532 272\n",
      " 508 392 801 344  84 370 343 625 956 581 728   9 185 413 334 592  80 923\n",
      " 782 492 195 894 647 638 694 776 166 280]\n",
      "Test:  [796 250 892 790 641 616 829 256 389  47 759 187 624 771 778 218 261 761\n",
      " 172 642 589  54 902 401  43 884 497 313 367 317 316 944 235 850 852 495\n",
      " 471 919 814 915 727 522 368 645  17 126 101 365 640  78  91 889 544 804\n",
      " 209 835 879 458 781  21 377 816 360 241 659 598 897 351 139 103 304 114\n",
      "  50 930 860 687   7  32 745 654  75 927 614 406 104 405 423 361 629 579\n",
      " 158 799 939 251 421 309 929  39 227 476 163 721 701 744 478 549 673 518\n",
      " 135  86 238 823  57 330 757 527  27 556 395 528 926 600 583 594 210 573\n",
      " 571 706 650 145 748 658  14 840  45 677 488 566 451 769 293 754 738 890\n",
      " 465 186 866 394 803 350 303 247 951 561 809 464 820 407 397 775 402 419\n",
      " 932 364 766 477 679 793 886  62 617 144 602 547 264  48 134 178 366 435\n",
      " 909 563 362 175 924 681 118  71  99 798 576 196]\n",
      "=========================================================================\n",
      "==============VALIDACIÓN SIMPLE CON GERMAN DATA 75%======================\n",
      "Train: [857 253 852 614  42 366 787 627 870 886 629 703 650 396 139 131 474 867\n",
      " 688 690 831 325  89 140 300 897 103 776 986 107 859 940 296 445 856 573\n",
      " 202 442 223 368 314 769 895 304  22  74 626 346 550  50  91 589  48 321\n",
      "  35 920 755 488 138 903 601 473 500 788 618 939 820 753 692 695 912 347\n",
      " 398 855 258 714 882 883 646 262 892 750 511  33 250 240  73 604 239 187\n",
      " 943 405   8 901 385 294 291 778 263 453  65 836 677 963 602 256 651 249\n",
      "  11 858 254 829 432 937 461 590 228 281 234 265 449 638 519 409 430   0\n",
      " 781 333 569 871 547 128  23 288 865 597 516  67 177 761 793 316 418 512\n",
      " 837 913 606  56 454 799 670  28 699 165 823 730 191 546 975 770 501 587\n",
      " 370 795 731 383 746 749 900   1 997 153 683 275 710 389 197 875 380 754\n",
      " 121 784  45 247 921 896 307 644 180 616 884 745  80 944 967 475 471 666\n",
      " 751 255 779 399 218 217 332 586 926 352 407 696 358 458 568 356 669 929\n",
      " 592 455 286 824 185 780 342 552 880 528 881 848 301 694 236 157 983 504\n",
      " 160 526 816 906 292 738 582 371 772 329 863 782 252 760 935 462 459 419\n",
      " 143 132 309 992  94 182 747 879 642 476 723 235 955 230 822 246 426 764\n",
      " 199 596 630 216 231 433 198   5 767 374 354 613 700 114 624 494 293  86\n",
      " 226 806 543 285  37 732 721 362 122 178 348 719  99  24  51 406 922 415\n",
      " 830 805 825 953 600 259 363 388 792 416 189 628 563 227 722 549 978 981\n",
      " 711 390 284 580 205 527  57 438 215 674 206 135 988 375  20   9  69 209\n",
      " 804 241 149   3 310 741 350 517 649 428 797 878 387 435 984 148 585 914\n",
      " 274 404 847 827 996 564 924 308  66 211 756 278 359 766 931 145 885 583\n",
      " 120 272 340 839 842 422 936 277 269  31 343  32  55 441 311 193 423 655\n",
      " 993 744  10 318 949 439 505 968 184 295 687 554 443 345 965 327 704 124\n",
      " 617  21 849 874 679  39 420 720 414 588 509 472 725 427 941 155 572 513\n",
      " 990 962 959 164 130 498 522 444 970 141 869 693 581 192 319 994 119 305\n",
      " 395 487 154 392 908 171 302  98 437 918 183 326 594 872  26 105 497 408\n",
      "  44 382 794 765 676  97 809 367 625 851 127  52 574 451 850 532 686 718\n",
      " 889 774 245 961 224 763 861 531 134 248 976 891 902 257 417 657 621 290\n",
      " 493 675 709 525  53 266   6 821 948 890 898 724 222 156 233  90 893 394\n",
      " 457 980  63 840 800 656 557 928 299 684 126 652 645 317 510 181 168  59\n",
      " 276  58 873 759 337 203 479 538 229 344  81 775 960 599 789  76 611 376\n",
      " 932 360 748 551  38 204 737  83  18 232 561  47 100 384 887  16 289 595\n",
      " 534 631 832 514 641 424 483 484 151 591 556 560 956 562 647 521 142 783\n",
      " 974 917 659 530 386   2 860 667 166 835  40 220 537 927 495 577  78 535\n",
      " 791 951 378  64 950 565 734 612 221 118 998 768 648 845 843 421 331  84\n",
      " 712 989 133 456 491 447 987 173 862 251 785 174 801 815 213  72 411 478\n",
      " 502 868 973 113  62 144 634  95 279 575  30 102 942 466 312 713 934 702\n",
      " 653 238 170 158  70 136 315 736 819 752 207 446 899 673 480 397 425 904\n",
      " 957 910 620 357 175 152 811 758 796 110 786 716 834 812 818 548 864 717\n",
      " 440 979 958 915 225 663 985 545 916 460 450 452 242 436 938 846  79 273\n",
      " 214 330 706 351 802 412 888 810 567 334 701 607 907 506  49 112 972 947\n",
      " 320 244 176 635  27 270 172 485 707  92 991 541]\n",
      "Test:  [ 43 603 518 169  75 964 355 195 123 945 643 715 905 179 608  71 559 571\n",
      " 306 619  46 739 190 410 697 507  54 817 615 982 680 115 946  17 297 364\n",
      " 186 469  36 814 952 283 735 212 400 111 999 116 919 467 104 137 866 622\n",
      " 727  41 264 660 708 803 570 369 726 373 773 969 282 303  88 268 201 838\n",
      " 833 465 637 260 379 844 662 324 682  77 853 523 665 555 609  61 742 798\n",
      " 807 877 129 377 539  60 757 237 579 106 109 463  85 194 200 636 162 733\n",
      " 909 808  96 261 196 341 393 287 167 336 298  68 925 161 147 431 698 470\n",
      "  15 632 536 584 210 681  34 668 841 971 464 977 828 477 481  13 529 372\n",
      " 826 163 558 593  82 365 540  29 578 894 401 639 664 429 339 661 966 728\n",
      " 448 671 468 954 322 271  12 762 353 499 771 691 520 381 544 101 933 117\n",
      " 219 108 503 743 402 911  87 923 489 658 508 188 995 146 496 482   7 486\n",
      " 349 150  19 243 685 633 413 654  14 313 492 610 125 515 208 542 598 338\n",
      "   4 524 689 605 533 335 640 790  25 876 672 403 777 159 391  93 740 930\n",
      " 361 553 323 854 280 623 678 490 566 434 813 729 267 328 705 576]\n",
      "=========================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"==============VALIDACIÓN SIMPLE CON LENSES DATA 70%======================\")\n",
    "estrategia = ValidacionSimple(0.7)\n",
    "estrategia.creaParticiones(dataset)\n",
    "print(estrategia.particiones[0])\n",
    "print(\"=========================================================================\")\n",
    "print(\"==============VALIDACIÓN SIMPLE CON TIC-TAC-TOE DATA 80%=================\")\n",
    "estrategia2 = ValidacionSimple(0.8)\n",
    "estrategia2.creaParticiones(dataset2)\n",
    "print(estrategia2.particiones[0])\n",
    "print(\"=========================================================================\")\n",
    "print(\"==============VALIDACIÓN SIMPLE CON GERMAN DATA 75%======================\")\n",
    "estrategia3 = ValidacionSimple(0.75)\n",
    "estrategia3.creaParticiones(dataset3)\n",
    "print(estrategia3.particiones[0])\n",
    "print(\"=========================================================================\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Nuestra estrategia de <strong>validación cruzada</strong> consiste en dividir nuestro conjunto de datos en particiones de Train y Test como en validación simple, pero este proceso lo vamos a hacer K veces, para que todos los datos esten presenten en los dos subconjunto de datos para poder obtener una mejora a la hora de entrenar y clasificar.A continuación, mostraremos nuestra implementación de la validación cruzada; donde vamos a hacer K veces las divisiones del conunto de datos y si por algún motivo nuestra división de todos los subconjuntos no es perfecta balancearemos los datos sobrantes para poder entrenarlos y clasificarlos.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValidacionCruzada(EstrategiaParticionado):\n",
    "\n",
    "  # Crea particiones segun el metodo de validacion cruzada.\n",
    "  # El conjunto de entrenamiento se crea con las nfolds-1 particiones y el de test con la particion restante\n",
    "  # Esta funcion devuelve una lista de particiones (clase Particion)\n",
    "  # TODO: implementar\n",
    "\n",
    "  def __init__(self, k):\n",
    "    self.k = k\n",
    "    super().__init__(\"Validacion cruzada\")\n",
    "\n",
    "  def creaParticiones(self, datos, seed=None):\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    self.numeroParticiones = self.k\n",
    "\n",
    "    # Generamos una lista con todos los números de datos aleatorios\n",
    "    indicesAleatorios = np.random.permutation(int(datos.numDatos))\n",
    "\n",
    "    # Hallamos el tamaño de cada bloque\n",
    "    tamBloque = int(datos.numDatos / self.k)\n",
    "\n",
    "    datosSobran = datos.numDatos - (tamBloque * self.k)\n",
    "    count = 0\n",
    "    for i in range(self.k):\n",
    "\n",
    "      train = np.delete(indicesAleatorios, range(i * tamBloque, (i + 1) * tamBloque))\n",
    "      test = indicesAleatorios[i * tamBloque:(i + 1) * tamBloque]\n",
    "\n",
    "      # Caso en el que la cuenta es justa\n",
    "      if datosSobran == 0:\n",
    "        self.particiones.append(Particion(train, test))\n",
    "\n",
    "      # Contemplamos el caso de que la division para sacar el numero de subconjuntos no fuese entera\n",
    "      if datosSobran > 0:\n",
    "        count += 1\n",
    "        particionTest = np.append(test, train[(datos.numDatos - tamBloque) - i - 1])\n",
    "        particionTrain = np.delete(train, (datos.numDatos - tamBloque) - i - 1)\n",
    "        datosSobran -= 1\n",
    "        self.particiones.append(Particion(particionTrain, particionTest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>A continuación, mostraremos la ejecución de nuestra estrategia de partcionado validación simple o también llamada K-fold, con los diferentes conjuntos de datos y con diferentes K's. Vamos poder observar en la salida de nuestra celda que todos los valores van a estar una vez en la partición de Test.</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============VALIDACIÓN CRUZADA CON LENSES DATA K=5======================\n",
      "Train: [10 13  0 16 19  5 12  8 22  2 21 17 20  1 23  6 15  7 18]\n",
      "Test:  [ 3 11  4  9 14]\n",
      "Train: [ 3 11  4  9 19  5 12  8 22  2 21 17 20  1 23  6 15  7 14]\n",
      "Test:  [10 13  0 16 18]\n",
      "Train: [ 3 11  4  9 10 13  0 16 22  2 21 17 20  1 23  6 15 18 14]\n",
      "Test:  [19  5 12  8  7]\n",
      "Train: [ 3 11  4  9 10 13  0 16 19  5 12  8 20  1 23  6  7 18 14]\n",
      "Test:  [22  2 21 17 15]\n",
      "Train: [ 3 11  4  9 10 13  0 16 19  5 12  8 22  2 21 17 15  7 18 14]\n",
      "Test:  [20  1 23  6]\n",
      "==========================================================================\n",
      "==============VALIDACIÓN CRUZADA CON TIC-TAC-TOE DATA K=8=================\n",
      "Train: [157  87 584 644 544 756 734 640 147 437 841  98 439 123 111  41 946 504\n",
      " 746 293 867 531 754 716 741 766 218 140 944 698 820 758 352 113  16 935\n",
      " 226 750 418 396 911 478 180 664 322 233 399 402 628 470 572 490 650 343\n",
      "  31 186 950 389 647 388 393 582 258 384 381 220 925 445 747  66 557 866\n",
      " 365 953 831 430 199 198 211  27 477 377 903 755  89 776 930 404 804 264\n",
      " 163 697 702 744 380 360 192 251 337 359 296 556 837 942 681 735 341 780\n",
      " 846 432 589 794 374 610 231 577 143 212  47 514  50  57 684 895 116  35\n",
      " 246 586 824 923 601 603 449 426 714 151 616 425  55 173 407 667  86  46\n",
      " 520 469 705 692 329 166 294 588 197 291 821  15 349  91  44 924 651 184\n",
      " 321 164 707 355 861 222 420  83  42 578 519 585 299 104 778 346 560 920\n",
      " 847 656 279  17 339 890 283 788 807 312 295 436 600 345 486 229 171 860\n",
      " 165 281 120 720 701 790 408   7  48 896 676 302  40 917 791 816 539 749\n",
      " 811 629 317 409 473  65 256 862 893 527  70 771 926 530 863 883 442 562\n",
      " 419  30 604  75 215 415 916 784 248 908 378 235 878 369  88  11 292 789\n",
      "  63 526 204 792  29 155 740 873 168 547 307 181 886 242 232 336 444 533\n",
      "  39 301 269 857 540 273 454 759 280 843 156 306  54  82 127 326 569 532\n",
      " 922 403 145 266  45 612  26 889 429 174  94 178 172 551 196 666 941 768\n",
      " 928 910 161 276  72 782 567 643 721 472 694 315  20 348 516  85 126 563\n",
      "  32 435 626  59 763 879 240 160 605 761 943 219 506  53 559 510  95 375\n",
      " 413 652 492 859 405 261 835  33 423 513 358 395 552 290 598 106 498 304\n",
      " 683 361 250 783 124 433 546 645 787 595 529  56 125 440 695 537  97 912\n",
      " 311 936  43 891 954 202 751 558 931 690 894 743  78 725 255 176  90 476\n",
      " 570 829 542 779 773 772  64 364 583 177 669 672 733 485 856 152 397 102\n",
      " 955 210 728 483 882 593 813 507 872 424 221 576 206 109   6 362 611 822\n",
      " 461 828 671 515 417 885 905 613 528 263 452 708 131 254 357 762  22 700\n",
      "  23 368 937 679 105 481 460 536 487 316 719  24 488 748 661 390 575 715\n",
      " 103 118 325 844  28 765 318  69 243 386 169 817 945 823 342 278 718 723\n",
      " 428 915 265 564 494 596 599 680 609 347 249  76 682 297 501 213  37 665\n",
      " 799 284 618 107 842 670   5 696 459 808 853 796 502 541 901 387 726 687\n",
      " 121 319 770  19 949 400 534 587 625 216  49 114 262 275 803 568 620 314\n",
      " 812 209 775 801 130 285 191 330 877 849 332 815 456 739 722 189 615 642\n",
      " 704 509 913 138 335  79 898 806 952 874 194 888 633 881 892 731 331 631\n",
      " 224  93 663 101  77 464 646 148  80 662 602 175 457 474 854 153 288 876\n",
      " 818 182 303 497 660  99 253 855 852 706 907  38 458 324 208 110 205 259\n",
      " 475 566 688  60 668 737 133 622 703 655 446 328 805 724 350 956 108  84\n",
      " 466 135 826  21 561  18 270 363 351 480 277 934 159 757 422 366 871 309\n",
      " 257 717 851 887 830 282 372  92 136  34 648 649 286 827  68 838 308 745\n",
      " 383 738 800 797 289 940  61 406 939 512 334 699 522 230 179 427 909 550\n",
      "  10 617 785 385 795  13 482 495 833 554 187 848 658 465 543 947  81 608\n",
      " 467 693 185 927 122 225 117 260 441 868 227   0 850 677 203 503 802   9\n",
      " 447 139 150 657 431 538 443 730 373  62 678 573 764 760 129 839 639 932\n",
      " 162 450 777  36 354 786 511 591 327 234 190 635 401 394  12 607 653 619\n",
      " 410 606 899 287 414 239 919 489 238 711 241 247 448 310 493 305 237 914\n",
      " 344 391 463 865  58 434 479 825 836 571 712 742 505 819 729 884 948 525\n",
      " 370 902 732 869 379 382 201 581 320 864   2 638 200 929 579 753 875 752\n",
      " 367 630 188 491   3  51 244 274 499 313 900 228 685 637 517 154 597 371\n",
      " 158 412 675 769 207 691 809 594  67 548]\n",
      "Test:  [713  74 623  25 736 634 921 592 496 938 518 141 659 845 957 673 686 614\n",
      " 524 632 416 411 624 840 115 641 565 137 549 167 267 555 271 798 223 128\n",
      "  96 906 523 654 951 580 710  14 535 338 793 621 897 834 918 245 142 340\n",
      " 132 214 500 112 880 781 438 933 574 627  73 119 300 727 689 272 858   4\n",
      " 146 195 674 183 421 451 508 553 521 100 323 455 193 333  52 392 810 545\n",
      " 709 832   8 774 268 484 590  71 767 217 870 144 170 236   1 376 468 904\n",
      " 462 453 814 471 636 149 353 252 356 134 298 398]\n",
      "Train: [713  74 623  25 736 634 921 592 496 938 518 141 659 845 957 673 686 614\n",
      " 524 632 416 411 624 840 115 641 565 137 549 167 267 555 271 798 223 128\n",
      "  96 906 523 654 951 580 710  14 535 338 793 621 897 834 918 245 142 340\n",
      " 132 214 500 112 880 781 438 933 574 627  73 119 300 727 689 272 858   4\n",
      " 146 195 674 183 421 451 508 553 521 100 323 455 193 333  52 392 810 545\n",
      " 709 832   8 774 268 484 590  71 767 217 870 144 170 236   1 376 468 904\n",
      " 462 453 814 471 636 149 353 252 356 134 298 514  50  57 684 895 116  35\n",
      " 246 586 824 923 601 603 449 426 714 151 616 425  55 173 407 667  86  46\n",
      " 520 469 705 692 329 166 294 588 197 291 821  15 349  91  44 924 651 184\n",
      " 321 164 707 355 861 222 420  83  42 578 519 585 299 104 778 346 560 920\n",
      " 847 656 279  17 339 890 283 788 807 312 295 436 600 345 486 229 171 860\n",
      " 165 281 120 720 701 790 408   7  48 896 676 302  40 917 791 816 539 749\n",
      " 811 629 317 409 473  65 256 862 893 527  70 771 926 530 863 883 442 562\n",
      " 419  30 604  75 215 415 916 784 248 908 378 235 878 369  88  11 292 789\n",
      "  63 526 204 792  29 155 740 873 168 547 307 181 886 242 232 336 444 533\n",
      "  39 301 269 857 540 273 454 759 280 843 156 306  54  82 127 326 569 532\n",
      " 922 403 145 266  45 612  26 889 429 174  94 178 172 551 196 666 941 768\n",
      " 928 910 161 276  72 782 567 643 721 472 694 315  20 348 516  85 126 563\n",
      "  32 435 626  59 763 879 240 160 605 761 943 219 506  53 559 510  95 375\n",
      " 413 652 492 859 405 261 835  33 423 513 358 395 552 290 598 106 498 304\n",
      " 683 361 250 783 124 433 546 645 787 595 529  56 125 440 695 537  97 912\n",
      " 311 936  43 891 954 202 751 558 931 690 894 743  78 725 255 176  90 476\n",
      " 570 829 542 779 773 772  64 364 583 177 669 672 733 485 856 152 397 102\n",
      " 955 210 728 483 882 593 813 507 872 424 221 576 206 109   6 362 611 822\n",
      " 461 828 671 515 417 885 905 613 528 263 452 708 131 254 357 762  22 700\n",
      "  23 368 937 679 105 481 460 536 487 316 719  24 488 748 661 390 575 715\n",
      " 103 118 325 844  28 765 318  69 243 386 169 817 945 823 342 278 718 723\n",
      " 428 915 265 564 494 596 599 680 609 347 249  76 682 297 501 213  37 665\n",
      " 799 284 618 107 842 670   5 696 459 808 853 796 502 541 901 387 726 687\n",
      " 121 319 770  19 949 400 534 587 625 216  49 114 262 275 803 568 620 314\n",
      " 812 209 775 801 130 285 191 330 877 849 332 815 456 739 722 189 615 642\n",
      " 704 509 913 138 335  79 898 806 952 874 194 888 633 881 892 731 331 631\n",
      " 224  93 663 101  77 464 646 148  80 662 602 175 457 474 854 153 288 876\n",
      " 818 182 303 497 660  99 253 855 852 706 907  38 458 324 208 110 205 259\n",
      " 475 566 688  60 668 737 133 622 703 655 446 328 805 724 350 956 108  84\n",
      " 466 135 826  21 561  18 270 363 351 480 277 934 159 757 422 366 871 309\n",
      " 257 717 851 887 830 282 372  92 136  34 648 649 286 827  68 838 308 745\n",
      " 383 738 800 797 289 940  61 406 939 512 334 699 522 230 179 427 909 550\n",
      "  10 617 785 385 795  13 482 495 833 554 187 848 658 465 543 947  81 608\n",
      " 467 693 185 927 122 225 117 260 441 868 227   0 850 677 203 503 802   9\n",
      " 447 139 150 657 431 538 443 730 373  62 678 573 764 760 129 839 639 932\n",
      " 162 450 777  36 354 786 511 591 327 234 190 635 401 394  12 607 653 619\n",
      " 410 606 899 287 414 239 919 489 238 711 241 247 448 310 493 305 237 914\n",
      " 344 391 463 865  58 434 479 825 836 571 712 742 505 819 729 884 948 525\n",
      " 370 902 732 869 379 382 201 581 320 864   2 638 200 929 579 753 875 752\n",
      " 367 630 188 491   3  51 244 274 499 313 900 228 685 637 517 154 597 371\n",
      " 158 412 675 769 207 691 809 594  67 398]\n",
      "Test:  [157  87 584 644 544 756 734 640 147 437 841  98 439 123 111  41 946 504\n",
      " 746 293 867 531 754 716 741 766 218 140 944 698 820 758 352 113  16 935\n",
      " 226 750 418 396 911 478 180 664 322 233 399 402 628 470 572 490 650 343\n",
      "  31 186 950 389 647 388 393 582 258 384 381 220 925 445 747  66 557 866\n",
      " 365 953 831 430 199 198 211  27 477 377 903 755  89 776 930 404 804 264\n",
      " 163 697 702 744 380 360 192 251 337 359 296 556 837 942 681 735 341 780\n",
      " 846 432 589 794 374 610 231 577 143 212  47 548]\n",
      "Train: [713  74 623  25 736 634 921 592 496 938 518 141 659 845 957 673 686 614\n",
      " 524 632 416 411 624 840 115 641 565 137 549 167 267 555 271 798 223 128\n",
      "  96 906 523 654 951 580 710  14 535 338 793 621 897 834 918 245 142 340\n",
      " 132 214 500 112 880 781 438 933 574 627  73 119 300 727 689 272 858   4\n",
      " 146 195 674 183 421 451 508 553 521 100 323 455 193 333  52 392 810 545\n",
      " 709 832   8 774 268 484 590  71 767 217 870 144 170 236   1 376 468 904\n",
      " 462 453 814 471 636 149 353 252 356 134 298 157  87 584 644 544 756 734\n",
      " 640 147 437 841  98 439 123 111  41 946 504 746 293 867 531 754 716 741\n",
      " 766 218 140 944 698 820 758 352 113  16 935 226 750 418 396 911 478 180\n",
      " 664 322 233 399 402 628 470 572 490 650 343  31 186 950 389 647 388 393\n",
      " 582 258 384 381 220 925 445 747  66 557 866 365 953 831 430 199 198 211\n",
      "  27 477 377 903 755  89 776 930 404 804 264 163 697 702 744 380 360 192\n",
      " 251 337 359 296 556 837 942 681 735 341 780 846 432 589 794 374 610 231\n",
      " 577 143 212  47 215 415 916 784 248 908 378 235 878 369  88  11 292 789\n",
      "  63 526 204 792  29 155 740 873 168 547 307 181 886 242 232 336 444 533\n",
      "  39 301 269 857 540 273 454 759 280 843 156 306  54  82 127 326 569 532\n",
      " 922 403 145 266  45 612  26 889 429 174  94 178 172 551 196 666 941 768\n",
      " 928 910 161 276  72 782 567 643 721 472 694 315  20 348 516  85 126 563\n",
      "  32 435 626  59 763 879 240 160 605 761 943 219 506  53 559 510  95 375\n",
      " 413 652 492 859 405 261 835  33 423 513 358 395 552 290 598 106 498 304\n",
      " 683 361 250 783 124 433 546 645 787 595 529  56 125 440 695 537  97 912\n",
      " 311 936  43 891 954 202 751 558 931 690 894 743  78 725 255 176  90 476\n",
      " 570 829 542 779 773 772  64 364 583 177 669 672 733 485 856 152 397 102\n",
      " 955 210 728 483 882 593 813 507 872 424 221 576 206 109   6 362 611 822\n",
      " 461 828 671 515 417 885 905 613 528 263 452 708 131 254 357 762  22 700\n",
      "  23 368 937 679 105 481 460 536 487 316 719  24 488 748 661 390 575 715\n",
      " 103 118 325 844  28 765 318  69 243 386 169 817 945 823 342 278 718 723\n",
      " 428 915 265 564 494 596 599 680 609 347 249  76 682 297 501 213  37 665\n",
      " 799 284 618 107 842 670   5 696 459 808 853 796 502 541 901 387 726 687\n",
      " 121 319 770  19 949 400 534 587 625 216  49 114 262 275 803 568 620 314\n",
      " 812 209 775 801 130 285 191 330 877 849 332 815 456 739 722 189 615 642\n",
      " 704 509 913 138 335  79 898 806 952 874 194 888 633 881 892 731 331 631\n",
      " 224  93 663 101  77 464 646 148  80 662 602 175 457 474 854 153 288 876\n",
      " 818 182 303 497 660  99 253 855 852 706 907  38 458 324 208 110 205 259\n",
      " 475 566 688  60 668 737 133 622 703 655 446 328 805 724 350 956 108  84\n",
      " 466 135 826  21 561  18 270 363 351 480 277 934 159 757 422 366 871 309\n",
      " 257 717 851 887 830 282 372  92 136  34 648 649 286 827  68 838 308 745\n",
      " 383 738 800 797 289 940  61 406 939 512 334 699 522 230 179 427 909 550\n",
      "  10 617 785 385 795  13 482 495 833 554 187 848 658 465 543 947  81 608\n",
      " 467 693 185 927 122 225 117 260 441 868 227   0 850 677 203 503 802   9\n",
      " 447 139 150 657 431 538 443 730 373  62 678 573 764 760 129 839 639 932\n",
      " 162 450 777  36 354 786 511 591 327 234 190 635 401 394  12 607 653 619\n",
      " 410 606 899 287 414 239 919 489 238 711 241 247 448 310 493 305 237 914\n",
      " 344 391 463 865  58 434 479 825 836 571 712 742 505 819 729 884 948 525\n",
      " 370 902 732 869 379 382 201 581 320 864   2 638 200 929 579 753 875 752\n",
      " 367 630 188 491   3  51 244 274 499 313 900 228 685 637 517 154 597 371\n",
      " 158 412 675 769 207 691 809 594 548 398]\n",
      "Test:  [514  50  57 684 895 116  35 246 586 824 923 601 603 449 426 714 151 616\n",
      " 425  55 173 407 667  86  46 520 469 705 692 329 166 294 588 197 291 821\n",
      "  15 349  91  44 924 651 184 321 164 707 355 861 222 420  83  42 578 519\n",
      " 585 299 104 778 346 560 920 847 656 279  17 339 890 283 788 807 312 295\n",
      " 436 600 345 486 229 171 860 165 281 120 720 701 790 408   7  48 896 676\n",
      " 302  40 917 791 816 539 749 811 629 317 409 473  65 256 862 893 527  70\n",
      " 771 926 530 863 883 442 562 419  30 604  75  67]\n",
      "Train: [713  74 623  25 736 634 921 592 496 938 518 141 659 845 957 673 686 614\n",
      " 524 632 416 411 624 840 115 641 565 137 549 167 267 555 271 798 223 128\n",
      "  96 906 523 654 951 580 710  14 535 338 793 621 897 834 918 245 142 340\n",
      " 132 214 500 112 880 781 438 933 574 627  73 119 300 727 689 272 858   4\n",
      " 146 195 674 183 421 451 508 553 521 100 323 455 193 333  52 392 810 545\n",
      " 709 832   8 774 268 484 590  71 767 217 870 144 170 236   1 376 468 904\n",
      " 462 453 814 471 636 149 353 252 356 134 298 157  87 584 644 544 756 734\n",
      " 640 147 437 841  98 439 123 111  41 946 504 746 293 867 531 754 716 741\n",
      " 766 218 140 944 698 820 758 352 113  16 935 226 750 418 396 911 478 180\n",
      " 664 322 233 399 402 628 470 572 490 650 343  31 186 950 389 647 388 393\n",
      " 582 258 384 381 220 925 445 747  66 557 866 365 953 831 430 199 198 211\n",
      "  27 477 377 903 755  89 776 930 404 804 264 163 697 702 744 380 360 192\n",
      " 251 337 359 296 556 837 942 681 735 341 780 846 432 589 794 374 610 231\n",
      " 577 143 212  47 514  50  57 684 895 116  35 246 586 824 923 601 603 449\n",
      " 426 714 151 616 425  55 173 407 667  86  46 520 469 705 692 329 166 294\n",
      " 588 197 291 821  15 349  91  44 924 651 184 321 164 707 355 861 222 420\n",
      "  83  42 578 519 585 299 104 778 346 560 920 847 656 279  17 339 890 283\n",
      " 788 807 312 295 436 600 345 486 229 171 860 165 281 120 720 701 790 408\n",
      "   7  48 896 676 302  40 917 791 816 539 749 811 629 317 409 473  65 256\n",
      " 862 893 527  70 771 926 530 863 883 442 562 419  30 604  75 106 498 304\n",
      " 683 361 250 783 124 433 546 645 787 595 529  56 125 440 695 537  97 912\n",
      " 311 936  43 891 954 202 751 558 931 690 894 743  78 725 255 176  90 476\n",
      " 570 829 542 779 773 772  64 364 583 177 669 672 733 485 856 152 397 102\n",
      " 955 210 728 483 882 593 813 507 872 424 221 576 206 109   6 362 611 822\n",
      " 461 828 671 515 417 885 905 613 528 263 452 708 131 254 357 762  22 700\n",
      "  23 368 937 679 105 481 460 536 487 316 719  24 488 748 661 390 575 715\n",
      " 103 118 325 844  28 765 318  69 243 386 169 817 945 823 342 278 718 723\n",
      " 428 915 265 564 494 596 599 680 609 347 249  76 682 297 501 213  37 665\n",
      " 799 284 618 107 842 670   5 696 459 808 853 796 502 541 901 387 726 687\n",
      " 121 319 770  19 949 400 534 587 625 216  49 114 262 275 803 568 620 314\n",
      " 812 209 775 801 130 285 191 330 877 849 332 815 456 739 722 189 615 642\n",
      " 704 509 913 138 335  79 898 806 952 874 194 888 633 881 892 731 331 631\n",
      " 224  93 663 101  77 464 646 148  80 662 602 175 457 474 854 153 288 876\n",
      " 818 182 303 497 660  99 253 855 852 706 907  38 458 324 208 110 205 259\n",
      " 475 566 688  60 668 737 133 622 703 655 446 328 805 724 350 956 108  84\n",
      " 466 135 826  21 561  18 270 363 351 480 277 934 159 757 422 366 871 309\n",
      " 257 717 851 887 830 282 372  92 136  34 648 649 286 827  68 838 308 745\n",
      " 383 738 800 797 289 940  61 406 939 512 334 699 522 230 179 427 909 550\n",
      "  10 617 785 385 795  13 482 495 833 554 187 848 658 465 543 947  81 608\n",
      " 467 693 185 927 122 225 117 260 441 868 227   0 850 677 203 503 802   9\n",
      " 447 139 150 657 431 538 443 730 373  62 678 573 764 760 129 839 639 932\n",
      " 162 450 777  36 354 786 511 591 327 234 190 635 401 394  12 607 653 619\n",
      " 410 606 899 287 414 239 919 489 238 711 241 247 448 310 493 305 237 914\n",
      " 344 391 463 865  58 434 479 825 836 571 712 742 505 819 729 884 948 525\n",
      " 370 902 732 869 379 382 201 581 320 864   2 638 200 929 579 753 875 752\n",
      " 367 630 188 491   3  51 244 274 499 313 900 228 685 637 517 154 597 371\n",
      " 158 412 675 769 207 691 809  67 548 398]\n",
      "Test:  [215 415 916 784 248 908 378 235 878 369  88  11 292 789  63 526 204 792\n",
      "  29 155 740 873 168 547 307 181 886 242 232 336 444 533  39 301 269 857\n",
      " 540 273 454 759 280 843 156 306  54  82 127 326 569 532 922 403 145 266\n",
      "  45 612  26 889 429 174  94 178 172 551 196 666 941 768 928 910 161 276\n",
      "  72 782 567 643 721 472 694 315  20 348 516  85 126 563  32 435 626  59\n",
      " 763 879 240 160 605 761 943 219 506  53 559 510  95 375 413 652 492 859\n",
      " 405 261 835  33 423 513 358 395 552 290 598 594]\n",
      "Train: [713  74 623  25 736 634 921 592 496 938 518 141 659 845 957 673 686 614\n",
      " 524 632 416 411 624 840 115 641 565 137 549 167 267 555 271 798 223 128\n",
      "  96 906 523 654 951 580 710  14 535 338 793 621 897 834 918 245 142 340\n",
      " 132 214 500 112 880 781 438 933 574 627  73 119 300 727 689 272 858   4\n",
      " 146 195 674 183 421 451 508 553 521 100 323 455 193 333  52 392 810 545\n",
      " 709 832   8 774 268 484 590  71 767 217 870 144 170 236   1 376 468 904\n",
      " 462 453 814 471 636 149 353 252 356 134 298 157  87 584 644 544 756 734\n",
      " 640 147 437 841  98 439 123 111  41 946 504 746 293 867 531 754 716 741\n",
      " 766 218 140 944 698 820 758 352 113  16 935 226 750 418 396 911 478 180\n",
      " 664 322 233 399 402 628 470 572 490 650 343  31 186 950 389 647 388 393\n",
      " 582 258 384 381 220 925 445 747  66 557 866 365 953 831 430 199 198 211\n",
      "  27 477 377 903 755  89 776 930 404 804 264 163 697 702 744 380 360 192\n",
      " 251 337 359 296 556 837 942 681 735 341 780 846 432 589 794 374 610 231\n",
      " 577 143 212  47 514  50  57 684 895 116  35 246 586 824 923 601 603 449\n",
      " 426 714 151 616 425  55 173 407 667  86  46 520 469 705 692 329 166 294\n",
      " 588 197 291 821  15 349  91  44 924 651 184 321 164 707 355 861 222 420\n",
      "  83  42 578 519 585 299 104 778 346 560 920 847 656 279  17 339 890 283\n",
      " 788 807 312 295 436 600 345 486 229 171 860 165 281 120 720 701 790 408\n",
      "   7  48 896 676 302  40 917 791 816 539 749 811 629 317 409 473  65 256\n",
      " 862 893 527  70 771 926 530 863 883 442 562 419  30 604  75 215 415 916\n",
      " 784 248 908 378 235 878 369  88  11 292 789  63 526 204 792  29 155 740\n",
      " 873 168 547 307 181 886 242 232 336 444 533  39 301 269 857 540 273 454\n",
      " 759 280 843 156 306  54  82 127 326 569 532 922 403 145 266  45 612  26\n",
      " 889 429 174  94 178 172 551 196 666 941 768 928 910 161 276  72 782 567\n",
      " 643 721 472 694 315  20 348 516  85 126 563  32 435 626  59 763 879 240\n",
      " 160 605 761 943 219 506  53 559 510  95 375 413 652 492 859 405 261 835\n",
      "  33 423 513 358 395 552 290 598 243 386 169 817 945 823 342 278 718 723\n",
      " 428 915 265 564 494 596 599 680 609 347 249  76 682 297 501 213  37 665\n",
      " 799 284 618 107 842 670   5 696 459 808 853 796 502 541 901 387 726 687\n",
      " 121 319 770  19 949 400 534 587 625 216  49 114 262 275 803 568 620 314\n",
      " 812 209 775 801 130 285 191 330 877 849 332 815 456 739 722 189 615 642\n",
      " 704 509 913 138 335  79 898 806 952 874 194 888 633 881 892 731 331 631\n",
      " 224  93 663 101  77 464 646 148  80 662 602 175 457 474 854 153 288 876\n",
      " 818 182 303 497 660  99 253 855 852 706 907  38 458 324 208 110 205 259\n",
      " 475 566 688  60 668 737 133 622 703 655 446 328 805 724 350 956 108  84\n",
      " 466 135 826  21 561  18 270 363 351 480 277 934 159 757 422 366 871 309\n",
      " 257 717 851 887 830 282 372  92 136  34 648 649 286 827  68 838 308 745\n",
      " 383 738 800 797 289 940  61 406 939 512 334 699 522 230 179 427 909 550\n",
      "  10 617 785 385 795  13 482 495 833 554 187 848 658 465 543 947  81 608\n",
      " 467 693 185 927 122 225 117 260 441 868 227   0 850 677 203 503 802   9\n",
      " 447 139 150 657 431 538 443 730 373  62 678 573 764 760 129 839 639 932\n",
      " 162 450 777  36 354 786 511 591 327 234 190 635 401 394  12 607 653 619\n",
      " 410 606 899 287 414 239 919 489 238 711 241 247 448 310 493 305 237 914\n",
      " 344 391 463 865  58 434 479 825 836 571 712 742 505 819 729 884 948 525\n",
      " 370 902 732 869 379 382 201 581 320 864   2 638 200 929 579 753 875 752\n",
      " 367 630 188 491   3  51 244 274 499 313 900 228 685 637 517 154 597 371\n",
      " 158 412 675 769 207 691 594  67 548 398]\n",
      "Test:  [106 498 304 683 361 250 783 124 433 546 645 787 595 529  56 125 440 695\n",
      " 537  97 912 311 936  43 891 954 202 751 558 931 690 894 743  78 725 255\n",
      " 176  90 476 570 829 542 779 773 772  64 364 583 177 669 672 733 485 856\n",
      " 152 397 102 955 210 728 483 882 593 813 507 872 424 221 576 206 109   6\n",
      " 362 611 822 461 828 671 515 417 885 905 613 528 263 452 708 131 254 357\n",
      " 762  22 700  23 368 937 679 105 481 460 536 487 316 719  24 488 748 661\n",
      " 390 575 715 103 118 325 844  28 765 318  69 809]\n",
      "Train: [713  74 623  25 736 634 921 592 496 938 518 141 659 845 957 673 686 614\n",
      " 524 632 416 411 624 840 115 641 565 137 549 167 267 555 271 798 223 128\n",
      "  96 906 523 654 951 580 710  14 535 338 793 621 897 834 918 245 142 340\n",
      " 132 214 500 112 880 781 438 933 574 627  73 119 300 727 689 272 858   4\n",
      " 146 195 674 183 421 451 508 553 521 100 323 455 193 333  52 392 810 545\n",
      " 709 832   8 774 268 484 590  71 767 217 870 144 170 236   1 376 468 904\n",
      " 462 453 814 471 636 149 353 252 356 134 298 157  87 584 644 544 756 734\n",
      " 640 147 437 841  98 439 123 111  41 946 504 746 293 867 531 754 716 741\n",
      " 766 218 140 944 698 820 758 352 113  16 935 226 750 418 396 911 478 180\n",
      " 664 322 233 399 402 628 470 572 490 650 343  31 186 950 389 647 388 393\n",
      " 582 258 384 381 220 925 445 747  66 557 866 365 953 831 430 199 198 211\n",
      "  27 477 377 903 755  89 776 930 404 804 264 163 697 702 744 380 360 192\n",
      " 251 337 359 296 556 837 942 681 735 341 780 846 432 589 794 374 610 231\n",
      " 577 143 212  47 514  50  57 684 895 116  35 246 586 824 923 601 603 449\n",
      " 426 714 151 616 425  55 173 407 667  86  46 520 469 705 692 329 166 294\n",
      " 588 197 291 821  15 349  91  44 924 651 184 321 164 707 355 861 222 420\n",
      "  83  42 578 519 585 299 104 778 346 560 920 847 656 279  17 339 890 283\n",
      " 788 807 312 295 436 600 345 486 229 171 860 165 281 120 720 701 790 408\n",
      "   7  48 896 676 302  40 917 791 816 539 749 811 629 317 409 473  65 256\n",
      " 862 893 527  70 771 926 530 863 883 442 562 419  30 604  75 215 415 916\n",
      " 784 248 908 378 235 878 369  88  11 292 789  63 526 204 792  29 155 740\n",
      " 873 168 547 307 181 886 242 232 336 444 533  39 301 269 857 540 273 454\n",
      " 759 280 843 156 306  54  82 127 326 569 532 922 403 145 266  45 612  26\n",
      " 889 429 174  94 178 172 551 196 666 941 768 928 910 161 276  72 782 567\n",
      " 643 721 472 694 315  20 348 516  85 126 563  32 435 626  59 763 879 240\n",
      " 160 605 761 943 219 506  53 559 510  95 375 413 652 492 859 405 261 835\n",
      "  33 423 513 358 395 552 290 598 106 498 304 683 361 250 783 124 433 546\n",
      " 645 787 595 529  56 125 440 695 537  97 912 311 936  43 891 954 202 751\n",
      " 558 931 690 894 743  78 725 255 176  90 476 570 829 542 779 773 772  64\n",
      " 364 583 177 669 672 733 485 856 152 397 102 955 210 728 483 882 593 813\n",
      " 507 872 424 221 576 206 109   6 362 611 822 461 828 671 515 417 885 905\n",
      " 613 528 263 452 708 131 254 357 762  22 700  23 368 937 679 105 481 460\n",
      " 536 487 316 719  24 488 748 661 390 575 715 103 118 325 844  28 765 318\n",
      "  69 182 303 497 660  99 253 855 852 706 907  38 458 324 208 110 205 259\n",
      " 475 566 688  60 668 737 133 622 703 655 446 328 805 724 350 956 108  84\n",
      " 466 135 826  21 561  18 270 363 351 480 277 934 159 757 422 366 871 309\n",
      " 257 717 851 887 830 282 372  92 136  34 648 649 286 827  68 838 308 745\n",
      " 383 738 800 797 289 940  61 406 939 512 334 699 522 230 179 427 909 550\n",
      "  10 617 785 385 795  13 482 495 833 554 187 848 658 465 543 947  81 608\n",
      " 467 693 185 927 122 225 117 260 441 868 227   0 850 677 203 503 802   9\n",
      " 447 139 150 657 431 538 443 730 373  62 678 573 764 760 129 839 639 932\n",
      " 162 450 777  36 354 786 511 591 327 234 190 635 401 394  12 607 653 619\n",
      " 410 606 899 287 414 239 919 489 238 711 241 247 448 310 493 305 237 914\n",
      " 344 391 463 865  58 434 479 825 836 571 712 742 505 819 729 884 948 525\n",
      " 370 902 732 869 379 382 201 581 320 864   2 638 200 929 579 753 875 752\n",
      " 367 630 188 491   3  51 244 274 499 313 900 228 685 637 517 154 597 371\n",
      " 158 412 675 769 207 809 594  67 548 398]\n",
      "Test:  [243 386 169 817 945 823 342 278 718 723 428 915 265 564 494 596 599 680\n",
      " 609 347 249  76 682 297 501 213  37 665 799 284 618 107 842 670   5 696\n",
      " 459 808 853 796 502 541 901 387 726 687 121 319 770  19 949 400 534 587\n",
      " 625 216  49 114 262 275 803 568 620 314 812 209 775 801 130 285 191 330\n",
      " 877 849 332 815 456 739 722 189 615 642 704 509 913 138 335  79 898 806\n",
      " 952 874 194 888 633 881 892 731 331 631 224  93 663 101  77 464 646 148\n",
      "  80 662 602 175 457 474 854 153 288 876 818 691]\n",
      "Train: [713  74 623  25 736 634 921 592 496 938 518 141 659 845 957 673 686 614\n",
      " 524 632 416 411 624 840 115 641 565 137 549 167 267 555 271 798 223 128\n",
      "  96 906 523 654 951 580 710  14 535 338 793 621 897 834 918 245 142 340\n",
      " 132 214 500 112 880 781 438 933 574 627  73 119 300 727 689 272 858   4\n",
      " 146 195 674 183 421 451 508 553 521 100 323 455 193 333  52 392 810 545\n",
      " 709 832   8 774 268 484 590  71 767 217 870 144 170 236   1 376 468 904\n",
      " 462 453 814 471 636 149 353 252 356 134 298 157  87 584 644 544 756 734\n",
      " 640 147 437 841  98 439 123 111  41 946 504 746 293 867 531 754 716 741\n",
      " 766 218 140 944 698 820 758 352 113  16 935 226 750 418 396 911 478 180\n",
      " 664 322 233 399 402 628 470 572 490 650 343  31 186 950 389 647 388 393\n",
      " 582 258 384 381 220 925 445 747  66 557 866 365 953 831 430 199 198 211\n",
      "  27 477 377 903 755  89 776 930 404 804 264 163 697 702 744 380 360 192\n",
      " 251 337 359 296 556 837 942 681 735 341 780 846 432 589 794 374 610 231\n",
      " 577 143 212  47 514  50  57 684 895 116  35 246 586 824 923 601 603 449\n",
      " 426 714 151 616 425  55 173 407 667  86  46 520 469 705 692 329 166 294\n",
      " 588 197 291 821  15 349  91  44 924 651 184 321 164 707 355 861 222 420\n",
      "  83  42 578 519 585 299 104 778 346 560 920 847 656 279  17 339 890 283\n",
      " 788 807 312 295 436 600 345 486 229 171 860 165 281 120 720 701 790 408\n",
      "   7  48 896 676 302  40 917 791 816 539 749 811 629 317 409 473  65 256\n",
      " 862 893 527  70 771 926 530 863 883 442 562 419  30 604  75 215 415 916\n",
      " 784 248 908 378 235 878 369  88  11 292 789  63 526 204 792  29 155 740\n",
      " 873 168 547 307 181 886 242 232 336 444 533  39 301 269 857 540 273 454\n",
      " 759 280 843 156 306  54  82 127 326 569 532 922 403 145 266  45 612  26\n",
      " 889 429 174  94 178 172 551 196 666 941 768 928 910 161 276  72 782 567\n",
      " 643 721 472 694 315  20 348 516  85 126 563  32 435 626  59 763 879 240\n",
      " 160 605 761 943 219 506  53 559 510  95 375 413 652 492 859 405 261 835\n",
      "  33 423 513 358 395 552 290 598 106 498 304 683 361 250 783 124 433 546\n",
      " 645 787 595 529  56 125 440 695 537  97 912 311 936  43 891 954 202 751\n",
      " 558 931 690 894 743  78 725 255 176  90 476 570 829 542 779 773 772  64\n",
      " 364 583 177 669 672 733 485 856 152 397 102 955 210 728 483 882 593 813\n",
      " 507 872 424 221 576 206 109   6 362 611 822 461 828 671 515 417 885 905\n",
      " 613 528 263 452 708 131 254 357 762  22 700  23 368 937 679 105 481 460\n",
      " 536 487 316 719  24 488 748 661 390 575 715 103 118 325 844  28 765 318\n",
      "  69 243 386 169 817 945 823 342 278 718 723 428 915 265 564 494 596 599\n",
      " 680 609 347 249  76 682 297 501 213  37 665 799 284 618 107 842 670   5\n",
      " 696 459 808 853 796 502 541 901 387 726 687 121 319 770  19 949 400 534\n",
      " 587 625 216  49 114 262 275 803 568 620 314 812 209 775 801 130 285 191\n",
      " 330 877 849 332 815 456 739 722 189 615 642 704 509 913 138 335  79 898\n",
      " 806 952 874 194 888 633 881 892 731 331 631 224  93 663 101  77 464 646\n",
      " 148  80 662 602 175 457 474 854 153 288 876 818 850 677 203 503 802   9\n",
      " 447 139 150 657 431 538 443 730 373  62 678 573 764 760 129 839 639 932\n",
      " 162 450 777  36 354 786 511 591 327 234 190 635 401 394  12 607 653 619\n",
      " 410 606 899 287 414 239 919 489 238 711 241 247 448 310 493 305 237 914\n",
      " 344 391 463 865  58 434 479 825 836 571 712 742 505 819 729 884 948 525\n",
      " 370 902 732 869 379 382 201 581 320 864   2 638 200 929 579 753 875 752\n",
      " 367 630 188 491   3  51 244 274 499 313 900 228 685 637 517 154 597 371\n",
      " 158 412 675 769 207 691 809 594  67 548 398]\n",
      "Test:  [182 303 497 660  99 253 855 852 706 907  38 458 324 208 110 205 259 475\n",
      " 566 688  60 668 737 133 622 703 655 446 328 805 724 350 956 108  84 466\n",
      " 135 826  21 561  18 270 363 351 480 277 934 159 757 422 366 871 309 257\n",
      " 717 851 887 830 282 372  92 136  34 648 649 286 827  68 838 308 745 383\n",
      " 738 800 797 289 940  61 406 939 512 334 699 522 230 179 427 909 550  10\n",
      " 617 785 385 795  13 482 495 833 554 187 848 658 465 543 947  81 608 467\n",
      " 693 185 927 122 225 117 260 441 868 227   0]\n",
      "Train: [713  74 623  25 736 634 921 592 496 938 518 141 659 845 957 673 686 614\n",
      " 524 632 416 411 624 840 115 641 565 137 549 167 267 555 271 798 223 128\n",
      "  96 906 523 654 951 580 710  14 535 338 793 621 897 834 918 245 142 340\n",
      " 132 214 500 112 880 781 438 933 574 627  73 119 300 727 689 272 858   4\n",
      " 146 195 674 183 421 451 508 553 521 100 323 455 193 333  52 392 810 545\n",
      " 709 832   8 774 268 484 590  71 767 217 870 144 170 236   1 376 468 904\n",
      " 462 453 814 471 636 149 353 252 356 134 298 157  87 584 644 544 756 734\n",
      " 640 147 437 841  98 439 123 111  41 946 504 746 293 867 531 754 716 741\n",
      " 766 218 140 944 698 820 758 352 113  16 935 226 750 418 396 911 478 180\n",
      " 664 322 233 399 402 628 470 572 490 650 343  31 186 950 389 647 388 393\n",
      " 582 258 384 381 220 925 445 747  66 557 866 365 953 831 430 199 198 211\n",
      "  27 477 377 903 755  89 776 930 404 804 264 163 697 702 744 380 360 192\n",
      " 251 337 359 296 556 837 942 681 735 341 780 846 432 589 794 374 610 231\n",
      " 577 143 212  47 514  50  57 684 895 116  35 246 586 824 923 601 603 449\n",
      " 426 714 151 616 425  55 173 407 667  86  46 520 469 705 692 329 166 294\n",
      " 588 197 291 821  15 349  91  44 924 651 184 321 164 707 355 861 222 420\n",
      "  83  42 578 519 585 299 104 778 346 560 920 847 656 279  17 339 890 283\n",
      " 788 807 312 295 436 600 345 486 229 171 860 165 281 120 720 701 790 408\n",
      "   7  48 896 676 302  40 917 791 816 539 749 811 629 317 409 473  65 256\n",
      " 862 893 527  70 771 926 530 863 883 442 562 419  30 604  75 215 415 916\n",
      " 784 248 908 378 235 878 369  88  11 292 789  63 526 204 792  29 155 740\n",
      " 873 168 547 307 181 886 242 232 336 444 533  39 301 269 857 540 273 454\n",
      " 759 280 843 156 306  54  82 127 326 569 532 922 403 145 266  45 612  26\n",
      " 889 429 174  94 178 172 551 196 666 941 768 928 910 161 276  72 782 567\n",
      " 643 721 472 694 315  20 348 516  85 126 563  32 435 626  59 763 879 240\n",
      " 160 605 761 943 219 506  53 559 510  95 375 413 652 492 859 405 261 835\n",
      "  33 423 513 358 395 552 290 598 106 498 304 683 361 250 783 124 433 546\n",
      " 645 787 595 529  56 125 440 695 537  97 912 311 936  43 891 954 202 751\n",
      " 558 931 690 894 743  78 725 255 176  90 476 570 829 542 779 773 772  64\n",
      " 364 583 177 669 672 733 485 856 152 397 102 955 210 728 483 882 593 813\n",
      " 507 872 424 221 576 206 109   6 362 611 822 461 828 671 515 417 885 905\n",
      " 613 528 263 452 708 131 254 357 762  22 700  23 368 937 679 105 481 460\n",
      " 536 487 316 719  24 488 748 661 390 575 715 103 118 325 844  28 765 318\n",
      "  69 243 386 169 817 945 823 342 278 718 723 428 915 265 564 494 596 599\n",
      " 680 609 347 249  76 682 297 501 213  37 665 799 284 618 107 842 670   5\n",
      " 696 459 808 853 796 502 541 901 387 726 687 121 319 770  19 949 400 534\n",
      " 587 625 216  49 114 262 275 803 568 620 314 812 209 775 801 130 285 191\n",
      " 330 877 849 332 815 456 739 722 189 615 642 704 509 913 138 335  79 898\n",
      " 806 952 874 194 888 633 881 892 731 331 631 224  93 663 101  77 464 646\n",
      " 148  80 662 602 175 457 474 854 153 288 876 818 182 303 497 660  99 253\n",
      " 855 852 706 907  38 458 324 208 110 205 259 475 566 688  60 668 737 133\n",
      " 622 703 655 446 328 805 724 350 956 108  84 466 135 826  21 561  18 270\n",
      " 363 351 480 277 934 159 757 422 366 871 309 257 717 851 887 830 282 372\n",
      "  92 136  34 648 649 286 827  68 838 308 745 383 738 800 797 289 940  61\n",
      " 406 939 512 334 699 522 230 179 427 909 550  10 617 785 385 795  13 482\n",
      " 495 833 554 187 848 658 465 543 947  81 608 467 693 185 927 122 225 117\n",
      " 260 441 868 227   0 691 809 594  67 548 398]\n",
      "Test:  [850 677 203 503 802   9 447 139 150 657 431 538 443 730 373  62 678 573\n",
      " 764 760 129 839 639 932 162 450 777  36 354 786 511 591 327 234 190 635\n",
      " 401 394  12 607 653 619 410 606 899 287 414 239 919 489 238 711 241 247\n",
      " 448 310 493 305 237 914 344 391 463 865  58 434 479 825 836 571 712 742\n",
      " 505 819 729 884 948 525 370 902 732 869 379 382 201 581 320 864   2 638\n",
      " 200 929 579 753 875 752 367 630 188 491   3  51 244 274 499 313 900 228\n",
      " 685 637 517 154 597 371 158 412 675 769 207]\n",
      "=========================================================================\n",
      "==============VALIDACIÓN CRUZADA CON GERMAN DATA K=4=====================\n",
      "Train: [738 499 767  24 236 799  12 441 781 575  25 466 496  32 628 121 708 977\n",
      " 304 250 131 264 850 354 285 747 113 929 573 958 382 284 498 522 272 151\n",
      " 877 145 269 940 107 440 327 406 465 494 656 110 166  15 926 163 308 428\n",
      " 784 404 596 617 605  14 917 916 366 797 699 782 211 720 411 417 965 937\n",
      " 647 835 109 820 604 157 119 254 492 532 182 602 763 914 992 409 986 362\n",
      " 167 783 554 814 377 436 987 546 715 923 419 319 673 694 201 155 342 613\n",
      "  40  61 655 900   0 724 364 143 270 415 199 899 989 464 122 216 690 230\n",
      " 320 806 402 660 641 654 584 988 816 589   8 132 104 888 185 959  60  58\n",
      " 501  97 571 689 491 880  68 108 396 271 726 144 343 260 607 500 962  96\n",
      "  73 443 237 663 232  93 148 902 291 998 609 665 664 981 453 525 463 794\n",
      " 479 744 489 399 527 788  47  33 474 222 401 356 524 993 429 341 755 103\n",
      "  91 473 286 606 255  83 648 147 195 730 348 913 821  70 847 661 256 161\n",
      " 457 667 170 513 754 725 976 736 295 778 574 583 682 557 558 439 403 623\n",
      " 919 535 630  74 963 743 669 288 468 252 804 658 827 328 149 231 711  46\n",
      " 261 670 698 361 416 175 685 495 994 874 581 578  49 268  20 615 101 873\n",
      " 347 511 257 204  92 706 645 883 461 865 156 825 918 512 384  77 376 728\n",
      "  30 450 480  63 353 312 882 223 855 219 638 298 761 449 279 903 234 915\n",
      " 721 884 828 486 287 921 970 886 313 314  23 212 811 220 153 205 942 560\n",
      " 346 227 563 674  89 999 168 306 627 329 198 826 801 534 462 722 368 350\n",
      " 798 657 957 545 275 701  72 130 691  86  62 662  90 651 832 953 759 841\n",
      " 455 118 809 684 142 635 956 938 263 719 595 776 904 570 442 541 791 531\n",
      " 947 192 863 550 582 357 478  26 338 420 872 389 831  51 735  59 967 786\n",
      " 881   1 533 251 766 927 431 751  18 952 911  43 246 160 178 878 266 506\n",
      " 933 764 138  39 857  87 508 311 853 896 924 659 789  80 843 424 591  56\n",
      " 756 737  57 750 893 600 601 414 460 614 410 561 214 982 780 129 164 317\n",
      " 206 385 387 188 324 197 739  27 753 369  71 969 818 946 810 813 618 948\n",
      " 459 390 297 567 476  75   4 869 209 340 217 249 472 939 487 983 796  55\n",
      " 830 553 330  88 423 643 680  95 325 418 228 447 290  78 772 321 586 808\n",
      " 991 676   6 894 408 183 576 345 612 380 871 485  52  13 323 210 100 556\n",
      " 265 165 336 352 218  45 430 717  65 363 289 407 585 849 650 549 400 666\n",
      " 493 616 748 358 123 334 593 274 646 693 709 241 344 529 514 620 475   9\n",
      " 245 137 793 770  64 504 705 451 964 171 548 824 483 281 975 775 437 807\n",
      " 619 538 331 259  54 769 812 381 397  22 742 277 860 412 859 391 477 642\n",
      " 842 840 879 746 120 510 837 891 169 734 681 632 610  10 181 398 454 452\n",
      "  38 731 779 253 240 180 229 579 839   7  82 446 844 925 239 829 543 184\n",
      " 760 248 373 974 861 280 518 507 515 920 378 242 262 836 456 186 202 114\n",
      "  19 282 203 768 134  31 805 316 292 555 207 846 729 890 749  17 551 592\n",
      " 687 678 303 111 634 191 539 187 333 716 852 351 587 594 713 467  84 509\n",
      " 315 714 966 337 718 608 481 943 955 971 173 858 432 696 379 258 727  11\n",
      " 611 294 360 867 636 639 907 322 819 950 471 106 762 470 887 445 422 901\n",
      " 372 740 590 930 224 773  94 941 374 732 945 572 267  69 528 302 136 629\n",
      "  35 954 686 936 371 395 133 566 833 215 310 221]\n",
      "Test:  [ 29 179 365 126 905  53 817 741  85 771 388 503 225  67 972 931  37 910\n",
      " 427 523 158  41 951 745 502  48 488 979 996 540 758 625 116 895 909 497\n",
      " 668 695 425 815 444 856 961 482 897 200  76 293 928 679 875 235 848 370\n",
      " 238 906 309 332 150 196 299  99 765 603 115 908 177 247 997 885 733 359\n",
      " 141 547 932 675 934 838  81 517 822 995 521 152 174 469 626   2 213 862\n",
      " 530 823  16 335 707 128 421 124 803 876 702 854 622 367  36 426 898 307\n",
      " 189 172 677 710 117 960 935 283 800 588 243 394 631 438 978 146 640 621\n",
      " 834 683 154 790 176 851 435 301 672 892 139 692 125 785 700  44 985  66\n",
      " 922 990 392  50 434 599 490 624 568 405 537 649 889 870 448 633 326 845\n",
      " 386 544  42 968 135 193 278 276 375 703 355 526 652 162  21 562 637 127\n",
      " 577 597 802 413 505 296 233 102 520 552  79 564 697 777 864 484 569 774\n",
      "  34 559 712 598 433  98 653 723 944 519 112 795 949 912   3 866 757 644\n",
      " 318 140 868 542 194 752 980 226 704  28 671 516 792 190 536 208 300 349\n",
      " 305 339 580 984 973   5 393 688 159 565 458 787 244 105 383 273]\n",
      "Train: [ 29 179 365 126 905  53 817 741  85 771 388 503 225  67 972 931  37 910\n",
      " 427 523 158  41 951 745 502  48 488 979 996 540 758 625 116 895 909 497\n",
      " 668 695 425 815 444 856 961 482 897 200  76 293 928 679 875 235 848 370\n",
      " 238 906 309 332 150 196 299  99 765 603 115 908 177 247 997 885 733 359\n",
      " 141 547 932 675 934 838  81 517 822 995 521 152 174 469 626   2 213 862\n",
      " 530 823  16 335 707 128 421 124 803 876 702 854 622 367  36 426 898 307\n",
      " 189 172 677 710 117 960 935 283 800 588 243 394 631 438 978 146 640 621\n",
      " 834 683 154 790 176 851 435 301 672 892 139 692 125 785 700  44 985  66\n",
      " 922 990 392  50 434 599 490 624 568 405 537 649 889 870 448 633 326 845\n",
      " 386 544  42 968 135 193 278 276 375 703 355 526 652 162  21 562 637 127\n",
      " 577 597 802 413 505 296 233 102 520 552  79 564 697 777 864 484 569 774\n",
      "  34 559 712 598 433  98 653 723 944 519 112 795 949 912   3 866 757 644\n",
      " 318 140 868 542 194 752 980 226 704  28 671 516 792 190 536 208 300 349\n",
      " 305 339 580 984 973   5 393 688 159 565 458 787 244 105 383 273 711  46\n",
      " 261 670 698 361 416 175 685 495 994 874 581 578  49 268  20 615 101 873\n",
      " 347 511 257 204  92 706 645 883 461 865 156 825 918 512 384  77 376 728\n",
      "  30 450 480  63 353 312 882 223 855 219 638 298 761 449 279 903 234 915\n",
      " 721 884 828 486 287 921 970 886 313 314  23 212 811 220 153 205 942 560\n",
      " 346 227 563 674  89 999 168 306 627 329 198 826 801 534 462 722 368 350\n",
      " 798 657 957 545 275 701  72 130 691  86  62 662  90 651 832 953 759 841\n",
      " 455 118 809 684 142 635 956 938 263 719 595 776 904 570 442 541 791 531\n",
      " 947 192 863 550 582 357 478  26 338 420 872 389 831  51 735  59 967 786\n",
      " 881   1 533 251 766 927 431 751  18 952 911  43 246 160 178 878 266 506\n",
      " 933 764 138  39 857  87 508 311 853 896 924 659 789  80 843 424 591  56\n",
      " 756 737  57 750 893 600 601 414 460 614 410 561 214 982 780 129 164 317\n",
      " 206 385 387 188 324 197 739  27 753 369  71 969 818 946 810 813 618 948\n",
      " 459 390 297 567 476  75   4 869 209 340 217 249 472 939 487 983 796  55\n",
      " 830 553 330  88 423 643 680  95 325 418 228 447 290  78 772 321 586 808\n",
      " 991 676   6 894 408 183 576 345 612 380 871 485  52  13 323 210 100 556\n",
      " 265 165 336 352 218  45 430 717  65 363 289 407 585 849 650 549 400 666\n",
      " 493 616 748 358 123 334 593 274 646 693 709 241 344 529 514 620 475   9\n",
      " 245 137 793 770  64 504 705 451 964 171 548 824 483 281 975 775 437 807\n",
      " 619 538 331 259  54 769 812 381 397  22 742 277 860 412 859 391 477 642\n",
      " 842 840 879 746 120 510 837 891 169 734 681 632 610  10 181 398 454 452\n",
      "  38 731 779 253 240 180 229 579 839   7  82 446 844 925 239 829 543 184\n",
      " 760 248 373 974 861 280 518 507 515 920 378 242 262 836 456 186 202 114\n",
      "  19 282 203 768 134  31 805 316 292 555 207 846 729 890 749  17 551 592\n",
      " 687 678 303 111 634 191 539 187 333 716 852 351 587 594 713 467  84 509\n",
      " 315 714 966 337 718 608 481 943 955 971 173 858 432 696 379 258 727  11\n",
      " 611 294 360 867 636 639 907 322 819 950 471 106 762 470 887 445 422 901\n",
      " 372 740 590 930 224 773  94 941 374 732 945 572 267  69 528 302 136 629\n",
      "  35 954 686 936 371 395 133 566 833 215 310 221]\n",
      "Test:  [738 499 767  24 236 799  12 441 781 575  25 466 496  32 628 121 708 977\n",
      " 304 250 131 264 850 354 285 747 113 929 573 958 382 284 498 522 272 151\n",
      " 877 145 269 940 107 440 327 406 465 494 656 110 166  15 926 163 308 428\n",
      " 784 404 596 617 605  14 917 916 366 797 699 782 211 720 411 417 965 937\n",
      " 647 835 109 820 604 157 119 254 492 532 182 602 763 914 992 409 986 362\n",
      " 167 783 554 814 377 436 987 546 715 923 419 319 673 694 201 155 342 613\n",
      "  40  61 655 900   0 724 364 143 270 415 199 899 989 464 122 216 690 230\n",
      " 320 806 402 660 641 654 584 988 816 589   8 132 104 888 185 959  60  58\n",
      " 501  97 571 689 491 880  68 108 396 271 726 144 343 260 607 500 962  96\n",
      "  73 443 237 663 232  93 148 902 291 998 609 665 664 981 453 525 463 794\n",
      " 479 744 489 399 527 788  47  33 474 222 401 356 524 993 429 341 755 103\n",
      "  91 473 286 606 255  83 648 147 195 730 348 913 821  70 847 661 256 161\n",
      " 457 667 170 513 754 725 976 736 295 778 574 583 682 557 558 439 403 623\n",
      " 919 535 630  74 963 743 669 288 468 252 804 658 827 328 149 231]\n",
      "Train: [ 29 179 365 126 905  53 817 741  85 771 388 503 225  67 972 931  37 910\n",
      " 427 523 158  41 951 745 502  48 488 979 996 540 758 625 116 895 909 497\n",
      " 668 695 425 815 444 856 961 482 897 200  76 293 928 679 875 235 848 370\n",
      " 238 906 309 332 150 196 299  99 765 603 115 908 177 247 997 885 733 359\n",
      " 141 547 932 675 934 838  81 517 822 995 521 152 174 469 626   2 213 862\n",
      " 530 823  16 335 707 128 421 124 803 876 702 854 622 367  36 426 898 307\n",
      " 189 172 677 710 117 960 935 283 800 588 243 394 631 438 978 146 640 621\n",
      " 834 683 154 790 176 851 435 301 672 892 139 692 125 785 700  44 985  66\n",
      " 922 990 392  50 434 599 490 624 568 405 537 649 889 870 448 633 326 845\n",
      " 386 544  42 968 135 193 278 276 375 703 355 526 652 162  21 562 637 127\n",
      " 577 597 802 413 505 296 233 102 520 552  79 564 697 777 864 484 569 774\n",
      "  34 559 712 598 433  98 653 723 944 519 112 795 949 912   3 866 757 644\n",
      " 318 140 868 542 194 752 980 226 704  28 671 516 792 190 536 208 300 349\n",
      " 305 339 580 984 973   5 393 688 159 565 458 787 244 105 383 273 738 499\n",
      " 767  24 236 799  12 441 781 575  25 466 496  32 628 121 708 977 304 250\n",
      " 131 264 850 354 285 747 113 929 573 958 382 284 498 522 272 151 877 145\n",
      " 269 940 107 440 327 406 465 494 656 110 166  15 926 163 308 428 784 404\n",
      " 596 617 605  14 917 916 366 797 699 782 211 720 411 417 965 937 647 835\n",
      " 109 820 604 157 119 254 492 532 182 602 763 914 992 409 986 362 167 783\n",
      " 554 814 377 436 987 546 715 923 419 319 673 694 201 155 342 613  40  61\n",
      " 655 900   0 724 364 143 270 415 199 899 989 464 122 216 690 230 320 806\n",
      " 402 660 641 654 584 988 816 589   8 132 104 888 185 959  60  58 501  97\n",
      " 571 689 491 880  68 108 396 271 726 144 343 260 607 500 962  96  73 443\n",
      " 237 663 232  93 148 902 291 998 609 665 664 981 453 525 463 794 479 744\n",
      " 489 399 527 788  47  33 474 222 401 356 524 993 429 341 755 103  91 473\n",
      " 286 606 255  83 648 147 195 730 348 913 821  70 847 661 256 161 457 667\n",
      " 170 513 754 725 976 736 295 778 574 583 682 557 558 439 403 623 919 535\n",
      " 630  74 963 743 669 288 468 252 804 658 827 328 149 231 772 321 586 808\n",
      " 991 676   6 894 408 183 576 345 612 380 871 485  52  13 323 210 100 556\n",
      " 265 165 336 352 218  45 430 717  65 363 289 407 585 849 650 549 400 666\n",
      " 493 616 748 358 123 334 593 274 646 693 709 241 344 529 514 620 475   9\n",
      " 245 137 793 770  64 504 705 451 964 171 548 824 483 281 975 775 437 807\n",
      " 619 538 331 259  54 769 812 381 397  22 742 277 860 412 859 391 477 642\n",
      " 842 840 879 746 120 510 837 891 169 734 681 632 610  10 181 398 454 452\n",
      "  38 731 779 253 240 180 229 579 839   7  82 446 844 925 239 829 543 184\n",
      " 760 248 373 974 861 280 518 507 515 920 378 242 262 836 456 186 202 114\n",
      "  19 282 203 768 134  31 805 316 292 555 207 846 729 890 749  17 551 592\n",
      " 687 678 303 111 634 191 539 187 333 716 852 351 587 594 713 467  84 509\n",
      " 315 714 966 337 718 608 481 943 955 971 173 858 432 696 379 258 727  11\n",
      " 611 294 360 867 636 639 907 322 819 950 471 106 762 470 887 445 422 901\n",
      " 372 740 590 930 224 773  94 941 374 732 945 572 267  69 528 302 136 629\n",
      "  35 954 686 936 371 395 133 566 833 215 310 221]\n",
      "Test:  [711  46 261 670 698 361 416 175 685 495 994 874 581 578  49 268  20 615\n",
      " 101 873 347 511 257 204  92 706 645 883 461 865 156 825 918 512 384  77\n",
      " 376 728  30 450 480  63 353 312 882 223 855 219 638 298 761 449 279 903\n",
      " 234 915 721 884 828 486 287 921 970 886 313 314  23 212 811 220 153 205\n",
      " 942 560 346 227 563 674  89 999 168 306 627 329 198 826 801 534 462 722\n",
      " 368 350 798 657 957 545 275 701  72 130 691  86  62 662  90 651 832 953\n",
      " 759 841 455 118 809 684 142 635 956 938 263 719 595 776 904 570 442 541\n",
      " 791 531 947 192 863 550 582 357 478  26 338 420 872 389 831  51 735  59\n",
      " 967 786 881   1 533 251 766 927 431 751  18 952 911  43 246 160 178 878\n",
      " 266 506 933 764 138  39 857  87 508 311 853 896 924 659 789  80 843 424\n",
      " 591  56 756 737  57 750 893 600 601 414 460 614 410 561 214 982 780 129\n",
      " 164 317 206 385 387 188 324 197 739  27 753 369  71 969 818 946 810 813\n",
      " 618 948 459 390 297 567 476  75   4 869 209 340 217 249 472 939 487 983\n",
      " 796  55 830 553 330  88 423 643 680  95 325 418 228 447 290  78]\n",
      "Train: [ 29 179 365 126 905  53 817 741  85 771 388 503 225  67 972 931  37 910\n",
      " 427 523 158  41 951 745 502  48 488 979 996 540 758 625 116 895 909 497\n",
      " 668 695 425 815 444 856 961 482 897 200  76 293 928 679 875 235 848 370\n",
      " 238 906 309 332 150 196 299  99 765 603 115 908 177 247 997 885 733 359\n",
      " 141 547 932 675 934 838  81 517 822 995 521 152 174 469 626   2 213 862\n",
      " 530 823  16 335 707 128 421 124 803 876 702 854 622 367  36 426 898 307\n",
      " 189 172 677 710 117 960 935 283 800 588 243 394 631 438 978 146 640 621\n",
      " 834 683 154 790 176 851 435 301 672 892 139 692 125 785 700  44 985  66\n",
      " 922 990 392  50 434 599 490 624 568 405 537 649 889 870 448 633 326 845\n",
      " 386 544  42 968 135 193 278 276 375 703 355 526 652 162  21 562 637 127\n",
      " 577 597 802 413 505 296 233 102 520 552  79 564 697 777 864 484 569 774\n",
      "  34 559 712 598 433  98 653 723 944 519 112 795 949 912   3 866 757 644\n",
      " 318 140 868 542 194 752 980 226 704  28 671 516 792 190 536 208 300 349\n",
      " 305 339 580 984 973   5 393 688 159 565 458 787 244 105 383 273 738 499\n",
      " 767  24 236 799  12 441 781 575  25 466 496  32 628 121 708 977 304 250\n",
      " 131 264 850 354 285 747 113 929 573 958 382 284 498 522 272 151 877 145\n",
      " 269 940 107 440 327 406 465 494 656 110 166  15 926 163 308 428 784 404\n",
      " 596 617 605  14 917 916 366 797 699 782 211 720 411 417 965 937 647 835\n",
      " 109 820 604 157 119 254 492 532 182 602 763 914 992 409 986 362 167 783\n",
      " 554 814 377 436 987 546 715 923 419 319 673 694 201 155 342 613  40  61\n",
      " 655 900   0 724 364 143 270 415 199 899 989 464 122 216 690 230 320 806\n",
      " 402 660 641 654 584 988 816 589   8 132 104 888 185 959  60  58 501  97\n",
      " 571 689 491 880  68 108 396 271 726 144 343 260 607 500 962  96  73 443\n",
      " 237 663 232  93 148 902 291 998 609 665 664 981 453 525 463 794 479 744\n",
      " 489 399 527 788  47  33 474 222 401 356 524 993 429 341 755 103  91 473\n",
      " 286 606 255  83 648 147 195 730 348 913 821  70 847 661 256 161 457 667\n",
      " 170 513 754 725 976 736 295 778 574 583 682 557 558 439 403 623 919 535\n",
      " 630  74 963 743 669 288 468 252 804 658 827 328 149 231 711  46 261 670\n",
      " 698 361 416 175 685 495 994 874 581 578  49 268  20 615 101 873 347 511\n",
      " 257 204  92 706 645 883 461 865 156 825 918 512 384  77 376 728  30 450\n",
      " 480  63 353 312 882 223 855 219 638 298 761 449 279 903 234 915 721 884\n",
      " 828 486 287 921 970 886 313 314  23 212 811 220 153 205 942 560 346 227\n",
      " 563 674  89 999 168 306 627 329 198 826 801 534 462 722 368 350 798 657\n",
      " 957 545 275 701  72 130 691  86  62 662  90 651 832 953 759 841 455 118\n",
      " 809 684 142 635 956 938 263 719 595 776 904 570 442 541 791 531 947 192\n",
      " 863 550 582 357 478  26 338 420 872 389 831  51 735  59 967 786 881   1\n",
      " 533 251 766 927 431 751  18 952 911  43 246 160 178 878 266 506 933 764\n",
      " 138  39 857  87 508 311 853 896 924 659 789  80 843 424 591  56 756 737\n",
      "  57 750 893 600 601 414 460 614 410 561 214 982 780 129 164 317 206 385\n",
      " 387 188 324 197 739  27 753 369  71 969 818 946 810 813 618 948 459 390\n",
      " 297 567 476  75   4 869 209 340 217 249 472 939 487 983 796  55 830 553\n",
      " 330  88 423 643 680  95 325 418 228 447 290  78]\n",
      "Test:  [772 321 586 808 991 676   6 894 408 183 576 345 612 380 871 485  52  13\n",
      " 323 210 100 556 265 165 336 352 218  45 430 717  65 363 289 407 585 849\n",
      " 650 549 400 666 493 616 748 358 123 334 593 274 646 693 709 241 344 529\n",
      " 514 620 475   9 245 137 793 770  64 504 705 451 964 171 548 824 483 281\n",
      " 975 775 437 807 619 538 331 259  54 769 812 381 397  22 742 277 860 412\n",
      " 859 391 477 642 842 840 879 746 120 510 837 891 169 734 681 632 610  10\n",
      " 181 398 454 452  38 731 779 253 240 180 229 579 839   7  82 446 844 925\n",
      " 239 829 543 184 760 248 373 974 861 280 518 507 515 920 378 242 262 836\n",
      " 456 186 202 114  19 282 203 768 134  31 805 316 292 555 207 846 729 890\n",
      " 749  17 551 592 687 678 303 111 634 191 539 187 333 716 852 351 587 594\n",
      " 713 467  84 509 315 714 966 337 718 608 481 943 955 971 173 858 432 696\n",
      " 379 258 727  11 611 294 360 867 636 639 907 322 819 950 471 106 762 470\n",
      " 887 445 422 901 372 740 590 930 224 773  94 941 374 732 945 572 267  69\n",
      " 528 302 136 629  35 954 686 936 371 395 133 566 833 215 310 221]\n",
      "=========================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"==============VALIDACIÓN CRUZADA CON LENSES DATA K=5======================\")\n",
    "estrategia1 = ValidacionCruzada(5)\n",
    "estrategia1.creaParticiones(dataset)\n",
    "for particion in estrategia1.particiones:\n",
    "    print(particion)\n",
    "print(\"==========================================================================\")\n",
    "print(\"==============VALIDACIÓN CRUZADA CON TIC-TAC-TOE DATA K=8=================\")\n",
    "estrategia21 = ValidacionCruzada(8)\n",
    "estrategia21.creaParticiones(dataset2)\n",
    "for particion in estrategia21.particiones:\n",
    "    print(particion)\n",
    "print(\"=========================================================================\")\n",
    "print(\"==============VALIDACIÓN CRUZADA CON GERMAN DATA K=4=====================\")\n",
    "estrategia31 = ValidacionCruzada(4)\n",
    "estrategia31.creaParticiones(dataset3)\n",
    "for particion in estrategia31.particiones:\n",
    "    print(particion)\n",
    "print(\"=========================================================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Apartado 2: Naive-Bayes</h3>\n",
    "<p>Es un clasificador de datos que se basa en la regla de Bayes. Donde primero vamos a dividir el conjunto de datos y poder particionarlo en dos subconunto de datos con validación simple o en varios subconjunto de datos con validación cruzada. En el código que mostramos a continuación es lo que van hacer todos los clasificadores que podemos implementar. Los métodos de entrenamiento y clasifica cada clasificador lo hace el clasificador debido a que los métodos que van a utilizar son únicos y los tenemos que implementar en las clases especificas de cada clasificador.</p>\n",
    "<p>El método <strong>error</strong> va a comprobar los errores que ha obtenido nuestro clasificador, para ver el error que hemos obtenido se hace comparando la última columna de nuestra matriz de datos con la predicción que hemos obtenido en el método de clasifica del clasificador. Si es dintinto la predicción con la última columna de los datos le sumamos uno y lo vamos a dividir entre el número de lineas que tiene el subconjunto de datos Test.</p>\n",
    "<p>El método <strong>validación</strong> lo que va hacer es realizar los métodos de entrenamiento, clasifica y calcula error del clasificador seguido sin ninguna interrupcion. El método va a comprobar si le estan pasando validación simple o cruzada con el número de partciones que tiene. Si el tamaño es igual a 1 va a llamar a los métodos mencionados anteriormente y va a devolver el error que ha obtenido tras el entrenamiento y la clasificación. Si el tamaño es mayor a 1 hacemos un bucle que cubra todas las partciones para entrenarlas, clasificarlas y obtener su error, despues de la realización de esos métodos vamos a hacer la media aritmetica de todos los errores obtenidos con las diferentes particiones que tenemos en nuestra estrategia.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Clasificador:\n",
    "  # Clase abstracta\n",
    "  __metaclass__ = ABCMeta\n",
    "\n",
    "  # Metodos abstractos que se implementan en casa clasificador concreto\n",
    "  @abstractmethod\n",
    "  # TODO: esta funcion debe ser implementada en cada clasificador concreto\n",
    "  # datosTrain: matriz numpy con los datos de entrenamiento\n",
    "  # atributosDiscretos: array bool con la indicatriz de los atributos nominales\n",
    "  # diccionario: array de diccionarios de la estructura Datos utilizados para la codificacion de variables discretas\n",
    "  def entrenamiento(self, datos, datosTrain, atributosDiscretos, diccionario):\n",
    "    pass\n",
    "\n",
    "  @abstractmethod\n",
    "  # TODO: esta funcion debe ser implementada en cada clasificador concreto\n",
    "  # devuelve un numpy array con las predicciones\n",
    "  def clasifica(self, datosTest, atributosDiscretos, diccionario):\n",
    "    pass\n",
    "\n",
    "  # Obtiene el numero de aciertos y errores para calcular la tasa de fallo\n",
    "  # TODO: implementar\n",
    "  def error(self, datos, pred):\n",
    "    # Aqui se compara la prediccion (pred) con las clases reales y se calcula el error\n",
    "    i = 0\n",
    "    real = datos[:, -1]\n",
    "    error = 0\n",
    "    for i in range(len(real)):\n",
    "      if real[i] != pred[i]:\n",
    "        error += 1\n",
    "    err = (error) / (len(real) + 0.0)\n",
    "    return err\n",
    "\n",
    "  # Realiza una clasificacion utilizando una estrategia de particionado determinada\n",
    "  # TODO: implementar esta funcion\n",
    "  def validacion(self, particionado, dataset, clasificador, seed=None):\n",
    "\n",
    "    # Creamos las particiones siguiendo la estrategia llamando a particionado.creaParticiones\n",
    "    # - Para validacion cruzada: en el bucle hasta nv entrenamos el clasificador con la particion de train i\n",
    "    # y obtenemos el error en la particion de test i\n",
    "    # - Para validacion simple (hold-out): entrenamos el clasificador con la particion de train\n",
    "    # y obtenemos el error en la particion test. Otra opci�n es repetir la validaci�n simple un n�mero especificado de veces, obteniendo en cada una un error. Finalmente se calcular�a la media.\n",
    "    errores = 0\n",
    "    # particionado.creaParticiones(dataset, seed)\n",
    "    # Comprobamos si es por validación cruzada o simple, por la longitud de la lista de particiones\n",
    "\n",
    "    particionado.creaParticiones(dataset)\n",
    "    #for particion in particionado.particiones:\n",
    "    #  print(particion)\n",
    "\n",
    "    # Validación Simple\n",
    "    if len(particionado.particiones) == 1:\n",
    "      clasificador.entrenamiento(dataset, particionado.particiones[0].indicesTrain)\n",
    "      pred = clasificador.clasifica(dataset, particionado.particiones[0].indicesTest)\n",
    "      ret = self.error(dataset.extraeDatos(particionado.particiones[0].indicesTest), pred)\n",
    "      if ret > 0:\n",
    "        return ret\n",
    "      else:\n",
    "        return 0\n",
    "\n",
    "    # Validación Cruzada\n",
    "    else:\n",
    "      for particion in particionado.particiones:\n",
    "        clasificador.entrenamiento(dataset, particion.indicesTrain)\n",
    "        pred = clasificador.clasifica(dataset, particion.indicesTest)\n",
    "        ret = self.error(dataset.extraeDatos(particion.indicesTest), pred)\n",
    "        errores += ret\n",
    "      error = errores / len(particionado.particiones)\n",
    "\n",
    "      # Devolucion de la media de los errores\n",
    "      return error\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Implementación del Clasificador Naive-Bayes</h3>\n",
    "<p>En la siguiente celda vamos a poder observar el codigo de entrenamiento y clasifica del clasificador de Naive-Bayes, a continuación explicaremos brevemente el funcionamiento de cada uno de los métodos especificos del clasificador. En este claisificador podemos aplicar la regla de Laplace, que es si obtenemos un cero en algunas de las mátrices de conteos de los datos, es decir, a la hora de calcular P(D|H), tendremos que sumar 1 a todas las celdas de conteos de esa P(D|H).</p>\n",
    "<p>El método <strong>entrenamiento</strong>, lo primero que hace este método es obtener las probabilidades a priori de las clases que hay en el subconjunto de Test. Al calcular esas probabbilades las introducimos en un diccionario para poder utilizarlas más tarde. Ahora vamos a calcular la P(D|H)que se va a calcular de diferentes maneras para datos continuos o discretos\n",
    "    <ol>\n",
    "        <li><strong>Atributos discretos</strong>: se calcula haciendo los conteos de las veces que sale la P(D|H) en el subconjunto de datos Train.</li>\n",
    "        <li><strong>Atributos continuos</strong>: se calcula la media y desviación tipica del subconjunto de datos Train.</li>\n",
    "    </ol>\n",
    "Todo esto se mete en una matriz que tiene los diferentes datos y las diferentes clases del conjunto de datos, donde esa matriz la vamos a utilizar para calcular las probabilidades a posteriori con todos las datos que tiene las matrices que hemos creado.</p>\n",
    "<p>El método <strong> clasifica</strong> dependiendo de los datos que vamos obteniendo del subconjunto de datos de Test, y vamos a obtener las diferentes probabilidades de las distintas clases que tenemos el problema si nos llega ese dato. Esas probabilidades las guardamos en una lista para luego multiplicarlas por los a priori y poder coger la clase que de más probabilidad para guardarla en la lista de las predicciones de nuestro clasificador Naive-Bayes y así despues obtener el error que hemos obtenenido. Este método tambien clasifica de manera distinta los atributos discretos y continuos:\n",
    "    <ol>\n",
    "        <li><strong>Atributos discretos</strong>: se calcula obteniendo el número que hay en la matriz de probabilidades a posteriori que hemos creado anteriormente.</li>\n",
    "        <li><strong>Atributos continuos</strong>: se calcula haciendo la ecuación de la distribución normal.\n",
    "   </ol>\n",
    " </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClasificadorNaiveBayes(Clasificador):\n",
    "\n",
    "  def __init__(self, laplace):\n",
    "    self.laplace = laplace\n",
    "\n",
    "  def entrenamiento(self, dataset, datosTrain):\n",
    "\n",
    "    # Cargamos todos los datos de la clase del dataset desde la matriz de datos\n",
    "    clasesTrain = dataset.extraeDatos(datosTrain)\n",
    "    self.numClases = clasesTrain[:, -1]\n",
    "\n",
    "    # Contamos las apariciones de cada uno para luego calcular la probabilidad a priori de cada clase\n",
    "    counter = Counter(self.numClases)\n",
    "\n",
    "    # Calculamos la probabilidad de la clase y lo metemos en un diccionario ordenado segun el numero\n",
    "    # correspondiente a cada clase asignado en el diccionario\n",
    "    self.dictPrioris = {}\n",
    "    for k in counter:\n",
    "      k = int(k)\n",
    "      counter[k] = counter[k] / len(self.numClases)\n",
    "      self.dictPrioris[k] = counter[k]\n",
    "\n",
    "    # Aqui ordenamos el diccionario para que esten en el mismo orden de como extraemos los datos del dataset\n",
    "    self.dictPrioris = SortedDict(self.dictPrioris)\n",
    "\n",
    "    # Calcular tablas de probabilidades del entrenamiento. Tenemos que calcular por cada atributo una cuenta\n",
    "    # de las apariciones en cada clase\n",
    "    # Creamos una lista de matrices, donde vamos almacenar todos los datos que hemos obtenido en los datos de Test\n",
    "    self.posteriori = np.zeros(len(dataset.nombreAtributos) - 1, dtype=object)\n",
    "\n",
    "    # Recorremos todos los datos de la matriz sin llegar a la clase\n",
    "    for i in range(len(dataset.nombreAtributos) - 1):\n",
    "\n",
    "      # Si el dato que obtenemos es Nominal haremos el recuento de todas las veces que sale la P(D|H)\n",
    "      if dataset.nominalAtributos[i] == True:\n",
    "\n",
    "        # Creamos una matriz de tamaño X: Número de Atributos menos la clase Y: Número de clases\n",
    "        post = np.zeros((len(dataset.listaDicts[i]), len(dataset.listaDicts[-1])))\n",
    "\n",
    "        # Aqui contamos todos las datos que queremos del datos Train para construir la matriz de entrenamiento\n",
    "        for c in range(len(dataset.listaDicts[-1])):\n",
    "          datosEnt = dataset.extraeDatos(datosTrain)\n",
    "          dat = datosEnt[:, i]\n",
    "          repes = Counter(dat[datosEnt[:, -1] == c])\n",
    "          for r in repes:\n",
    "            post[int(r), c] = repes[r]\n",
    "          if self.laplace == True:\n",
    "            self.posteriori[i] = post + 1\n",
    "          else:\n",
    "            self.posteriori[i] = post\n",
    "\n",
    "      # Si el dato es Continuo obtendremos la media y la desviación tipica de la clase\n",
    "      else:\n",
    "\n",
    "        # Creamos una matriz de X: Los datos de Media y Desivación típica Y: Número de clases\n",
    "        post = np.zeros((2, len(dataset.listaDicts[-1])))\n",
    "\n",
    "        # Aqui obtenemos la media y desviación tipica de cada clase, despues de tener los datos de entrenamiento\n",
    "        for c in range(len(dataset.listaDicts[-1])):\n",
    "          datosEnt = dataset.extraeDatos(datosTrain)\n",
    "          dat = datosEnt[:, i]\n",
    "          datos = dat[datosEnt[:, -1] == c]\n",
    "          post[0][c] = np.mean(datos)\n",
    "          post[1][c] = np.std(datos)\n",
    "        self.posteriori[i] = post\n",
    "\n",
    "\n",
    "    # Calculamos los valores de los posteriori de todos las tablas anteriores\n",
    "    for i in range(len(dataset.listaDicts) - 1):\n",
    "      if dataset.nominalAtributos[i] == True:\n",
    "        self.posteriori[i] /= sum(self.posteriori[i])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "  def clasifica(self, dataset, datosTest):\n",
    "    acum_probs = 1\n",
    "    self.prediccion = []\n",
    "    datTest = dataset.extraeDatos(datosTest)\n",
    "\n",
    "    # Ahora vamos a estudiar la probabilidad de la clase con los datos obtenidos en el entrenamiento\n",
    "    # Recorremos todos las datos de la matriz de los datos Test\n",
    "    for dato in datTest:\n",
    "      mapa = []\n",
    "      # Aqui obtenemos los prioris de cada clase para poder obtener la probabilidad de cada una\n",
    "      for clase in range(len(self.dictPrioris)):\n",
    "        listaVerosimilitudes = []\n",
    "        # Aqui obtenemos cada valor posteriori de nuestro entrenamiento de los datos, es decir, P(D|H)\n",
    "        for atributo in range(len(self.posteriori)):\n",
    "          if dataset.nominalAtributos[atributo] == True:\n",
    "            prob = self.posteriori[atributo][int(dato[atributo])][clase]\n",
    "            listaVerosimilitudes.append(prob)\n",
    "\n",
    "          # Aqui obtenemos la probabilidad de los atibutos continuos\n",
    "          else:\n",
    "            # Hacemos la formula de la distribucion normal\n",
    "            exp1 = 1 / (self.posteriori[atributo][1][clase] * math.sqrt(2 * math.pi))\n",
    "            exp2 = np.power((dato[atributo] - self.posteriori[atributo][0][clase]), 2)\n",
    "            exp3 = np.power(self.posteriori[atributo][1][clase], 2)\n",
    "            exp4 = exp2 / exp3\n",
    "            exp4 = math.exp((-1 / 2) * exp4)\n",
    "            prob = exp1 * exp4\n",
    "            listaVerosimilitudes.append(prob)\n",
    "\n",
    "        for verosimilitud in listaVerosimilitudes:\n",
    "          acum_probs *= verosimilitud\n",
    "        acum_probs *= self.dictPrioris.get(clase)\n",
    "        mapa.append(acum_probs)\n",
    "        acum_probs = 1\n",
    "\n",
    "      # Aqui obtenemos la predicción de mayor probabilidad y la guardamos en nuestra lista de predicciones\n",
    "      self.prediccion.append(np.argmax(mapa))\n",
    "\n",
    "\n",
    "    # Devolvemos la lista con la predicción de nuestro clasifica\n",
    "    return self.prediccion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>A continuación, vamos a mostrar una ejecución del claisificador de Naive-Bayes con las diferentes validaciones y los diferentes conjuntos de datos que tenemos, al final de esto mostraremos la probabilidad que hemos obtenido</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============NAIVE-BAYES CON LENSES DATA ======================\n",
      "==============CON LAPLACE Y VALIDACION SIMPLE===================\n",
      "ERROR OBTENIDO: 0.375\n",
      "==============CON LAPLACE Y VALIDACION CRUZADA==================\n",
      "ERROR OBTENIDO: 0.335\n",
      "==============SIN LAPLACE Y VALIDACION SIMPLE===================\n",
      "ERROR OBTENIDO: 0.25\n",
      "==============SIN LAPLACE Y VALIDACION CRUZADA==================\n",
      "ERROR OBTENIDO: 0.2966666666666667\n",
      "================================================================\n",
      "==============NAIVE-BAYES CON TIC-TAC-TOE DATA==================\n",
      "==============CON LAPLACE Y VALIDACION SIMPLE===================\n",
      "ERROR OBTENIDO: 0.3333333333333333\n",
      "==============CON LAPLACE Y VALIDACION CRUZADA==================\n",
      "ERROR OBTENIDO: 0.3005646008403361\n",
      "==============SIN LAPLACE Y VALIDACION SIMPLE===================\n",
      "ERROR OBTENIDO: 0.2604166666666667\n",
      "==============SIN LAPLACE Y VALIDACION CRUZADA==================\n",
      "ERROR OBTENIDO: 0.3019899626517273\n",
      "================================================================\n",
      "==============NAIVE-BAYES CON GERMAN DATA ======================\n",
      "==============CON LAPLACE Y VALIDACION SIMPLE===================\n",
      "ERROR OBTENIDO: 0.288\n",
      "==============CON LAPLACE Y VALIDACION CRUZADA==================\n",
      "ERROR OBTENIDO: 0.256\n",
      "==============SIN LAPLACE Y VALIDACION SIMPLE===================\n",
      "ERROR OBTENIDO: 0.28\n",
      "==============SIN LAPLACE Y VALIDACION CRUZADA==================\n",
      "ERROR OBTENIDO: 0.2536666666666667\n",
      "================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"==============NAIVE-BAYES CON LENSES DATA ======================\")\n",
    "print(\"==============CON LAPLACE Y VALIDACION SIMPLE===================\")\n",
    "nb = ClasificadorNaiveBayes(True)\n",
    "error = nb.validacion(estrategia,dataset,nb)\n",
    "print(\"ERROR OBTENIDO:\",error)\n",
    "print(\"==============CON LAPLACE Y VALIDACION CRUZADA==================\")\n",
    "nb = ClasificadorNaiveBayes(True)\n",
    "error = nb.validacion(estrategia1,dataset,nb)\n",
    "print(\"ERROR OBTENIDO:\",error)\n",
    "print(\"==============SIN LAPLACE Y VALIDACION SIMPLE===================\")\n",
    "nb = ClasificadorNaiveBayes(False)\n",
    "error = nb.validacion(estrategia,dataset,nb)\n",
    "print(\"ERROR OBTENIDO:\",error)\n",
    "print(\"==============SIN LAPLACE Y VALIDACION CRUZADA==================\")\n",
    "nb = ClasificadorNaiveBayes(False)\n",
    "error = nb.validacion(estrategia1,dataset,nb)\n",
    "print(\"ERROR OBTENIDO:\",error)\n",
    "print(\"================================================================\")\n",
    "print(\"==============NAIVE-BAYES CON TIC-TAC-TOE DATA==================\")\n",
    "print(\"==============CON LAPLACE Y VALIDACION SIMPLE===================\")\n",
    "nb = ClasificadorNaiveBayes(True)\n",
    "error = nb.validacion(estrategia2,dataset2,nb)\n",
    "print(\"ERROR OBTENIDO:\",error)\n",
    "print(\"==============CON LAPLACE Y VALIDACION CRUZADA==================\")\n",
    "nb = ClasificadorNaiveBayes(True)\n",
    "error = nb.validacion(estrategia21,dataset2,nb)\n",
    "print(\"ERROR OBTENIDO:\",error)\n",
    "print(\"==============SIN LAPLACE Y VALIDACION SIMPLE===================\")\n",
    "nb = ClasificadorNaiveBayes(False)\n",
    "error = nb.validacion(estrategia2,dataset2,nb)\n",
    "print(\"ERROR OBTENIDO:\",error)\n",
    "print(\"==============SIN LAPLACE Y VALIDACION CRUZADA==================\")\n",
    "nb = ClasificadorNaiveBayes(False)\n",
    "error = nb.validacion(estrategia21,dataset2,nb)\n",
    "print(\"ERROR OBTENIDO:\",error)\n",
    "print(\"================================================================\")\n",
    "print(\"==============NAIVE-BAYES CON GERMAN DATA ======================\")\n",
    "print(\"==============CON LAPLACE Y VALIDACION SIMPLE===================\")\n",
    "nb = ClasificadorNaiveBayes(True)\n",
    "error = nb.validacion(estrategia3,dataset3,nb)\n",
    "print(\"ERROR OBTENIDO:\",error)\n",
    "print(\"==============CON LAPLACE Y VALIDACION CRUZADA==================\")\n",
    "nb = ClasificadorNaiveBayes(True)\n",
    "error = nb.validacion(estrategia31,dataset3,nb)\n",
    "print(\"ERROR OBTENIDO:\",error)\n",
    "print(\"==============SIN LAPLACE Y VALIDACION SIMPLE===================\")\n",
    "nb = ClasificadorNaiveBayes(False)\n",
    "error = nb.validacion(estrategia3,dataset3,nb)\n",
    "print(\"ERROR OBTENIDO:\",error)\n",
    "print(\"==============SIN LAPLACE Y VALIDACION CRUZADA==================\")\n",
    "nb = ClasificadorNaiveBayes(False)\n",
    "error = nb.validacion(estrategia31,dataset3,nb)\n",
    "print(\"ERROR OBTENIDO:\",error)\n",
    "print(\"================================================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Apartado 3: Scikit-Learn</h3>\n",
    "<p>Es una libreria de python que nos proporciona diferentes implementaciones del clasificador de Naive-Bayes. A continuación, vamos a explicar lo que hace cada método que hemos implementado en la celda de abajo con la libreria de Scikit-Learn.</p>\n",
    "<p>El método <strong>validacion_simple_sklearn</strong> donde le introducimos por parametro el conjunto de datos del cual queremos hacer la validación y el porcentaje que queremos tener del subconjunto de entrenamiento. La función de sklearn nos va a devolver los subconjuntos de datos de entrenamiento y de clasificación.</p>\n",
    "<p>El método <strong>validacion_cruzada_sklearn</strong> donde le introducimos por parametro el conjunto de datos del cual queremos hacer la validación y el número de partciones que vamos a hacer del conjunto de datos. El método nos va a devolver la lista de particiones que hemos obtenido con el método de sklearn.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold, cross_val_predict\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from EstrategiaParticionado import Particion\n",
    "def validacion_simple_sklearn(dataset, porcentaje):\n",
    "\n",
    "    # Matriz con los atributos\n",
    "    X = dataset.datos[:, :-1]\n",
    "\n",
    "    # Array con las clases\n",
    "    y = dataset.datos[:, -1]\n",
    "\n",
    "    # Realizamos la divison en train-test, X_train es la partición sobre la que se va a entrenar e X_test sobre la que se va a clasificar\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=porcentaje, test_size=1 - porcentaje, shuffle=True)\n",
    "\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "def validacion_cruzada_sklearn(dataset, k):\n",
    "\n",
    "    # Matriz con los atributos\n",
    "    X = dataset.datos[:, :-1]\n",
    "\n",
    "    # Array con las clases\n",
    "    y = dataset.datos[:, -1]\n",
    "\n",
    "    kf = KFold(n_splits=k, shuffle=True)\n",
    "\n",
    "    particiones = []\n",
    "\n",
    "    for train_index, test_index in kf.split(X,y):\n",
    "        particiones.append(Particion(train_index,test_index))\n",
    "\n",
    "    return particiones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============SKLEARN VALIDACIÓN SIMPLE 70% LENSES DATA==================\n",
      "TRAIN:\n",
      " [[0. 1. 1. 0.]\n",
      " [2. 0. 0. 1.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 1. 1.]\n",
      " [2. 1. 0. 0.]\n",
      " [1. 0. 1. 0.]\n",
      " [1. 1. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [1. 1. 1. 0.]\n",
      " [1. 1. 1. 1.]\n",
      " [2. 1. 0. 1.]\n",
      " [0. 1. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [1. 0. 1. 1.]\n",
      " [2. 0. 1. 1.]]\n",
      "TEST:\n",
      " [[2. 1. 1. 0.]\n",
      " [1. 0. 0. 1.]\n",
      " [0. 1. 1. 1.]\n",
      " [1. 1. 0. 1.]\n",
      " [1. 0. 0. 0.]\n",
      " [2. 1. 1. 1.]\n",
      " [2. 0. 1. 0.]\n",
      " [2. 0. 0. 0.]]\n",
      "==============SKLEARN VALIDACIÓN CRUZADA K=5 LENSES DATA=================\n",
      "Train: [ 0  1  2  4  5  6  7  8  9 10 11 12 14 17 18 20 21 22 23]\n",
      "Test:  [ 3 13 15 16 19]\n",
      "Train: [ 0  1  2  3  5  7  8  9 10 11 13 15 16 17 18 19 20 22 23]\n",
      "Test:  [ 4  6 12 14 21]\n",
      "Train: [ 0  1  2  3  4  5  6  8  9 11 12 13 14 15 16 18 19 20 21]\n",
      "Test:  [ 7 10 17 22 23]\n",
      "Train: [ 3  4  6  7  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23]\n",
      "Test:  [0 1 2 5 8]\n",
      "Train: [ 0  1  2  3  4  5  6  7  8 10 12 13 14 15 16 17 19 21 22 23]\n",
      "Test:  [ 9 11 18 20]\n",
      "=========================================================================\n",
      "==============SKLEARN VALIDACIÓN SIMPLE 80% TIC-TAC-TOE DATA=============\n",
      "TRAIN:\n",
      " [[2. 0. 2. ... 0. 1. 2.]\n",
      " [2. 2. 1. ... 1. 2. 1.]\n",
      " [2. 2. 0. ... 1. 2. 1.]\n",
      " ...\n",
      " [2. 1. 1. ... 2. 1. 1.]\n",
      " [1. 0. 1. ... 2. 0. 1.]\n",
      " [2. 0. 2. ... 2. 2. 1.]]\n",
      "TEST:\n",
      " [[1. 0. 1. ... 0. 2. 1.]\n",
      " [1. 1. 1. ... 2. 0. 2.]\n",
      " [2. 2. 1. ... 1. 1. 2.]\n",
      " ...\n",
      " [1. 0. 1. ... 2. 2. 2.]\n",
      " [2. 2. 2. ... 2. 0. 1.]\n",
      " [2. 1. 0. ... 2. 1. 2.]]\n",
      "==============SKLEARN VALIDACIÓN CRUZADA K=8 TIC-TAC-TOE DATA============\n",
      "Train: [  0   1   3   4   6   8   9  10  11  12  14  15  16  17  18  19  20  21\n",
      "  22  23  24  25  26  27  28  29  31  32  33  34  35  36  38  39  40  42\n",
      "  43  44  45  48  49  50  52  53  54  55  56  57  58  59  60  61  62  64\n",
      "  65  66  67  69  70  71  72  73  74  75  77  78  79  80  81  82  83  86\n",
      "  87  89  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105\n",
      " 106 107 109 110 111 112 113 114 115 116 117 118 120 121 122 123 124 125\n",
      " 126 128 129 130 131 132 133 135 136 137 138 139 140 141 142 143 144 145\n",
      " 146 147 148 150 151 152 153 154 155 156 157 160 161 162 163 164 165 166\n",
      " 167 168 169 170 171 172 173 174 175 176 177 180 181 182 183 184 185 186\n",
      " 187 188 189 190 192 193 194 195 196 197 198 199 200 201 202 203 204 206\n",
      " 208 210 211 212 213 214 215 216 217 218 219 220 221 223 224 225 226 227\n",
      " 228 229 231 232 233 234 235 236 237 238 240 241 243 244 245 246 247 249\n",
      " 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267\n",
      " 268 269 270 271 273 274 275 276 277 279 280 281 282 284 285 286 287 288\n",
      " 289 290 291 292 293 294 296 297 299 300 301 303 304 306 307 308 309 310\n",
      " 311 312 313 314 315 316 317 318 319 320 321 322 324 325 326 327 329 330\n",
      " 331 332 333 334 335 336 337 338 339 340 341 343 345 346 347 348 349 351\n",
      " 352 353 354 355 357 358 359 360 361 362 363 364 365 366 367 368 369 370\n",
      " 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 388 389\n",
      " 390 391 393 394 395 396 399 400 401 402 404 405 406 407 408 409 410 411\n",
      " 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 430\n",
      " 431 432 433 434 436 437 438 440 441 442 443 444 445 446 447 448 449 450\n",
      " 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 469\n",
      " 472 473 474 475 476 477 478 479 481 482 483 485 487 488 489 490 491 492\n",
      " 493 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511\n",
      " 512 513 514 515 516 517 519 520 521 522 523 524 525 526 527 528 529 530\n",
      " 531 532 533 535 536 537 539 542 544 545 546 548 549 550 551 552 553 554\n",
      " 555 556 557 558 559 560 561 563 564 565 566 567 568 569 570 571 572 573\n",
      " 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 591 592\n",
      " 593 595 596 597 598 599 600 601 603 604 605 606 607 608 609 610 611 612\n",
      " 613 616 617 618 619 622 624 625 626 627 629 630 631 632 633 635 637 638\n",
      " 639 640 641 642 643 644 645 646 647 649 650 651 652 653 654 655 656 657\n",
      " 658 660 661 662 663 664 665 667 668 669 670 671 672 673 674 675 677 679\n",
      " 680 681 682 683 685 686 687 689 690 691 692 693 694 695 697 698 699 700\n",
      " 701 702 703 704 705 706 707 708 709 710 711 712 714 715 717 718 719 721\n",
      " 722 723 724 725 726 727 728 729 730 732 733 735 737 738 739 740 741 742\n",
      " 743 744 745 746 747 749 750 751 752 753 754 756 758 759 760 761 762 763\n",
      " 765 766 767 768 769 770 772 773 774 775 776 777 778 779 780 782 783 784\n",
      " 785 786 787 789 790 791 792 794 795 797 798 799 800 801 802 803 804 805\n",
      " 806 807 808 809 811 813 814 816 817 818 819 820 821 822 823 824 826 827\n",
      " 828 829 830 831 832 834 835 836 837 838 839 840 841 842 843 844 845 846\n",
      " 847 848 849 850 851 852 854 855 857 858 859 860 861 862 863 864 865 866\n",
      " 867 868 869 870 871 872 873 874 875 876 878 879 880 881 882 884 885 886\n",
      " 887 888 889 890 891 892 893 894 896 897 898 899 900 901 902 903 904 906\n",
      " 907 908 910 911 912 913 914 915 917 918 919 920 921 922 923 924 925 926\n",
      " 927 928 929 930 932 933 934 935 936 937 938 940 941 943 944 945 946 947\n",
      " 948 949 950 951 952 953 954 955 956 957]\n",
      "Test:  [  2   5   7  13  30  37  41  46  47  51  63  68  76  84  85  88 108 119\n",
      " 127 134 149 158 159 178 179 191 205 207 209 222 230 239 242 248 272 278\n",
      " 283 295 298 302 305 323 328 342 344 350 356 387 392 397 398 403 429 435\n",
      " 439 468 470 471 480 484 486 494 518 534 538 540 541 543 547 562 590 594\n",
      " 602 614 615 620 621 623 628 634 636 648 659 666 676 678 684 688 696 713\n",
      " 716 720 731 734 736 748 755 757 764 771 781 788 793 796 810 812 815 825\n",
      " 833 853 856 877 883 895 905 909 916 931 939 942]\n",
      "Train: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  16  17  18\n",
      "  20  21  22  23  24  25  26  27  30  31  34  35  36  37  38  39  40  41\n",
      "  42  43  44  45  46  47  48  49  51  52  53  54  55  56  57  58  59  60\n",
      "  61  62  63  64  65  66  67  68  69  70  71  72  73  74  75  76  77  78\n",
      "  79  80  81  82  83  84  85  86  87  88  89  90  91  93  94  95  96  97\n",
      "  98  99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115\n",
      " 116 117 118 119 120 121 122 123 124 125 126 127 128 129 131 132 134 135\n",
      " 136 137 138 139 140 141 142 143 144 145 147 148 149 150 151 152 153 154\n",
      " 155 156 158 159 160 161 162 163 164 166 167 168 169 170 171 173 174 175\n",
      " 176 177 178 179 180 181 182 183 184 185 188 189 190 191 192 193 194 195\n",
      " 196 197 198 199 201 202 203 204 205 206 207 208 209 210 211 212 214 215\n",
      " 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 234\n",
      " 235 236 237 238 239 240 242 243 244 246 248 249 250 253 254 255 256 257\n",
      " 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 276\n",
      " 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294\n",
      " 295 296 297 298 299 300 301 302 303 304 305 307 308 309 310 311 312 313\n",
      " 315 316 317 318 319 320 321 323 324 325 326 328 330 333 335 336 337 338\n",
      " 339 340 342 344 345 346 347 349 350 351 352 353 354 355 356 357 358 360\n",
      " 361 362 363 364 365 366 367 368 369 370 371 372 373 374 377 378 379 380\n",
      " 381 382 384 385 386 387 388 390 391 392 394 395 396 397 398 399 400 401\n",
      " 402 403 404 405 408 410 411 412 413 414 415 416 417 418 419 420 421 422\n",
      " 423 424 425 426 427 428 429 430 431 432 434 435 436 437 438 439 440 441\n",
      " 442 443 444 445 446 447 448 449 450 451 453 455 456 457 459 460 461 462\n",
      " 463 464 465 466 467 468 469 470 471 472 474 475 477 480 481 482 483 484\n",
      " 485 486 487 488 489 490 491 492 493 494 496 497 498 499 500 501 502 504\n",
      " 505 507 508 509 511 512 513 514 515 517 518 519 520 521 522 524 525 526\n",
      " 527 528 529 530 532 533 534 536 537 538 539 540 541 542 543 544 545 546\n",
      " 547 548 549 550 551 552 553 554 556 557 558 559 560 561 562 563 564 565\n",
      " 566 568 569 570 571 572 573 575 576 577 579 580 581 582 584 586 588 590\n",
      " 591 592 593 594 595 596 597 600 601 602 603 604 605 606 607 609 610 611\n",
      " 612 613 614 615 616 617 618 619 620 621 622 623 624 627 628 629 630 631\n",
      " 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 650\n",
      " 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 669\n",
      " 670 671 672 673 674 675 676 677 678 679 680 681 682 684 685 686 688 689\n",
      " 690 692 693 695 696 697 698 700 701 702 703 704 706 707 708 709 710 711\n",
      " 712 713 714 715 716 717 719 720 721 722 723 725 726 728 729 730 731 732\n",
      " 734 735 736 737 739 740 744 746 747 748 749 750 751 753 755 756 757 759\n",
      " 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 776 777 778\n",
      " 779 780 781 787 788 789 790 791 792 793 794 795 796 797 798 799 801 802\n",
      " 804 805 807 808 809 810 811 812 813 815 816 817 818 819 820 821 823 824\n",
      " 825 826 827 828 829 830 831 832 833 835 836 837 838 839 840 841 842 843\n",
      " 844 846 847 849 851 852 853 854 855 856 857 859 860 861 862 863 864 865\n",
      " 866 867 869 870 871 872 873 875 876 877 878 879 881 882 883 884 885 886\n",
      " 887 888 889 890 893 894 895 896 897 898 899 901 902 903 904 905 906 908\n",
      " 909 910 911 912 913 914 915 916 917 918 919 920 921 922 923 924 925 926\n",
      " 927 928 929 930 931 932 933 935 936 937 938 939 940 941 942 943 944 946\n",
      " 947 948 949 950 951 953 954 955 956 957]\n",
      "Test:  [ 15  19  28  29  32  33  50  92 130 133 146 157 165 172 186 187 200 213\n",
      " 233 241 245 247 251 252 275 306 314 322 327 329 331 332 334 341 343 348\n",
      " 359 375 376 383 389 393 406 407 409 433 452 454 458 473 476 478 479 495\n",
      " 503 506 510 516 523 531 535 555 567 574 578 583 585 587 589 598 599 608\n",
      " 625 626 649 668 683 687 691 694 699 705 718 724 727 733 738 741 742 743\n",
      " 745 752 754 758 775 782 783 784 785 786 800 803 806 814 822 834 845 848\n",
      " 850 858 868 874 880 891 892 900 907 934 945 952]\n",
      "Train: [  0   1   2   3   4   5   6   7   8  10  11  12  13  14  15  16  17  18\n",
      "  19  20  23  24  25  26  27  28  29  30  31  32  33  34  36  37  38  39\n",
      "  41  42  44  45  46  47  48  49  50  51  52  53  54  56  57  58  61  62\n",
      "  63  67  68  69  70  74  75  76  78  81  84  85  86  87  88  89  90  91\n",
      "  92  93  94  95  97  98  99 100 101 102 103 104 105 106 107 108 109 110\n",
      " 111 113 114 115 116 117 118 119 120 122 124 125 126 127 129 130 131 132\n",
      " 133 134 135 136 137 139 140 141 142 143 144 145 146 148 149 151 152 153\n",
      " 154 155 157 158 159 160 162 163 164 165 166 167 168 169 170 171 172 174\n",
      " 175 176 177 178 179 180 182 183 185 186 187 188 189 191 192 193 194 195\n",
      " 196 197 198 199 200 201 202 203 205 206 207 208 209 210 211 212 213 214\n",
      " 215 216 217 218 219 220 221 222 223 225 226 227 228 229 230 231 232 233\n",
      " 234 235 236 237 238 239 240 241 242 243 245 246 247 248 249 250 251 252\n",
      " 253 254 255 256 257 258 259 262 263 264 265 266 267 268 269 270 272 273\n",
      " 274 275 276 277 278 280 281 282 283 284 286 287 288 289 290 291 292 293\n",
      " 294 295 296 297 298 299 300 301 302 303 304 305 306 307 309 310 311 312\n",
      " 313 314 315 316 317 318 320 321 322 323 325 326 327 328 329 330 331 332\n",
      " 333 334 335 336 337 339 341 342 343 344 345 346 348 349 350 352 354 355\n",
      " 356 358 359 360 362 364 365 366 367 368 371 374 375 376 377 378 379 380\n",
      " 381 382 383 384 386 387 388 389 390 391 392 393 394 396 397 398 400 401\n",
      " 402 403 404 405 406 407 408 409 410 412 413 414 415 416 417 418 419 420\n",
      " 422 423 424 425 426 427 428 429 430 431 433 434 435 436 437 438 439 440\n",
      " 441 442 443 444 446 447 448 450 451 452 453 454 455 456 457 458 459 462\n",
      " 463 464 465 466 467 468 469 470 471 473 474 476 477 478 479 480 481 482\n",
      " 483 484 485 486 487 488 489 490 491 492 493 494 495 497 499 500 501 502\n",
      " 503 504 505 506 508 509 510 511 512 513 514 515 516 517 518 519 521 522\n",
      " 523 524 525 526 528 529 531 532 533 534 535 536 537 538 539 540 541 542\n",
      " 543 544 545 546 547 549 550 551 552 554 555 557 558 559 560 561 562 563\n",
      " 564 566 567 569 570 572 573 574 575 576 577 578 579 580 583 584 585 586\n",
      " 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604\n",
      " 605 606 607 608 609 610 611 612 613 614 615 616 617 619 620 621 622 623\n",
      " 624 625 626 628 629 630 631 632 633 634 635 636 637 638 640 642 643 644\n",
      " 645 646 647 648 649 650 651 652 654 655 656 657 659 660 661 662 664 665\n",
      " 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683\n",
      " 684 685 686 687 688 689 690 691 692 693 694 695 696 698 699 700 701 702\n",
      " 703 704 705 708 710 712 713 714 715 716 718 719 720 721 722 724 725 726\n",
      " 727 728 729 731 732 733 734 735 736 738 740 741 742 743 745 746 747 748\n",
      " 749 750 751 752 754 755 757 758 759 760 761 762 763 764 765 766 767 768\n",
      " 769 770 771 772 773 774 775 776 777 778 779 780 781 782 783 784 785 786\n",
      " 787 788 789 790 792 793 794 795 796 798 799 800 801 802 803 804 805 806\n",
      " 808 809 810 811 812 813 814 815 816 817 818 819 820 821 822 823 825 827\n",
      " 829 830 832 833 834 835 836 838 839 840 841 842 843 844 845 846 847 848\n",
      " 849 850 851 853 854 855 856 857 858 859 860 861 862 863 864 865 866 867\n",
      " 868 869 871 872 873 874 875 876 877 878 879 880 881 882 883 884 886 887\n",
      " 888 889 890 891 892 893 894 895 896 897 898 899 900 901 902 903 904 905\n",
      " 906 907 909 910 911 912 913 914 915 916 917 918 919 920 922 924 925 926\n",
      " 927 929 930 931 932 934 935 936 937 938 939 940 941 942 943 944 945 947\n",
      " 948 949 950 951 952 953 954 955 956 957]\n",
      "Test:  [  9  21  22  35  40  43  55  59  60  64  65  66  71  72  73  77  79  80\n",
      "  82  83  96 112 121 123 128 138 147 150 156 161 173 181 184 190 204 224\n",
      " 244 260 261 271 279 285 308 319 324 338 340 347 351 353 357 361 363 369\n",
      " 370 372 373 385 395 399 411 421 432 445 449 460 461 472 475 496 498 507\n",
      " 520 527 530 548 553 556 565 568 571 581 582 618 627 639 641 653 658 663\n",
      " 697 706 707 709 711 717 723 730 737 739 744 753 756 791 797 807 824 826\n",
      " 828 831 837 852 870 885 908 921 923 928 933 946]\n",
      "Train: [  1   2   3   4   5   6   7   9  10  11  12  13  14  15  16  17  19  20\n",
      "  21  22  23  24  25  26  28  29  30  32  33  34  35  36  37  38  39  40\n",
      "  41  42  43  44  46  47  48  49  50  51  53  54  55  56  57  58  59  60\n",
      "  61  62  63  64  65  66  67  68  69  70  71  72  73  74  75  76  77  78\n",
      "  79  80  81  82  83  84  85  86  87  88  89  90  91  92  93  94  95  96\n",
      "  97  99 100 101 102 103 106 107 108 110 111 112 113 114 115 117 118 119\n",
      " 120 121 122 123 124 126 127 128 130 131 132 133 134 135 136 137 138 140\n",
      " 141 142 143 144 145 146 147 148 149 150 151 152 155 156 157 158 159 160\n",
      " 161 162 163 164 165 167 169 170 171 172 173 174 178 179 180 181 182 184\n",
      " 185 186 187 188 189 190 191 192 193 194 195 196 199 200 201 202 203 204\n",
      " 205 206 207 208 209 210 212 213 214 215 216 217 218 219 220 221 222 223\n",
      " 224 226 227 228 230 233 234 236 238 239 240 241 242 243 244 245 247 248\n",
      " 250 251 252 253 254 255 258 260 261 262 263 265 267 268 269 271 272 273\n",
      " 274 275 276 278 279 280 281 282 283 284 285 286 287 288 290 291 292 294\n",
      " 295 296 298 299 300 301 302 303 304 305 306 307 308 309 310 311 313 314\n",
      " 315 316 319 320 322 323 324 325 326 327 328 329 330 331 332 333 334 335\n",
      " 336 337 338 339 340 341 342 343 344 346 347 348 349 350 351 352 353 354\n",
      " 355 356 357 358 359 361 362 363 364 365 366 368 369 370 371 372 373 374\n",
      " 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392\n",
      " 393 394 395 396 397 398 399 401 403 404 406 407 408 409 410 411 413 414\n",
      " 416 417 418 419 420 421 422 423 424 425 428 429 430 431 432 433 434 435\n",
      " 436 437 439 441 442 444 445 446 447 448 449 450 451 452 453 454 455 456\n",
      " 457 458 459 460 461 462 463 464 465 467 468 469 470 471 472 473 474 475\n",
      " 476 478 479 480 481 482 483 484 486 489 490 491 492 493 494 495 496 497\n",
      " 498 499 501 502 503 504 505 506 507 508 509 510 512 513 514 515 516 518\n",
      " 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536\n",
      " 537 538 539 540 541 542 543 544 545 547 548 549 551 553 555 556 557 558\n",
      " 559 560 561 562 564 565 566 567 568 569 570 571 572 574 575 576 577 578\n",
      " 579 580 581 582 583 584 585 586 587 589 590 591 592 594 595 596 598 599\n",
      " 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 618\n",
      " 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636\n",
      " 637 639 641 643 644 645 646 647 648 649 650 651 652 653 654 656 657 658\n",
      " 659 661 663 664 666 667 668 669 670 672 673 674 675 676 678 679 680 681\n",
      " 682 683 684 685 686 687 688 689 690 691 692 693 694 696 697 698 699 700\n",
      " 701 702 703 705 706 707 708 709 710 711 712 713 714 716 717 718 719 720\n",
      " 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739\n",
      " 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757\n",
      " 758 759 761 762 764 765 766 768 769 770 771 772 774 775 776 778 779 780\n",
      " 781 782 783 784 785 786 787 788 789 791 792 793 794 795 796 797 798 799\n",
      " 800 801 802 803 805 806 807 810 811 812 813 814 815 816 817 818 820 821\n",
      " 822 823 824 825 826 827 828 830 831 832 833 834 835 836 837 838 839 840\n",
      " 841 842 843 844 845 846 847 848 849 850 852 853 854 855 856 858 859 860\n",
      " 861 864 865 866 868 869 870 872 873 874 875 877 878 880 881 882 883 885\n",
      " 886 891 892 893 894 895 896 897 898 899 900 901 904 905 906 907 908 909\n",
      " 910 911 912 913 914 915 916 917 918 919 920 921 922 923 924 925 926 927\n",
      " 928 929 930 931 932 933 934 935 936 937 938 939 941 942 943 944 945 946\n",
      " 947 948 949 950 952 953 954 955 956 957]\n",
      "Test:  [  0   8  18  27  31  45  52  98 104 105 109 116 125 129 139 153 154 166\n",
      " 168 175 176 177 183 197 198 211 225 229 231 232 235 237 246 249 256 257\n",
      " 259 264 266 270 277 289 293 297 312 317 318 321 345 360 367 400 402 405\n",
      " 412 415 426 427 438 440 443 466 477 485 487 488 500 511 517 546 550 552\n",
      " 554 563 573 588 593 597 617 638 640 642 655 660 662 665 671 677 695 704\n",
      " 715 721 760 763 767 773 777 790 804 808 809 819 829 851 857 862 863 867\n",
      " 871 876 879 884 887 888 889 890 902 903 940 951]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [  0   1   2   4   5   6   7   8   9  10  12  13  14  15  16  18  19  21\n",
      "  22  23  25  26  27  28  29  30  31  32  33  34  35  37  40  41  42  43\n",
      "  44  45  46  47  48  49  50  51  52  54  55  58  59  60  63  64  65  66\n",
      "  67  68  71  72  73  75  76  77  78  79  80  82  83  84  85  88  90  91\n",
      "  92  93  95  96  98 100 101 103 104 105 106 107 108 109 112 113 115 116\n",
      " 119 120 121 122 123 124 125 126 127 128 129 130 131 133 134 137 138 139\n",
      " 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157\n",
      " 158 159 161 162 163 164 165 166 168 169 170 172 173 174 175 176 177 178\n",
      " 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 195 196 197\n",
      " 198 200 201 202 203 204 205 206 207 209 210 211 213 214 215 216 217 219\n",
      " 220 221 222 223 224 225 226 227 229 230 231 232 233 235 236 237 238 239\n",
      " 241 242 243 244 245 246 247 248 249 250 251 252 256 257 258 259 260 261\n",
      " 263 264 265 266 268 269 270 271 272 273 274 275 276 277 278 279 280 281\n",
      " 282 283 285 286 287 288 289 290 293 294 295 296 297 298 299 300 301 302\n",
      " 303 304 305 306 307 308 309 310 311 312 314 315 316 317 318 319 320 321\n",
      " 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339\n",
      " 340 341 342 343 344 345 347 348 350 351 352 353 354 355 356 357 358 359\n",
      " 360 361 362 363 367 368 369 370 371 372 373 374 375 376 377 378 381 382\n",
      " 383 384 385 386 387 389 390 392 393 394 395 396 397 398 399 400 401 402\n",
      " 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420\n",
      " 421 422 423 425 426 427 428 429 430 431 432 433 435 436 437 438 439 440\n",
      " 442 443 444 445 446 447 449 450 452 453 454 455 457 458 459 460 461 462\n",
      " 463 466 467 468 470 471 472 473 474 475 476 477 478 479 480 481 483 484\n",
      " 485 486 487 488 489 490 491 493 494 495 496 497 498 499 500 502 503 505\n",
      " 506 507 508 509 510 511 513 514 515 516 517 518 519 520 521 522 523 524\n",
      " 525 526 527 528 529 530 531 532 533 534 535 536 537 538 540 541 542 543\n",
      " 544 545 546 547 548 550 551 552 553 554 555 556 557 558 559 561 562 563\n",
      " 565 566 567 568 569 570 571 572 573 574 576 577 578 579 580 581 582 583\n",
      " 584 585 586 587 588 589 590 592 593 594 596 597 598 599 600 601 602 604\n",
      " 606 607 608 609 610 611 612 614 615 616 617 618 619 620 621 623 624 625\n",
      " 626 627 628 629 630 631 632 633 634 635 636 638 639 640 641 642 643 644\n",
      " 646 647 648 649 650 652 653 654 655 657 658 659 660 661 662 663 664 665\n",
      " 666 667 668 670 671 672 673 674 676 677 678 679 680 681 682 683 684 685\n",
      " 686 687 688 689 690 691 692 694 695 696 697 698 699 702 704 705 706 707\n",
      " 708 709 710 711 713 714 715 716 717 718 719 720 721 722 723 724 725 727\n",
      " 728 729 730 731 732 733 734 736 737 738 739 740 741 742 743 744 745 746\n",
      " 747 748 749 750 751 752 753 754 755 756 757 758 760 761 762 763 764 765\n",
      " 766 767 768 769 770 771 772 773 775 776 777 778 779 780 781 782 783 784\n",
      " 785 786 788 789 790 791 793 794 796 797 798 799 800 801 802 803 804 805\n",
      " 806 807 808 809 810 811 812 813 814 815 816 817 818 819 820 821 822 823\n",
      " 824 825 826 827 828 829 830 831 832 833 834 835 836 837 838 839 840 842\n",
      " 843 844 845 848 849 850 851 852 853 854 855 856 857 858 859 861 862 863\n",
      " 864 865 867 868 869 870 871 874 875 876 877 879 880 881 882 883 884 885\n",
      " 886 887 888 889 890 891 892 895 896 897 898 899 900 901 902 903 904 905\n",
      " 906 907 908 909 910 911 912 913 914 915 916 917 918 919 920 921 923 924\n",
      " 925 926 927 928 929 930 931 932 933 934 935 937 938 939 940 942 944 945\n",
      " 946 947 948 950 951 952 954 955 956 957]\n",
      "Test:  [  3  11  17  20  24  36  38  39  53  56  57  61  62  69  70  74  81  86\n",
      "  87  89  94  97  99 102 110 111 114 117 118 132 135 136 160 167 171 194\n",
      " 199 208 212 218 228 234 240 253 254 255 262 267 284 291 292 313 346 349\n",
      " 364 365 366 379 380 388 391 424 434 441 448 451 456 464 465 469 482 492\n",
      " 501 504 512 539 549 560 564 575 591 595 603 605 613 622 637 645 651 656\n",
      " 669 675 693 700 701 703 712 726 735 759 774 787 792 795 841 846 847 860\n",
      " 866 872 873 878 893 894 922 936 941 943 949 953]\n",
      "Train: [  0   1   2   3   4   5   7   8   9  11  13  15  16  17  18  19  20  21\n",
      "  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36  37  38  39\n",
      "  40  41  42  43  44  45  46  47  50  51  52  53  54  55  56  57  58  59\n",
      "  60  61  62  63  64  65  66  67  68  69  70  71  72  73  74  75  76  77\n",
      "  79  80  81  82  83  84  85  86  87  88  89  90  92  93  94  95  96  97\n",
      "  98  99 100 102 103 104 105 106 108 109 110 111 112 113 114 116 117 118\n",
      " 119 120 121 123 125 127 128 129 130 132 133 134 135 136 137 138 139 140\n",
      " 141 142 143 144 145 146 147 149 150 152 153 154 156 157 158 159 160 161\n",
      " 162 164 165 166 167 168 170 171 172 173 174 175 176 177 178 179 180 181\n",
      " 183 184 186 187 188 189 190 191 193 194 195 196 197 198 199 200 201 202\n",
      " 204 205 207 208 209 210 211 212 213 214 216 217 218 219 221 222 223 224\n",
      " 225 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243\n",
      " 244 245 246 247 248 249 251 252 253 254 255 256 257 258 259 260 261 262\n",
      " 263 264 266 267 269 270 271 272 273 274 275 277 278 279 281 283 284 285\n",
      " 286 288 289 291 292 293 294 295 296 297 298 299 302 303 305 306 307 308\n",
      " 309 310 312 313 314 315 317 318 319 320 321 322 323 324 326 327 328 329\n",
      " 330 331 332 333 334 335 336 337 338 340 341 342 343 344 345 346 347 348\n",
      " 349 350 351 352 353 354 355 356 357 358 359 360 361 363 364 365 366 367\n",
      " 369 370 371 372 373 375 376 378 379 380 381 382 383 384 385 386 387 388\n",
      " 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 405 406 407\n",
      " 409 411 412 414 415 416 417 419 420 421 422 423 424 426 427 429 430 432\n",
      " 433 434 435 436 437 438 439 440 441 442 443 444 445 446 448 449 450 451\n",
      " 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 468 469 470\n",
      " 471 472 473 475 476 477 478 479 480 481 482 484 485 486 487 488 490 492\n",
      " 493 494 495 496 497 498 499 500 501 502 503 504 506 507 508 510 511 512\n",
      " 513 515 516 517 518 519 520 522 523 524 526 527 528 529 530 531 533 534\n",
      " 535 536 538 539 540 541 543 544 545 546 547 548 549 550 551 552 553 554\n",
      " 555 556 557 558 559 560 561 562 563 564 565 567 568 569 570 571 572 573\n",
      " 574 575 576 578 580 581 582 583 584 585 587 588 589 590 591 592 593 594\n",
      " 595 597 598 599 601 602 603 605 607 608 610 611 612 613 614 615 617 618\n",
      " 620 621 622 623 624 625 626 627 628 629 630 632 633 634 635 636 637 638\n",
      " 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656\n",
      " 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674\n",
      " 675 676 677 678 679 680 682 683 684 685 686 687 688 691 693 694 695 696\n",
      " 697 698 699 700 701 702 703 704 705 706 707 709 711 712 713 714 715 716\n",
      " 717 718 720 721 723 724 725 726 727 728 730 731 732 733 734 735 736 737\n",
      " 738 739 740 741 742 743 744 745 747 748 749 750 752 753 754 755 756 757\n",
      " 758 759 760 761 762 763 764 765 767 768 770 771 772 773 774 775 776 777\n",
      " 778 780 781 782 783 784 785 786 787 788 789 790 791 792 793 794 795 796\n",
      " 797 798 799 800 801 802 803 804 806 807 808 809 810 811 812 814 815 816\n",
      " 817 819 821 822 823 824 825 826 828 829 830 831 833 834 835 837 838 839\n",
      " 840 841 842 843 845 846 847 848 849 850 851 852 853 854 855 856 857 858\n",
      " 859 860 862 863 866 867 868 870 871 872 873 874 875 876 877 878 879 880\n",
      " 883 884 885 886 887 888 889 890 891 892 893 894 895 897 898 900 901 902\n",
      " 903 904 905 906 907 908 909 910 913 915 916 917 918 920 921 922 923 924\n",
      " 925 926 928 929 930 931 932 933 934 936 938 939 940 941 942 943 944 945\n",
      " 946 947 949 950 951 952 953 954 956 957]\n",
      "Test:  [  6  10  12  14  48  49  78  91 101 107 115 122 124 126 131 148 151 155\n",
      " 163 169 182 185 192 203 206 215 220 226 250 265 268 276 280 282 287 290\n",
      " 300 301 304 311 316 325 339 362 368 374 377 404 408 410 413 418 425 428\n",
      " 431 447 467 474 483 489 491 505 509 514 521 525 532 537 542 566 577 579\n",
      " 586 596 600 604 606 609 616 619 631 681 689 690 692 708 710 719 722 729\n",
      " 746 751 766 769 779 805 813 818 820 827 832 836 844 861 864 865 869 881\n",
      " 882 896 899 911 912 914 919 927 935 937 948 955]\n",
      "Train: [  0   1   2   3   5   6   7   8   9  10  11  12  13  14  15  16  17  18\n",
      "  19  20  21  22  23  24  25  27  28  29  30  31  32  33  35  36  37  38\n",
      "  39  40  41  43  45  46  47  48  49  50  51  52  53  54  55  56  57  59\n",
      "  60  61  62  63  64  65  66  68  69  70  71  72  73  74  75  76  77  78\n",
      "  79  80  81  82  83  84  85  86  87  88  89  91  92  94  96  97  98  99\n",
      " 101 102 103 104 105 107 108 109 110 111 112 113 114 115 116 117 118 119\n",
      " 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137\n",
      " 138 139 140 143 144 146 147 148 149 150 151 153 154 155 156 157 158 159\n",
      " 160 161 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178\n",
      " 179 181 182 183 184 185 186 187 188 189 190 191 192 194 195 197 198 199\n",
      " 200 203 204 205 206 207 208 209 210 211 212 213 215 216 218 219 220 221\n",
      " 222 223 224 225 226 227 228 229 230 231 232 233 234 235 237 239 240 241\n",
      " 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 259 260\n",
      " 261 262 264 265 266 267 268 270 271 272 275 276 277 278 279 280 282 283\n",
      " 284 285 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302\n",
      " 303 304 305 306 308 311 312 313 314 316 317 318 319 321 322 323 324 325\n",
      " 326 327 328 329 330 331 332 334 335 336 337 338 339 340 341 342 343 344\n",
      " 345 346 347 348 349 350 351 353 354 355 356 357 358 359 360 361 362 363\n",
      " 364 365 366 367 368 369 370 372 373 374 375 376 377 379 380 382 383 385\n",
      " 386 387 388 389 390 391 392 393 395 396 397 398 399 400 401 402 403 404\n",
      " 405 406 407 408 409 410 411 412 413 414 415 417 418 419 420 421 422 424\n",
      " 425 426 427 428 429 430 431 432 433 434 435 437 438 439 440 441 443 445\n",
      " 446 447 448 449 450 451 452 454 456 457 458 459 460 461 463 464 465 466\n",
      " 467 468 469 470 471 472 473 474 475 476 477 478 479 480 482 483 484 485\n",
      " 486 487 488 489 490 491 492 493 494 495 496 497 498 500 501 502 503 504\n",
      " 505 506 507 509 510 511 512 514 516 517 518 520 521 522 523 525 526 527\n",
      " 528 530 531 532 534 535 536 537 538 539 540 541 542 543 546 547 548 549\n",
      " 550 552 553 554 555 556 557 558 560 561 562 563 564 565 566 567 568 569\n",
      " 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588\n",
      " 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606\n",
      " 607 608 609 613 614 615 616 617 618 619 620 621 622 623 625 626 627 628\n",
      " 629 631 632 634 636 637 638 639 640 641 642 644 645 647 648 649 650 651\n",
      " 653 655 656 657 658 659 660 662 663 664 665 666 668 669 671 674 675 676\n",
      " 677 678 680 681 683 684 685 686 687 688 689 690 691 692 693 694 695 696\n",
      " 697 699 700 701 703 704 705 706 707 708 709 710 711 712 713 715 716 717\n",
      " 718 719 720 721 722 723 724 726 727 729 730 731 733 734 735 736 737 738\n",
      " 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756\n",
      " 757 758 759 760 761 763 764 766 767 769 771 772 773 774 775 777 778 779\n",
      " 780 781 782 783 784 785 786 787 788 790 791 792 793 794 795 796 797 800\n",
      " 801 803 804 805 806 807 808 809 810 811 812 813 814 815 816 817 818 819\n",
      " 820 821 822 824 825 826 827 828 829 830 831 832 833 834 836 837 838 841\n",
      " 844 845 846 847 848 850 851 852 853 854 856 857 858 859 860 861 862 863\n",
      " 864 865 866 867 868 869 870 871 872 873 874 876 877 878 879 880 881 882\n",
      " 883 884 885 886 887 888 889 890 891 892 893 894 895 896 897 898 899 900\n",
      " 902 903 905 906 907 908 909 910 911 912 913 914 915 916 917 919 920 921\n",
      " 922 923 924 927 928 930 931 933 934 935 936 937 938 939 940 941 942 943\n",
      " 945 946 948 949 950 951 952 953 954 955 956]\n",
      "Test:  [  4  26  34  42  44  58  67  90  93  95 100 106 141 142 145 152 162 180\n",
      " 193 196 201 202 214 217 236 238 258 263 269 273 274 281 286 307 309 310\n",
      " 315 320 333 352 371 378 381 384 394 416 423 436 442 444 453 455 462 481\n",
      " 499 508 513 515 519 524 529 533 544 545 551 559 570 610 611 612 624 630\n",
      " 633 635 643 646 652 654 661 667 670 672 673 679 682 698 702 714 725 728\n",
      " 732 762 765 768 770 776 789 798 799 802 823 835 839 840 842 843 849 855\n",
      " 875 901 904 918 925 926 929 932 944 947 957]\n",
      "Train: [  0   2   3   4   5   6   7   8   9  10  11  12  13  14  15  17  18  19\n",
      "  20  21  22  24  26  27  28  29  30  31  32  33  34  35  36  37  38  39\n",
      "  40  41  42  43  44  45  46  47  48  49  50  51  52  53  55  56  57  58\n",
      "  59  60  61  62  63  64  65  66  67  68  69  70  71  72  73  74  76  77\n",
      "  78  79  80  81  82  83  84  85  86  87  88  89  90  91  92  93  94  95\n",
      "  96  97  98  99 100 101 102 104 105 106 107 108 109 110 111 112 114 115\n",
      " 116 117 118 119 121 122 123 124 125 126 127 128 129 130 131 132 133 134\n",
      " 135 136 138 139 141 142 145 146 147 148 149 150 151 152 153 154 155 156\n",
      " 157 158 159 160 161 162 163 165 166 167 168 169 171 172 173 175 176 177\n",
      " 178 179 180 181 182 183 184 185 186 187 190 191 192 193 194 196 197 198\n",
      " 199 200 201 202 203 204 205 206 207 208 209 211 212 213 214 215 217 218\n",
      " 220 222 224 225 226 228 229 230 231 232 233 234 235 236 237 238 239 240\n",
      " 241 242 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259\n",
      " 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277\n",
      " 278 279 280 281 282 283 284 285 286 287 289 290 291 292 293 295 297 298\n",
      " 300 301 302 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318\n",
      " 319 320 321 322 323 324 325 327 328 329 331 332 333 334 338 339 340 341\n",
      " 342 343 344 345 346 347 348 349 350 351 352 353 356 357 359 360 361 362\n",
      " 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380\n",
      " 381 383 384 385 387 388 389 391 392 393 394 395 397 398 399 400 402 403\n",
      " 404 405 406 407 408 409 410 411 412 413 415 416 418 421 423 424 425 426\n",
      " 427 428 429 431 432 433 434 435 436 438 439 440 441 442 443 444 445 447\n",
      " 448 449 451 452 453 454 455 456 458 460 461 462 464 465 466 467 468 469\n",
      " 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487\n",
      " 488 489 491 492 494 495 496 498 499 500 501 503 504 505 506 507 508 509\n",
      " 510 511 512 513 514 515 516 517 518 519 520 521 523 524 525 527 529 530\n",
      " 531 532 533 534 535 537 538 539 540 541 542 543 544 545 546 547 548 549\n",
      " 550 551 552 553 554 555 556 559 560 562 563 564 565 566 567 568 570 571\n",
      " 573 574 575 577 578 579 581 582 583 585 586 587 588 589 590 591 593 594\n",
      " 595 596 597 598 599 600 602 603 604 605 606 608 609 610 611 612 613 614\n",
      " 615 616 617 618 619 620 621 622 623 624 625 626 627 628 630 631 633 634\n",
      " 635 636 637 638 639 640 641 642 643 645 646 648 649 651 652 653 654 655\n",
      " 656 658 659 660 661 662 663 665 666 667 668 669 670 671 672 673 675 676\n",
      " 677 678 679 681 682 683 684 687 688 689 690 691 692 693 694 695 696 697\n",
      " 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715\n",
      " 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733\n",
      " 734 735 736 737 738 739 741 742 743 744 745 746 748 751 752 753 754 755\n",
      " 756 757 758 759 760 762 763 764 765 766 767 768 769 770 771 773 774 775\n",
      " 776 777 779 781 782 783 784 785 786 787 788 789 790 791 792 793 795 796\n",
      " 797 798 799 800 802 803 804 805 806 807 808 809 810 812 813 814 815 818\n",
      " 819 820 822 823 824 825 826 827 828 829 831 832 833 834 835 836 837 839\n",
      " 840 841 842 843 844 845 846 847 848 849 850 851 852 853 855 856 857 858\n",
      " 860 861 862 863 864 865 866 867 868 869 870 871 872 873 874 875 876 877\n",
      " 878 879 880 881 882 883 884 885 887 888 889 890 891 892 893 894 895 896\n",
      " 899 900 901 902 903 904 905 907 908 909 911 912 914 916 918 919 921 922\n",
      " 923 925 926 927 928 929 931 932 933 934 935 936 937 939 940 941 942 943\n",
      " 944 945 946 947 948 949 951 952 953 955 957]\n",
      "Test:  [  1  16  23  25  54  75 103 113 120 137 140 143 144 164 170 174 188 189\n",
      " 195 210 216 219 221 223 227 243 288 294 296 299 303 326 330 335 336 337\n",
      " 354 355 358 382 386 390 396 401 414 417 419 420 422 430 437 446 450 457\n",
      " 459 463 490 493 497 502 522 526 528 536 557 558 561 569 572 576 580 584\n",
      " 592 601 607 629 632 644 647 650 657 664 674 680 685 686 740 747 749 750\n",
      " 761 772 778 780 794 801 811 816 817 821 830 838 854 859 886 897 898 906\n",
      " 910 913 915 917 920 924 930 938 950 954 956]\n",
      "=========================================================================\n",
      "==============SKLEARN VALIDACIÓN SIMPLE 75% GERMAN DATA==================\n",
      "TRAIN:\n",
      " [[ 3. 15.  4. ...  1.  0.  0.]\n",
      " [ 3. 12.  2. ...  1.  1.  0.]\n",
      " [ 0. 36.  2. ...  1.  1.  0.]\n",
      " ...\n",
      " [ 2. 18.  1. ...  1.  0.  0.]\n",
      " [ 3. 12.  2. ...  1.  0.  0.]\n",
      " [ 0. 24.  2. ...  1.  0.  1.]]\n",
      "TEST:\n",
      " [[ 0. 21.  3. ...  1.  0.  0.]\n",
      " [ 1. 36.  2. ...  1.  0.  0.]\n",
      " [ 1. 48.  2. ...  1.  0.  0.]\n",
      " ...\n",
      " [ 3. 24.  2. ...  1.  1.  0.]\n",
      " [ 3. 24.  2. ...  1.  0.  0.]\n",
      " [ 1. 18.  2. ...  1.  0.  0.]]\n",
      "==============SKLEARN VALIDACIÓN CRUZADA K=4 GERMAN DATA=================\n",
      "Train: [  1   2   3   7   8   9  12  14  15  16  17  19  20  21  23  25  26  27\n",
      "  28  29  30  31  32  33  34  35  36  37  38  40  41  44  47  48  49  50\n",
      "  51  52  54  56  57  59  60  61  62  63  66  68  69  70  71  73  75  76\n",
      "  77  78  79  80  83  84  85  86  87  89  90  91  93  94  95  96  98 100\n",
      " 101 102 104 106 108 109 110 112 113 114 115 116 117 118 119 120 121 122\n",
      " 123 124 129 131 132 133 134 136 137 138 140 141 144 145 146 147 148 149\n",
      " 150 151 153 154 156 157 158 160 161 163 166 167 169 170 171 172 176 177\n",
      " 178 179 180 181 182 183 184 185 188 189 190 191 192 193 194 197 198 199\n",
      " 200 201 202 203 204 206 207 208 209 211 212 213 214 215 216 217 218 219\n",
      " 220 221 222 223 224 225 226 227 228 229 230 231 232 233 235 239 240 241\n",
      " 242 244 245 246 247 248 249 250 251 254 255 256 259 262 263 265 266 267\n",
      " 269 271 272 273 275 278 280 281 282 284 286 287 288 289 290 291 292 293\n",
      " 294 296 297 298 299 302 304 305 306 307 308 311 312 313 314 316 317 318\n",
      " 319 321 322 325 326 327 328 330 331 332 335 336 337 339 340 341 342 343\n",
      " 344 345 346 347 348 349 351 352 353 354 357 358 360 362 363 364 365 367\n",
      " 368 369 371 372 374 375 377 378 379 380 381 382 383 385 386 387 388 389\n",
      " 390 391 392 393 396 397 399 400 402 404 405 406 407 408 409 411 412 413\n",
      " 414 415 416 417 418 419 421 422 423 426 427 428 429 430 432 433 434 435\n",
      " 437 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455\n",
      " 456 457 458 459 460 461 462 464 465 466 467 468 469 471 472 473 475 476\n",
      " 478 479 480 481 482 483 484 486 487 490 493 494 495 497 498 499 500 501\n",
      " 503 504 505 506 507 508 509 510 511 512 515 516 517 518 519 520 521 523\n",
      " 524 525 526 527 528 529 530 531 534 535 536 537 540 543 544 546 547 550\n",
      " 551 552 553 554 556 557 559 560 561 562 563 564 565 569 571 574 576 579\n",
      " 580 581 582 583 585 586 588 590 592 593 595 598 599 601 603 606 607 608\n",
      " 609 610 611 612 614 616 617 618 621 622 623 624 625 627 629 631 634 635\n",
      " 637 638 639 641 642 643 644 647 648 649 650 651 652 653 654 655 658 659\n",
      " 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 677 678\n",
      " 680 681 684 685 686 687 688 689 691 692 693 694 695 697 699 700 702 703\n",
      " 704 705 707 708 709 710 711 712 713 714 715 716 718 722 723 724 725 726\n",
      " 727 729 730 733 734 737 738 740 741 742 743 745 746 749 750 751 753 755\n",
      " 756 757 760 761 762 763 764 765 766 767 768 770 771 774 775 776 777 778\n",
      " 780 781 782 783 784 785 787 788 789 790 792 793 794 797 798 799 800 801\n",
      " 802 803 804 805 806 808 810 813 815 816 817 818 819 820 821 822 824 826\n",
      " 828 829 830 831 832 833 834 836 837 838 839 840 841 842 843 844 846 847\n",
      " 848 849 850 851 852 853 854 856 857 858 860 861 862 863 864 865 866 867\n",
      " 868 870 871 872 873 878 879 880 881 882 883 884 885 886 887 889 892 893\n",
      " 894 895 896 897 898 899 900 902 903 905 906 908 909 910 911 912 913 914\n",
      " 915 916 917 918 921 922 923 925 926 928 929 930 931 932 933 934 935 936\n",
      " 937 938 939 940 941 942 943 944 945 948 949 950 951 952 953 954 955 956\n",
      " 957 958 960 962 963 965 966 967 968 969 970 971 972 973 974 976 977 978\n",
      " 979 981 982 985 987 988 989 990 993 994 995 999]\n",
      "Test:  [  0   4   5   6  10  11  13  18  22  24  39  42  43  45  46  53  55  58\n",
      "  64  65  67  72  74  81  82  88  92  97  99 103 105 107 111 125 126 127\n",
      " 128 130 135 139 142 143 152 155 159 162 164 165 168 173 174 175 186 187\n",
      " 195 196 205 210 234 236 237 238 243 252 253 257 258 260 261 264 268 270\n",
      " 274 276 277 279 283 285 295 300 301 303 309 310 315 320 323 324 329 333\n",
      " 334 338 350 355 356 359 361 366 370 373 376 384 394 395 398 401 403 410\n",
      " 420 424 425 431 436 438 463 470 474 477 485 488 489 491 492 496 502 513\n",
      " 514 522 532 533 538 539 541 542 545 548 549 555 558 566 567 568 570 572\n",
      " 573 575 577 578 584 587 589 591 594 596 597 600 602 604 605 613 615 619\n",
      " 620 626 628 630 632 633 636 640 645 646 656 657 676 679 682 683 690 696\n",
      " 698 701 706 717 719 720 721 728 731 732 735 736 739 744 747 748 752 754\n",
      " 758 759 769 772 773 779 786 791 795 796 807 809 811 812 814 823 825 827\n",
      " 835 845 855 859 869 874 875 876 877 888 890 891 901 904 907 919 920 924\n",
      " 927 946 947 959 961 964 975 980 983 984 986 991 992 996 997 998]\n",
      "Train: [  0   3   4   5   6   7   8   9  10  11  12  13  14  15  17  18  19  20\n",
      "  22  24  25  27  28  30  31  34  35  36  37  39  40  41  42  43  44  45\n",
      "  46  47  48  49  50  51  52  53  55  56  57  58  59  60  61  62  63  64\n",
      "  65  67  68  70  71  72  74  75  76  77  78  79  80  81  82  83  84  85\n",
      "  86  88  89  92  93  94  95  97  99 100 102 103 104 105 106 107 108 109\n",
      " 111 113 116 117 118 119 120 121 123 124 125 126 127 128 129 130 131 132\n",
      " 133 134 135 136 137 139 140 141 142 143 145 146 147 148 149 150 151 152\n",
      " 153 155 156 158 159 160 162 163 164 165 167 168 169 173 174 175 177 179\n",
      " 180 181 182 183 184 185 186 187 188 189 190 191 193 195 196 197 199 202\n",
      " 203 204 205 206 207 208 209 210 212 213 214 217 219 220 221 225 227 228\n",
      " 229 230 231 232 233 234 235 236 237 238 240 241 242 243 245 246 250 252\n",
      " 253 256 257 258 259 260 261 262 263 264 267 268 270 271 272 273 274 275\n",
      " 276 277 278 279 280 281 282 283 284 285 288 289 291 292 294 295 296 299\n",
      " 300 301 303 306 307 308 309 310 311 314 315 317 318 319 320 321 322 323\n",
      " 324 325 328 329 330 331 332 333 334 335 337 338 339 340 342 343 344 345\n",
      " 346 347 348 349 350 352 353 354 355 356 358 359 361 363 365 366 368 369\n",
      " 370 372 373 375 376 378 379 380 381 382 383 384 385 386 387 388 392 393\n",
      " 394 395 396 398 399 400 401 402 403 405 406 408 409 410 411 413 414 416\n",
      " 417 418 419 420 421 423 424 425 428 429 430 431 432 434 436 438 439 440\n",
      " 442 444 445 447 448 450 453 456 460 461 462 463 464 465 467 469 470 471\n",
      " 472 474 476 477 480 481 482 483 484 485 486 487 488 489 490 491 492 494\n",
      " 495 496 498 499 500 502 503 504 510 513 514 516 518 519 520 521 522 523\n",
      " 525 527 528 529 531 532 533 534 536 537 538 539 540 541 542 543 544 545\n",
      " 546 547 548 549 550 551 552 554 555 556 558 561 562 564 565 566 567 568\n",
      " 569 570 571 572 573 575 577 578 579 582 583 584 585 587 588 589 590 591\n",
      " 592 594 595 596 597 598 600 601 602 603 604 605 606 608 609 610 611 613\n",
      " 615 616 617 618 619 620 621 623 625 626 628 630 631 632 633 634 635 636\n",
      " 638 639 640 641 642 643 644 645 646 647 649 650 651 653 654 655 656 657\n",
      " 658 659 661 662 663 664 665 668 670 674 675 676 679 680 682 683 687 689\n",
      " 690 691 692 693 694 696 697 698 700 701 702 703 705 706 707 708 710 711\n",
      " 714 716 717 718 719 720 721 722 723 724 726 728 729 730 731 732 733 734\n",
      " 735 736 737 738 739 740 743 744 745 746 747 748 749 751 752 754 755 757\n",
      " 758 759 760 762 764 765 766 768 769 770 772 773 774 775 776 779 780 782\n",
      " 783 784 786 787 788 790 791 793 794 795 796 799 800 802 803 805 807 808\n",
      " 809 810 811 812 814 816 819 821 823 824 825 826 827 828 829 830 831 833\n",
      " 834 835 836 838 839 840 841 842 843 844 845 846 847 848 849 851 852 853\n",
      " 854 855 858 859 860 862 863 866 869 870 872 873 874 875 876 877 878 880\n",
      " 882 883 884 885 886 887 888 890 891 892 893 894 895 897 898 899 900 901\n",
      " 902 904 905 906 907 908 910 912 913 915 917 919 920 922 924 927 928 929\n",
      " 931 934 935 936 939 940 943 944 946 947 949 950 954 955 956 958 959 960\n",
      " 961 962 963 964 966 967 968 969 971 973 974 975 976 978 979 980 983 984\n",
      " 985 986 987 988 990 991 992 993 996 997 998 999]\n",
      "Test:  [  1   2  16  21  23  26  29  32  33  38  54  66  69  73  87  90  91  96\n",
      "  98 101 110 112 114 115 122 138 144 154 157 161 166 170 171 172 176 178\n",
      " 192 194 198 200 201 211 215 216 218 222 223 224 226 239 244 247 248 249\n",
      " 251 254 255 265 266 269 286 287 290 293 297 298 302 304 305 312 313 316\n",
      " 326 327 336 341 351 357 360 362 364 367 371 374 377 389 390 391 397 404\n",
      " 407 412 415 422 426 427 433 435 437 441 443 446 449 451 452 454 455 457\n",
      " 458 459 466 468 473 475 478 479 493 497 501 505 506 507 508 509 511 512\n",
      " 515 517 524 526 530 535 553 557 559 560 563 574 576 580 581 586 593 599\n",
      " 607 612 614 622 624 627 629 637 648 652 660 666 667 669 671 672 673 677\n",
      " 678 681 684 685 686 688 695 699 704 709 712 713 715 725 727 741 742 750\n",
      " 753 756 761 763 767 771 777 778 781 785 789 792 797 798 801 804 806 813\n",
      " 815 817 818 820 822 832 837 850 856 857 861 864 865 867 868 871 879 881\n",
      " 889 896 903 909 911 914 916 918 921 923 925 926 930 932 933 937 938 941\n",
      " 942 945 948 951 952 953 957 965 970 972 977 981 982 989 994 995]\n",
      "Train: [  0   1   2   3   4   5   6   8   9  10  11  12  13  16  18  21  22  23\n",
      "  24  25  26  28  29  31  32  33  36  37  38  39  40  42  43  44  45  46\n",
      "  47  48  52  53  54  55  58  59  60  62  63  64  65  66  67  69  72  73\n",
      "  74  76  77  78  79  80  81  82  83  84  85  87  88  89  90  91  92  94\n",
      "  96  97  98  99 101 102 103 104 105 107 110 111 112 114 115 116 121 122\n",
      " 125 126 127 128 130 134 135 138 139 140 141 142 143 144 145 146 148 149\n",
      " 150 152 153 154 155 157 159 161 162 163 164 165 166 167 168 169 170 171\n",
      " 172 173 174 175 176 178 180 182 184 185 186 187 189 190 191 192 194 195\n",
      " 196 198 200 201 202 203 204 205 206 207 209 210 211 213 214 215 216 217\n",
      " 218 219 220 222 223 224 225 226 227 228 229 230 233 234 235 236 237 238\n",
      " 239 240 243 244 247 248 249 251 252 253 254 255 256 257 258 260 261 264\n",
      " 265 266 268 269 270 271 273 274 275 276 277 279 280 281 282 283 284 285\n",
      " 286 287 288 290 292 293 295 297 298 299 300 301 302 303 304 305 306 308\n",
      " 309 310 311 312 313 315 316 318 320 321 323 324 325 326 327 329 333 334\n",
      " 336 338 340 341 342 343 345 346 349 350 351 355 356 357 359 360 361 362\n",
      " 364 366 367 370 371 372 373 374 376 377 380 383 384 387 388 389 390 391\n",
      " 394 395 397 398 399 401 402 403 404 407 410 411 412 413 415 420 422 424\n",
      " 425 426 427 428 429 430 431 433 434 435 436 437 438 441 443 445 446 447\n",
      " 448 449 451 452 454 455 457 458 459 460 461 462 463 465 466 467 468 469\n",
      " 470 473 474 475 476 477 478 479 480 481 483 484 485 487 488 489 490 491\n",
      " 492 493 494 495 496 497 498 500 501 502 504 505 506 507 508 509 510 511\n",
      " 512 513 514 515 517 518 519 521 522 524 526 527 528 529 530 531 532 533\n",
      " 535 536 537 538 539 540 541 542 545 546 547 548 549 550 551 553 555 556\n",
      " 557 558 559 560 561 562 563 566 567 568 569 570 572 573 574 575 576 577\n",
      " 578 579 580 581 584 586 587 589 591 592 593 594 595 596 597 599 600 601\n",
      " 602 603 604 605 607 609 612 613 614 615 616 619 620 622 623 624 626 627\n",
      " 628 629 630 632 633 636 637 640 641 643 644 645 646 647 648 649 650 652\n",
      " 653 655 656 657 659 660 662 663 666 667 668 669 670 671 672 673 674 676\n",
      " 677 678 679 680 681 682 683 684 685 686 688 689 690 691 693 694 695 696\n",
      " 698 699 700 701 704 706 707 708 709 711 712 713 714 715 716 717 718 719\n",
      " 720 721 723 724 725 726 727 728 730 731 732 734 735 736 738 739 740 741\n",
      " 742 744 746 747 748 750 752 753 754 755 756 758 759 761 762 763 764 767\n",
      " 769 771 772 773 775 777 778 779 781 783 785 786 787 789 790 791 792 793\n",
      " 794 795 796 797 798 799 800 801 802 803 804 806 807 808 809 810 811 812\n",
      " 813 814 815 816 817 818 819 820 821 822 823 824 825 827 828 830 832 833\n",
      " 835 836 837 839 840 841 842 845 847 850 854 855 856 857 859 861 864 865\n",
      " 867 868 869 871 872 873 874 875 876 877 879 880 881 883 884 886 888 889\n",
      " 890 891 892 896 900 901 902 903 904 905 906 907 909 911 913 914 915 916\n",
      " 917 918 919 920 921 922 923 924 925 926 927 929 930 932 933 934 936 937\n",
      " 938 941 942 944 945 946 947 948 950 951 952 953 954 957 959 960 961 963\n",
      " 964 965 969 970 971 972 973 975 976 977 978 980 981 982 983 984 985 986\n",
      " 987 988 989 990 991 992 993 994 995 996 997 998]\n",
      "Test:  [  7  14  15  17  19  20  27  30  34  35  41  49  50  51  56  57  61  68\n",
      "  70  71  75  86  93  95 100 106 108 109 113 117 118 119 120 123 124 129\n",
      " 131 132 133 136 137 147 151 156 158 160 177 179 181 183 188 193 197 199\n",
      " 208 212 221 231 232 241 242 245 246 250 259 262 263 267 272 278 289 291\n",
      " 294 296 307 314 317 319 322 328 330 331 332 335 337 339 344 347 348 352\n",
      " 353 354 358 363 365 368 369 375 378 379 381 382 385 386 392 393 396 400\n",
      " 405 406 408 409 414 416 417 418 419 421 423 432 439 440 442 444 450 453\n",
      " 456 464 471 472 482 486 499 503 516 520 523 525 534 543 544 552 554 564\n",
      " 565 571 582 583 585 588 590 598 606 608 610 611 617 618 621 625 631 634\n",
      " 635 638 639 642 651 654 658 661 664 665 675 687 692 697 702 703 705 710\n",
      " 722 729 733 737 743 745 749 751 757 760 765 766 768 770 774 776 780 782\n",
      " 784 788 805 826 829 831 834 838 843 844 846 848 849 851 852 853 858 860\n",
      " 862 863 866 870 878 882 885 887 893 894 895 897 898 899 908 910 912 928\n",
      " 931 935 939 940 943 949 955 956 958 962 966 967 968 974 979 999]\n",
      "Train: [  0   1   2   4   5   6   7  10  11  13  14  15  16  17  18  19  20  21\n",
      "  22  23  24  26  27  29  30  32  33  34  35  38  39  41  42  43  45  46\n",
      "  49  50  51  53  54  55  56  57  58  61  64  65  66  67  68  69  70  71\n",
      "  72  73  74  75  81  82  86  87  88  90  91  92  93  95  96  97  98  99\n",
      " 100 101 103 105 106 107 108 109 110 111 112 113 114 115 117 118 119 120\n",
      " 122 123 124 125 126 127 128 129 130 131 132 133 135 136 137 138 139 142\n",
      " 143 144 147 151 152 154 155 156 157 158 159 160 161 162 164 165 166 168\n",
      " 170 171 172 173 174 175 176 177 178 179 181 183 186 187 188 192 193 194\n",
      " 195 196 197 198 199 200 201 205 208 210 211 212 215 216 218 221 222 223\n",
      " 224 226 231 232 234 236 237 238 239 241 242 243 244 245 246 247 248 249\n",
      " 250 251 252 253 254 255 257 258 259 260 261 262 263 264 265 266 267 268\n",
      " 269 270 272 274 276 277 278 279 283 285 286 287 289 290 291 293 294 295\n",
      " 296 297 298 300 301 302 303 304 305 307 309 310 312 313 314 315 316 317\n",
      " 319 320 322 323 324 326 327 328 329 330 331 332 333 334 335 336 337 338\n",
      " 339 341 344 347 348 350 351 352 353 354 355 356 357 358 359 360 361 362\n",
      " 363 364 365 366 367 368 369 370 371 373 374 375 376 377 378 379 381 382\n",
      " 384 385 386 389 390 391 392 393 394 395 396 397 398 400 401 403 404 405\n",
      " 406 407 408 409 410 412 414 415 416 417 418 419 420 421 422 423 424 425\n",
      " 426 427 431 432 433 435 436 437 438 439 440 441 442 443 444 446 449 450\n",
      " 451 452 453 454 455 456 457 458 459 463 464 466 468 470 471 472 473 474\n",
      " 475 477 478 479 482 485 486 488 489 491 492 493 496 497 499 501 502 503\n",
      " 505 506 507 508 509 511 512 513 514 515 516 517 520 522 523 524 525 526\n",
      " 530 532 533 534 535 538 539 541 542 543 544 545 548 549 552 553 554 555\n",
      " 557 558 559 560 563 564 565 566 567 568 570 571 572 573 574 575 576 577\n",
      " 578 580 581 582 583 584 585 586 587 588 589 590 591 593 594 596 597 598\n",
      " 599 600 602 604 605 606 607 608 610 611 612 613 614 615 617 618 619 620\n",
      " 621 622 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639\n",
      " 640 642 645 646 648 651 652 654 656 657 658 660 661 664 665 666 667 669\n",
      " 671 672 673 675 676 677 678 679 681 682 683 684 685 686 687 688 690 692\n",
      " 695 696 697 698 699 701 702 703 704 705 706 709 710 712 713 715 717 719\n",
      " 720 721 722 725 727 728 729 731 732 733 735 736 737 739 741 742 743 744\n",
      " 745 747 748 749 750 751 752 753 754 756 757 758 759 760 761 763 765 766\n",
      " 767 768 769 770 771 772 773 774 776 777 778 779 780 781 782 784 785 786\n",
      " 788 789 791 792 795 796 797 798 801 804 805 806 807 809 811 812 813 814\n",
      " 815 817 818 820 822 823 825 826 827 829 831 832 834 835 837 838 843 844\n",
      " 845 846 848 849 850 851 852 853 855 856 857 858 859 860 861 862 863 864\n",
      " 865 866 867 868 869 870 871 874 875 876 877 878 879 881 882 885 887 888\n",
      " 889 890 891 893 894 895 896 897 898 899 901 903 904 907 908 909 910 911\n",
      " 912 914 916 918 919 920 921 923 924 925 926 927 928 930 931 932 933 935\n",
      " 937 938 939 940 941 942 943 945 946 947 948 949 951 952 953 955 956 957\n",
      " 958 959 961 962 964 965 966 967 968 970 972 974 975 977 979 980 981 982\n",
      " 983 984 986 989 991 992 994 995 996 997 998 999]\n",
      "Test:  [  3   8   9  12  25  28  31  36  37  40  44  47  48  52  59  60  62  63\n",
      "  76  77  78  79  80  83  84  85  89  94 102 104 116 121 134 140 141 145\n",
      " 146 148 149 150 153 163 167 169 180 182 184 185 189 190 191 202 203 204\n",
      " 206 207 209 213 214 217 219 220 225 227 228 229 230 233 235 240 256 271\n",
      " 273 275 280 281 282 284 288 292 299 306 308 311 318 321 325 340 342 343\n",
      " 345 346 349 372 380 383 387 388 399 402 411 413 428 429 430 434 445 447\n",
      " 448 460 461 462 465 467 469 476 480 481 483 484 487 490 494 495 498 500\n",
      " 504 510 518 519 521 527 528 529 531 536 537 540 546 547 550 551 556 561\n",
      " 562 569 579 592 595 601 603 609 616 623 641 643 644 647 649 650 653 655\n",
      " 659 662 663 668 670 674 680 689 691 693 694 700 707 708 711 714 716 718\n",
      " 723 724 726 730 734 738 740 746 755 762 764 775 783 787 790 793 794 799\n",
      " 800 802 803 808 810 816 819 821 824 828 830 833 836 839 840 841 842 847\n",
      " 854 872 873 880 883 884 886 892 900 902 905 906 913 915 917 922 929 934\n",
      " 936 944 950 954 960 963 969 971 973 976 978 985 987 988 990 993]\n",
      "=========================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"==============SKLEARN VALIDACIÓN SIMPLE 70% LENSES DATA==================\")\n",
    "x_train, x_test, y_train, y_test = validacion_simple_sklearn(dataset, 0.7)\n",
    "print(\"TRAIN:\\n\", x_train)\n",
    "print(\"TEST:\\n\", x_test)\n",
    "print(\"==============SKLEARN VALIDACIÓN CRUZADA K=5 LENSES DATA=================\")\n",
    "particiones = validacion_cruzada_sklearn(dataset,5)\n",
    "for particion in particiones:\n",
    "    print(particion)\n",
    "print(\"=========================================================================\")\n",
    "print(\"==============SKLEARN VALIDACIÓN SIMPLE 80% TIC-TAC-TOE DATA=============\")\n",
    "x_train1, x_test1, y_train1, y_test1 = validacion_simple_sklearn(dataset2, 0.8)\n",
    "print(\"TRAIN:\\n\", x_train1)\n",
    "print(\"TEST:\\n\", x_test1)\n",
    "print(\"==============SKLEARN VALIDACIÓN CRUZADA K=8 TIC-TAC-TOE DATA============\")\n",
    "particiones = validacion_cruzada_sklearn(dataset2,8)\n",
    "for particion in particiones:\n",
    "    print(particion)\n",
    "print(\"=========================================================================\")\n",
    "print(\"==============SKLEARN VALIDACIÓN SIMPLE 75% GERMAN DATA==================\")\n",
    "x_train2, x_test2, y_train2, y_test2 = validacion_simple_sklearn(dataset3, 0.75)\n",
    "print(\"TRAIN:\\n\", x_train2)\n",
    "print(\"TEST:\\n\", x_test2)\n",
    "print(\"==============SKLEARN VALIDACIÓN CRUZADA K=4 GERMAN DATA=================\")\n",
    "particiones = validacion_cruzada_sklearn(dataset3,4)\n",
    "for particion in particiones:\n",
    "    print(particion)\n",
    "print(\"=========================================================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Clasificador Naive-Bayes con Sklearn</h3>\n",
    "<p>Ahora vamos a realizar pruebas con la implementación que nos da sklearn del clasificador de Naive-Bayes, anteriormente hemos realizado las diferentes estrategias de particionado, por lo tanto, ahora solo nos falta introducir esos subconjuntos a los métodos que hemos creado para tener la implementacion de sklearn y que la libreria se encargue de hacer el entrenamiento y la clasificación del conjunto de datos. A continuación, explicaremos brevemente cada uno de los métodos que hemos creado.</p>\n",
    "<p>El método <strong>nb_sklearn</strong>: en este método introducimos como parametro el subconjunto de datos de entrenamiento y de clasificacion, el tipo que queremos calcular y si se va a utilizar la regla de Laplace, donde el tipo va estar predefinido a Multinominal y laplace va estar definido a True. Este método va a devolver la predicción de las clases.</p>\n",
    "<p>El método <strong>error</strong>: este método va a calcular los errores que vamos a obtener el pporcentaje de error que hemos obtenido con el clasificador Naive-Bayes.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nb_sklearn(x_train, y_train, x_test, tipo=\"Multinomial\", laplace=True):\n",
    "\n",
    "    if tipo == \"Gaussian\":\n",
    "        if laplace == True:\n",
    "            clf = GaussianNB(alpha=1.0)\n",
    "        else:\n",
    "            clf = GaussianNB()\n",
    "\n",
    "    elif tipo == \"Multinomial\":\n",
    "        if laplace == True:\n",
    "            clf = MultinomialNB(alpha=1.0, fit_prior = True, class_prior = None)\n",
    "        else:\n",
    "            clf = MultinomialNB(fit_prior=True, class_prior=False)\n",
    "    else:\n",
    "        print(\"Error, clasificador no valido. Utilizar GaussianNB o MultinomialNB\")\n",
    "        return\n",
    "\n",
    "    # Entrenamos el modelo\n",
    "    clf.fit(x_train, y_train)\n",
    "\n",
    "    # Clasificacion\n",
    "    prediccion = clf.predict(x_test)\n",
    "\n",
    "    return prediccion\n",
    "\n",
    "def nb_sklearn_validacion_cruzada(x_train, y_train, k):\n",
    "\n",
    "    clf = MultinomialNB(alpha = 1.0, fit_prior = True, class_prior = None)\n",
    "\n",
    "    error = cross_val_score(clf, x_train, y_train, cv = k)\n",
    "\n",
    "    return error\n",
    "\n",
    "\n",
    "\n",
    "def error(clases_predichas, clases_reales):\n",
    "\n",
    "    return 1 - (np.sum(np.equal(clases_predichas, clases_reales)) / len(clases_predichas))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============SKLEARN NAIVE-BAYES LENSES DATA==================\n",
      "==============CON LAPLACE Y VALIDACION SIMPLE==================\n",
      "ERROR OBTENIDO: 1.0\n",
      "==============SIN LAPLACE Y VALIDACION SIMPLE==================\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "object of type 'bool' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-4fcd5cd9dddf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"==============SIN LAPLACE Y VALIDACION SIMPLE==================\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidacion_simple_sklearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.7\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnb_sklearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Multinomial\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0merrornb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ERROR OBTENIDO:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrornb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-38-1f4318f6bd35>\u001b[0m in \u001b[0;36mnb_sklearn\u001b[0;34m(x_train, y_train, x_test, tipo, laplace)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;31m# Entrenamos el modelo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;31m# Clasificacion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/naive_bayes.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    614\u001b[0m         \u001b[0malpha\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_alpha\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    615\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_feature_log_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 616\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_class_log_prior\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclass_prior\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_prior\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    617\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/naive_bayes.py\u001b[0m in \u001b[0;36m_update_class_log_prior\u001b[0;34m(self, class_prior)\u001b[0m\n\u001b[1;32m    456\u001b[0m         \u001b[0mn_classes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    457\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mclass_prior\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 458\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclass_prior\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mn_classes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    459\u001b[0m                 raise ValueError(\"Number of priors must match number of\"\n\u001b[1;32m    460\u001b[0m                                  \" classes.\")\n",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'bool' has no len()"
     ]
    }
   ],
   "source": [
    "print(\"==============SKLEARN NAIVE-BAYES LENSES DATA==================\")\n",
    "print(\"==============CON LAPLACE Y VALIDACION SIMPLE==================\")\n",
    "x_train, x_test, y_train, y_test = validacion_simple_sklearn(dataset, 0.7)\n",
    "pred = nb_sklearn(x_train, y_train, x_test)\n",
    "errornb = error(pred, x_test[:,-1])\n",
    "print(\"ERROR OBTENIDO:\", errornb)\n",
    "print(\"==============SIN LAPLACE Y VALIDACION SIMPLE==================\")\n",
    "x_train, x_test, y_train, y_test = validacion_simple_sklearn(dataset, 0.7)\n",
    "pred = nb_sklearn(x_train, y_train, x_test, \"Multinomial\",False)\n",
    "errornb = error(pred, x_test[:,-1])\n",
    "print(\"ERROR OBTENIDO:\", errornb)\n",
    "print(\"==============SKLEARN NAIVE-BAYES TIC-TAC-TOE DATA=============\")\n",
    "print(\"==============CON LAPLACE Y VALIDACION SIMPLE==================\")\n",
    "x_train2, x_test2, y_train2, y_test2 = validacion_simple_sklearn(dataset2, 0.8)\n",
    "pred = nb_sklearn(x_train2, y_train2, x_test2)\n",
    "errornb2 = error(pred, x_test2[:,-1])\n",
    "print(\"ERROR OBTENIDO:\", errornb)\n",
    "print(\"==============SIN LAPLACE Y VALIDACION SIMPLE==================\")\n",
    "pred = nb_sklearn(x_train2, y_train2, x_test2, \"Multinomial\",False)\n",
    "errornb2 = error(pred, x_test2[:,-1])\n",
    "print(\"ERROR OBTENIDO:\", errornb)\n",
    "print(\"==============SKLEARN NAIVE-BAYES GERMAN DATA=============\")\n",
    "print(\"==============CON LAPLACE Y VALIDACION SIMPLE==================\")\n",
    "x_train3, x_test3, y_train3, y_test3 = validacion_simple_sklearn(dataset3, 0.75)\n",
    "pred = nb_sklearn(x_train3, y_train3, x_test3)\n",
    "errornb2 = error(pred, x_test3[:,-1])\n",
    "print(\"ERROR OBTENIDO:\", errornb)\n",
    "print(\"==============SIN LAPLACE Y VALIDACION SIMPLE==================\")\n",
    "pred = nb_sklearn(x_train3, y_train3, x_test3, \"Multinomial\",False)\n",
    "errornb2 = error(pred, x_test3[:,-1])\n",
    "print(\"ERROR OBTENIDO:\", errornb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Apartado 4:Evaluación de hipótesis mediante Análisis ROC</h3>\n",
    "<p>La curva ROC es una representación gráfica de la sensibilidad a la especifidad de un clasificador, en esta práctica este análisis lo vamos a realizar del clasificador implementado que es Naive-Bayes. En este gráfico se representan los verdaderos postivos frente a los falsos positivos.</p>\n",
    "<p>Es una herramienta que nos proporciona la selección de modelos más óptimos y descartar los menos óptimos.</p>\n",
    "<p>A continuación, mostraremos la implementación que hemos realizado en para crear este análisis ROC:</p>\n",
    "<ol>\n",
    "    <li>El primer paso es crear la <strong> matriz de confusión</strong>, donde esta matriz la hemos utilizado con un método de la libreria de sklearn que nos dibuja la matriz de confusion para los datos que queremos del conjunto de datos. Despues de haber creado la matriz calculamos los valores de verdaderos positivos, falsos positivos, falsos negativos y verdaderos negativos y, por último, calculamos las tasas de la matriz de confusion y las guardamos en una lista. </li>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Clasificador:\n",
    "    def matrizConfusion(self, dataset, datosTest, prediccion):\n",
    "\n",
    "        # Calculamos la matriz de confusion utlizando sk-learn. Solo se calcula en el caso de que la clasificacion sea binaria.\n",
    "        testData = dataset.extraeDatos(datosTest)\n",
    "        clase_real = testData[:, -1]\n",
    "\n",
    "        matriz = confusion_matrix(prediccion, clase_real)\n",
    "        # La funcion ravel() devuelve todas las estadisticas relacionadas con la matriz de confusion\n",
    "        tn, fp, fn, tp = matriz.ravel()\n",
    "\n",
    "\n",
    "        # Calculamos las tasas extraídas de la matriz de confusión\n",
    "        tpr = tp / (tp + fn)\n",
    "        fpr = fp / (fp + fn)\n",
    "\n",
    "        self.lista_tpr.append(tpr)\n",
    "        self.lista_fpr.append(fpr)\n",
    "\n",
    "\n",
    "        return matriz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Vamos a mostrar a continuación una ejecución del anterior codigo para verlo con diferentes datasets y diferentes predicciones.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "matrizConfusion() missing 1 required positional argument: 'prediccion'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-51-fcebc4065ffa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mnb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mentrenamiento\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparticion\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindicesTrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasifica\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mparticion\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindicesTest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mmatriz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmatrizConfusion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparticion\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindicesTest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatriz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: matrizConfusion() missing 1 required positional argument: 'prediccion'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ol>\n",
    "2. En segundo lugar, debemos sacar la gráfica de la curva ROC. Esta gráfica lo sacamos con la libreria pyplot, mas concretamente, con matplotlib. Donde en el eje Y pondremos los valores TPR y en el eje de las X pondremos los valores de FPR.\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def curvaROC(self):\n",
    "\n",
    "    x = np.linspace(0, 1, 100)\n",
    "    plt.plot(x, x, c='blue')\n",
    "    for i in range(len(self.lista_fpr)):\n",
    "        plt.plot(self.lista_fpr[i],self.lista_tpr[i],'ro')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> A contnuación, mostraremos todas las curvas ROC para todos los conjuntos de datos que tenemos y asi poder ver si nuestro clasificador es óptimo para este tipo de datos.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
