{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/python3\n",
    "# -*- coding: utf-8 -*-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importamos Librerias\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from sortedcontainers import SortedDict\n",
    "import sklearn.naive_bayes as nb\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold, cross_val_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Datos:\n",
    "\n",
    "  TiposDeAtributos=('Continuo','Nominal')\n",
    "\n",
    "  # TODO: procesar el fichero para asignar correctamente las variables tipoAtributos, nombreAtributos, nominalAtributos, datos y diccionarios\n",
    "  # NOTA: No confundir TiposDeAtributos con tipoAtributos\n",
    "  def __init__(self, nombreFichero):\n",
    "\n",
    "      with open(nombreFichero, \"r\") as f:\n",
    "        # Guardamos el numero de datos que contiene el DataSet y esta en la primera linea\n",
    "        self.numDatos = int(f.readline())\n",
    "\n",
    "        # Guardamos el nombre de los atributos\n",
    "        self.nombreAtributos = f.readline().strip('\\n').split(',')\n",
    "        #print(self.nombreAtributos)\n",
    "\n",
    "        # Leemos el tipo de los atributos de las variables y eliminamos el ultimo que es un salto de linea\n",
    "        self.tipoAtributos = f.readline().strip('\\n').split(',')\n",
    "        #print(self.tipoAtributos)\n",
    "\n",
    "        # Comprobamos que todos los atributos sean Continuos o Nominales\n",
    "        if any(atr not in Datos.TiposDeAtributos for atr in self.tipoAtributos):\n",
    "            raise ValueError(\"Tipo de atributo erroneo\")\n",
    "\n",
    "        # Segun el atributo, asignamos True o False.\n",
    "        self.nominalAtributos = []\n",
    "\n",
    "        # Guardamos en la lista nominalAtributos en la posicion de cada uno si es o no Nominal\n",
    "        for tipo in self.tipoAtributos:\n",
    "            if tipo == self.TiposDeAtributos[0]:\n",
    "                self.nominalAtributos.append(False)\n",
    "            else:\n",
    "                self.nominalAtributos.append(True)\n",
    "        #print(self.nominalAtributos)\n",
    "\n",
    "        # Guardamos los datos del fichero y los formateamos, de tal forma que cada linea es una lista\n",
    "        datos = f.readlines()\n",
    "        datosFormat = []\n",
    "        for lista in datos:\n",
    "            datosFormat.append(lista.strip('\\n').split(','))\n",
    "\n",
    "        # print(set(sorted(datosFormat[0])))\n",
    "        listaDatosAtributos = []\n",
    "        for i in range(len(self.tipoAtributos)):\n",
    "            listaDatosAtributos.append([])\n",
    "\n",
    "        # Hacemos la traspuesta de los datos que guardamos para que cada lista de atributo guarde todos los datos\n",
    "        # de cada atributo.\n",
    "        for lista in datosFormat:\n",
    "            i = 0\n",
    "            for item in lista:\n",
    "                listaDatosAtributos[i].append(item)\n",
    "                i += 1\n",
    "\n",
    "        # Ordenamos y hacemos un set para eliminar repetidos.\n",
    "        i = 0\n",
    "        for item in listaDatosAtributos:\n",
    "            listaDatosAtributos[i] = sorted(set(item))\n",
    "            i += 1\n",
    "\n",
    "\n",
    "        # Creacion de lista diccionarios, en caso de que el atributo sea Continuo, el diccionario estara vacio\n",
    "        self.listaDicts = []\n",
    "        for i in range(len(self.tipoAtributos)):\n",
    "            self.listaDicts.append({})\n",
    "\n",
    "        # Creamos el diccionario tal y como se describe en las diapositivas, por orden y asignando valores numericos crecientes\n",
    "        i = 0\n",
    "        for atributo in listaDatosAtributos:\n",
    "            k = 0\n",
    "            if self.tipoAtributos[i] == \"Nominal\":\n",
    "                for dato in atributo:\n",
    "                    self.listaDicts[i][dato] = k\n",
    "                    k += 1\n",
    "            i += 1\n",
    "\n",
    "        # Creacion de la matriz de datos utilizando el diccionario para mapear los valores\n",
    "        # En primer lugar, creamos una matriz vacia de tamaña numero de atributos.\n",
    "        self.datos = np.empty((int(self.numDatos),int(len(self.tipoAtributos))))\n",
    "        i = 0\n",
    "        j = 0\n",
    "\n",
    "        # Metemos los datos en la matriz, mapeando con los diccionarios en el caso de que sean Nominales, y si son continuos normal.\n",
    "        for i in range(int(self.numDatos)):\n",
    "            for j in range(len(self.tipoAtributos)):\n",
    "                if self.tipoAtributos[j] == 'Nominal':\n",
    "                    self.datos[i][j] = self.listaDicts[j].get(str(datosFormat[i][j]))\n",
    "                else:\n",
    "                    self.datos[i][j] = datosFormat[i][j]\n",
    "        \n",
    "        print(self.nombreAtributos)\n",
    "        print(self.listaDicts)\n",
    "        f.close()\n",
    "\n",
    "\n",
    "  # TODO: implementar en la practica 1\n",
    "  def extraeDatos(self, idx):\n",
    "    return self.datos[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Age', 'Spectacle', 'Astigmatic', 'Tear', 'Class']\n",
      "[{'1': 0, '2': 1, '3': 2}, {'1': 0, '2': 1}, {'1': 0, '2': 1}, {'1': 0, '2': 1}, {'1': 0, '2': 1, '3': 2}]\n",
      "[[0. 0. 0. 0. 2.]\n",
      " [0. 0. 0. 1. 1.]\n",
      " [0. 0. 1. 0. 2.]\n",
      " [0. 0. 1. 1. 0.]\n",
      " [0. 1. 0. 0. 2.]\n",
      " [0. 1. 0. 1. 1.]\n",
      " [0. 1. 1. 0. 2.]\n",
      " [0. 1. 1. 1. 0.]\n",
      " [1. 0. 0. 0. 2.]\n",
      " [1. 0. 0. 1. 1.]\n",
      " [1. 0. 1. 0. 2.]\n",
      " [1. 0. 1. 1. 0.]\n",
      " [1. 1. 0. 0. 2.]\n",
      " [1. 1. 0. 1. 1.]\n",
      " [1. 1. 1. 0. 2.]\n",
      " [1. 1. 1. 1. 2.]\n",
      " [2. 0. 0. 0. 2.]\n",
      " [2. 0. 0. 1. 2.]\n",
      " [2. 0. 1. 0. 2.]\n",
      " [2. 0. 1. 1. 0.]\n",
      " [2. 1. 0. 0. 2.]\n",
      " [2. 1. 0. 1. 1.]\n",
      " [2. 1. 1. 0. 2.]\n",
      " [2. 1. 1. 1. 2.]]\n"
     ]
    }
   ],
   "source": [
    "dataset = Datos('lenses.data')\n",
    "\n",
    "print(dataset.datos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABCMeta,abstractmethod\n",
    "\n",
    "\n",
    "class Particion():\n",
    "\n",
    "  # Esta clase mantiene la lista de �ndices de Train y Test para cada partici�n del conjunto de particiones\n",
    "    def __init__(self,train=[],test=[]):\n",
    "        self.indicesTrain=train\n",
    "        self.indicesTest=test\n",
    "    \n",
    "    def __str__(self):\n",
    "        return \"Train: {}\\nTest:  {}\".format(str(self.indicesTrain),str(self.indicesTest)) \n",
    "\n",
    "#####################################################################################################\n",
    "\n",
    "class EstrategiaParticionado:\n",
    "\n",
    "    # Clase abstracta\n",
    "    __metaclass__ = ABCMeta\n",
    "\n",
    "    # Lista de las particiones\n",
    "    def __init__(self, nombre=\"\"):\n",
    "        self.nombreEstrategia = nombre\n",
    "        self.numeroParticiones = 0\n",
    "        self.particiones=[]\n",
    "\n",
    "    # Atributos: deben rellenarse adecuadamente para cada estrategia concreta: nombreEstrategia, numeroParticiones, listaParticiones. Se pasan en el constructor\n",
    "\n",
    "    @abstractmethod\n",
    "    # TODO: esta funcion deben ser implementadas en cada estrategia concreta\n",
    "    def creaParticiones(self,datos,seed=None):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValidacionSimple(EstrategiaParticionado):\n",
    "    \n",
    "    def __init__(self, porcentaje):\n",
    "        self.porcentaje = porcentaje\n",
    "        super().__init__(\"Validacion simple\")\n",
    "        \n",
    "  # Crea particiones segun el metodo tradicional de division de los datos segun el porcentaje deseado.\n",
    "  # Devuelve una lista de particiones (clase Particion)\n",
    "  # TODO: implementar\n",
    "    def creaParticiones(self,datos,seed=None):\n",
    "        np.random.seed(seed)\n",
    "\n",
    "        self.numeroParticiones = 1\n",
    "    \n",
    "        # Generamos una lista con todos los números de datos aleatorios   \n",
    "        indicesAleatorios = np.random.permutation(int(datos.numDatos))\n",
    "        \n",
    "        # Creamos la particion, en funcion del porcentaje especificado\n",
    "        self.particiones = [Particion(indicesAleatorios[:int(datos.numDatos*self.porcentaje)],\n",
    "                                      indicesAleatorios[int(datos.numDatos*self.porcentaje):])]\n",
    "        \n",
    "        return self.particiones\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [11  2 23  4 20  9 16 18  3  8  7 15 12 10 13  5  0 19]\n",
      "Test:  [21 17 14  1 22  6]\n"
     ]
    }
   ],
   "source": [
    "validacion_simple = ValidacionSimple(0.75)\n",
    "validacion_simple.creaParticiones(dataset)\n",
    "print(validacion_simple.particiones[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValidacionCruzada(EstrategiaParticionado):\n",
    "\n",
    "  # Crea particiones segun el metodo de validacion cruzada.\n",
    "  # El conjunto de entrenamiento se crea con las nfolds-1 particiones y el de test con la particion restante\n",
    "  # Esta funcion devuelve una lista de particiones (clase Particion)\n",
    "  # TODO: implementar\n",
    "    \n",
    "    def __init__(self, k):\n",
    "        self.k = k\n",
    "        super().__init__(\"Validacion cruzada\")\n",
    "\n",
    "    def creaParticiones(self,datos,seed=None):\n",
    "        np.random.seed(seed)\n",
    "        \n",
    "        self.numeroParticiones = self.k\n",
    "        \n",
    "        # Generamos una lista con todos los números de datos aleatorios   \n",
    "        indicesAleatorios = np.random.permutation(int(datos.numDatos))\n",
    "        \n",
    "        # Hallamos el tamaño de cada bloque\n",
    "        tamBloque = int(datos.numDatos/self.k)\n",
    "        \n",
    "        print(indicesAleatorios)\n",
    "        \n",
    "        datosSobran = datos.numDatos - (tamBloque*self.k)\n",
    "        count = 0\n",
    "        for i in range(self.k):\n",
    "            \n",
    "            train = np.delete(indicesAleatorios, range(i*tamBloque,(i+1)*tamBloque))\n",
    "            test =  indicesAleatorios[i*tamBloque:(i+1)*tamBloque]\n",
    "            \n",
    "            # Caso en el que la cuenta es justa\n",
    "            if datosSobran == 0:\n",
    "                self.particiones.append(Particion(train, test))\n",
    "                \n",
    "            # Contemplamos el caso de que la division para sacar el numero de subconjuntos no fuese entera\n",
    "            if datosSobran > 0:\n",
    "                count += 1\n",
    "                particionTest = np.append(test, train[(datos.numDatos - tamBloque)- i - 1])\n",
    "                particionTrain = np.delete(train, (datos.numDatos - tamBloque)- i - 1)\n",
    "                datosSobran -= 1\n",
    "                self.particiones.append(Particion(particionTrain, particionTest))\n",
    "                \n",
    "            \n",
    "           \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 3  6 14 10 11 22  0  4 17  9 20 13 15 19  5  7  8 23 16 12 18  1  2 21]\n",
      "Train: [11 22  0  4 17  9 20 13 15 19  5  7  8 23 16 12 18  1  2]\n",
      "Test:  [ 3  6 14 10 21]\n",
      "Train: [ 3  6 14 10 17  9 20 13 15 19  5  7  8 23 16 12 18  1 21]\n",
      "Test:  [11 22  0  4  2]\n",
      "Train: [ 3  6 14 10 11 22  0  4 15 19  5  7  8 23 16 12 18  2 21]\n",
      "Test:  [17  9 20 13  1]\n",
      "Train: [ 3  6 14 10 11 22  0  4 17  9 20 13  8 23 16 12  1  2 21]\n",
      "Test:  [15 19  5  7 18]\n",
      "Train: [ 3  6 14 10 11 22  0  4 17  9 20 13 15 19  5  7 18  1  2 21]\n",
      "Test:  [ 8 23 16 12]\n"
     ]
    }
   ],
   "source": [
    "v_cruzada = ValidacionCruzada(5)\n",
    "v_cruzada.creaParticiones(dataset)\n",
    "\n",
    "for particion in v_cruzada.particiones:\n",
    "    print(particion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Clasificador:\n",
    "  \n",
    "    # Clase abstracta\n",
    "    __metaclass__ = ABCMeta\n",
    "\n",
    "    # Metodos abstractos que se implementan en casa clasificador concreto\n",
    "    @abstractmethod\n",
    "    # TODO: esta funcion debe ser implementada en cada clasificador concreto\n",
    "    # datosTrain: matriz numpy con los datos de entrenamiento\n",
    "    # atributosDiscretos: array bool con la indicatriz de los atributos nominales\n",
    "    # diccionario: array de diccionarios de la estructura Datos utilizados para la codificacion de variables discretas\n",
    "    def entrenamiento(self,datos,datosTrain,atributosDiscretos,diccionario):\n",
    "        pass\n",
    "\n",
    "\n",
    "    @abstractmethod\n",
    "    # TODO: esta funcion debe ser implementada en cada clasificador concreto\n",
    "    # devuelve un numpy array con las predicciones\n",
    "    def clasifica(self,datosTest,atributosDiscretos,diccionario):\n",
    "        pass\n",
    "\n",
    "\n",
    "    # Obtiene el numero de aciertos y errores para calcular la tasa de fallo\n",
    "    # TODO: implementar\n",
    "    def error(self,datos,pred):\n",
    "    # Aqui se compara la prediccion (pred) con las clases reales y se calcula el error    \n",
    "        i = 0\n",
    "        real = datos[:,-1]\n",
    "        error = 0\n",
    "        for i in range(len(real)):\n",
    "            if real[i] != pred[i]:\n",
    "                error += 1\n",
    "        err = (error)/(len(real)+0.0)\n",
    "        print(datos[:,-1])\n",
    "        print(pred)\n",
    "        return err\n",
    "\n",
    "\n",
    "    # Realiza una clasificacion utilizando una estrategia de particionado determinada\n",
    "    # TODO: implementar esta funcion\n",
    "    def validacion(self,particionado,dataset,clasificador,seed=None):\n",
    "\n",
    "    # Creamos las particiones siguiendo la estrategia llamando a particionado.creaParticiones\n",
    "    # - Para validacion cruzada: en el bucle hasta nv entrenamos el clasificador con la particion de train i\n",
    "    # y obtenemos el error en la particion de test i\n",
    "    # - Para validacion simple (hold-out): entrenamos el clasificador con la particion de train\n",
    "    # y obtenemos el error en la particion test. Otra opci�n es repetir la validaci�n simple un n�mero especificado de veces, obteniendo en cada una un error. Finalmente se calcular�a la media.\n",
    "       \n",
    "        particionado.creaParticiones(dataset, seed)\n",
    "    # Comprobamos si es por validación cruzada o simple, por la longitud de la lista de particiones\n",
    "    # Validación Simple\n",
    "        if len(particionado.particiones) == 1:\n",
    "            clasificador.entrenamiento(dataset, particionado.particiones[0].indicesTrain)\n",
    "            pred = clasificador.clasifica(dataset,particionado.particiones[0].indicesTest)\n",
    "            ret = self.error(dataset.extraeDatos(particionado.particiones[0].indicesTest), pred)\n",
    "            if ret > 0:\n",
    "                return ret\n",
    "            else:\n",
    "                return 0\n",
    "            \n",
    "    # Validación Cruzada        \n",
    "        else:\n",
    "            for particion in particionado.particiones:\n",
    "                clasificador.entrenamiento(dataset, particionado.particiones.indicesTrain)\n",
    "                pred = clasificador.clasifica(dataset,particionado.particiones.indicesTest)\n",
    "                ret = self.error(dataset.extraeDatos(particionado.particiones.indicesTest), pred)\n",
    "                errores = np.append(errores,ret)\n",
    "                \n",
    "                #Devolucion de la media de los errores\n",
    "            return errores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClasificadorNaiveBayes(Clasificador):\n",
    "    \n",
    "    def entrenamiento(self,dataset,datosTrain):\n",
    "     \n",
    "        # Cargamos todos los datos de la clase del dataset desde la matriz de datos\n",
    "        clasesTrain = dataset.extraeDatos(datosTrain)\n",
    "        self.numClases = clasesTrain[:,-1]\n",
    "        # Contamos las apariciones de cada uno para luego calcular la probabilidad a priori de cada clase\n",
    "        counter = Counter(self.numClases)\n",
    "        \n",
    "        # Calculamos la probabilidad de la clase y lo metemos en un diccionario ordenado segun el numero \n",
    "        # correspondiente a cada clase asignado en el diccionario\n",
    "        self.dictPrioris={}\n",
    "        for k in counter:\n",
    "            k = int(k)\n",
    "            counter[k] = counter[k]/len(self.numClases)\n",
    "            self.dictPrioris[k] = counter[k]\n",
    "            \n",
    "        # Aqui ordenamos el diccionario para que esten en el mismo orden de como extraemos los datos del dataset\n",
    "        self.dictPrioris = SortedDict(self.dictPrioris)\n",
    "        \n",
    "        # Calcular tablas de probabilidades del entrenamiento. Tenemos que calcular por cada atributo una cuenta\n",
    "        # de las apariciones en cada clase\n",
    "        # Creamos una lista de matrices, donde vamos almacenar todos los datos que hemos obtenido en los datos de Test\n",
    "        self.posteriori = np.zeros(len(dataset.nombreAtributos)-1,dtype=object)\n",
    "        \n",
    "        # Recorremos todos los datos de la matriz sin llegar a la clase\n",
    "        for i in range(len(dataset.nombreAtributos) - 1):\n",
    "            \n",
    "            # Si el dato que obtenemos es Nominal haremos el recuento de todas las veces que sale la P(D|H)\n",
    "            if dataset.nominalAtributos[i] == True:\n",
    "                \n",
    "                # Creamos una matriz de tamaño X: Número de Atributos menos la clase Y: Número de clases\n",
    "                post = np.zeros((len(dataset.listaDicts[i]),len(dataset.listaDicts[-1])))\n",
    "               \n",
    "                # Aqui contamos todos las datos que queremos del datos Train para construir la matriz de entrenamiento\n",
    "                for c in range(len(dataset.listaDicts[-1])):\n",
    "                    datosEnt = dataset.extraeDatos(datosTrain)\n",
    "                    dat = datosEnt[:,i]\n",
    "                    repes = Counter(dat[datosEnt[:,-1] == c])\n",
    "                    for r in repes:\n",
    "                        post[int(r),c] = repes[r]\n",
    "                self.posteriori[i] = post +1\n",
    "            \n",
    "            # Si el dato es Continuo obtendremos la media y la desviación tipica de la clase\n",
    "            else:\n",
    "                \n",
    "                # Creamos una matriz de X: Los datos de Media y Desivación típica Y: Número de clases\n",
    "                post = np.zeros((2,len(dataset.listaDicts[-1])))\n",
    "               \n",
    "                # Aqui obtenemos la media y desviación tipica de cada clase, despues de tener los datos de entrenamiento\n",
    "                for c in range(len(dataset.listaDicts[-1])):\n",
    "                    datosEnt = dataset.extraeDatos(datosTrain)\n",
    "                    dat = datosEnt[:,i]\n",
    "                    datos = dat[datosEnt[:,-1] == c]\n",
    "                    post[0][c] = np.mean(datos)\n",
    "                    post[1][c] = np.std(datos)\n",
    "                self.posteriori[i] = post \n",
    "        \n",
    "    def clasifica(self,dataset,datosTest):\n",
    "        j = 0\n",
    "        aux = 1\n",
    "        self.prediccion = []\n",
    "        datTest = dataset.extraeDatos(datosTest)\n",
    "        #Calculamos los valores de los posteriori de todos las tablas anteriores\n",
    "        for i in range(len(dataset.listaDicts) - 1):\n",
    "            self.posteriori[i] /= sum(self.posteriori[i])\n",
    "        \n",
    "        #Ahora vamos a estudiar la probabilidad de la clase con los datos obtenidos en el entrenamiento\n",
    "        #Recorremos todos las datos de la matriz de los datos Test\n",
    "        for dato in datTest:\n",
    "            mapa = []\n",
    "            if dataset.nominalAtributos[j] == True:\n",
    "            \n",
    "            #Aqui obtenemos los prioris de cada clase para poder obtener la probabilidad de cada una\n",
    "                for clase in range(len(self.dictPrioris)):\n",
    "               \n",
    "                #Aqui obtenemos cada valor posteriori de nuestro entrenamiento de los datos, es decir, P(D|H)\n",
    "                    for atributo in range(len(self.posteriori)):\n",
    "                        aux *= self.posteriori[atributo][int(dato[atributo])][clase]\n",
    "                    \n",
    "                    #Aqui obtenemos la siguiente probabilidad P(D|H)*P(H)\n",
    "                    aux2 = self.dictPrioris.get(clase)*aux\n",
    "                    aux = 1\n",
    "                \n",
    "                #Lo añadimos a una lista para obtener la probabilidad de las diferentes clases\n",
    "                    mapa.append(aux2)\n",
    "            \n",
    "            #Aqui obtenemos la probabilidad de los atibutos continuo\n",
    "            else:\n",
    "                for clase in range(len(self.dictPrioris)):\n",
    "                    for atributo in range(len(self.posteriori)):\n",
    "                        \n",
    "                        # Hacemos la formula de la distribucion normal\n",
    "                        exp1 = 1/(self.posteriori[atributo][0][clase]*math.sqrt(2*math.pi))\n",
    "                        exp2 = ((dato-self.posteriori[atributo][0][clase]) - dato[atributo]/self.posteriori[atributo][1][clase])\n",
    "                        exp3 = exp2 ** 2\n",
    "                        exp4 = math.exp((-1/2)* exp3)\n",
    "                        total = exp1 * exp4\n",
    "                    aux2 = total * self.dictPrioris.get(clase)\n",
    "                    mapa.append(aux2)\n",
    "            if j == len(dataset.nominalAtributos):\n",
    "                j = 0\n",
    "            \n",
    "            #Aqui obtenemos la predicción de mayor probabilidad y la guardamos en nuestra lista de predicciones\n",
    "            self.prediccion.append(np.argmax(mapa))\n",
    "        \n",
    "        #Devolvemos la lista con la predicción de nuestro clasifica   \n",
    "        return self.prediccion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2. 2. 2. 0. 1. 2.]\n",
      "[2, 2, 2, 2, 1, 2]\n",
      "El error del Clasificador NaiveBayes es: 0.16666666666666666\n"
     ]
    }
   ],
   "source": [
    "clasificador = ClasificadorNaiveBayes()\n",
    "#clasificador.entrenamiento(dataset, validacion_simple.particiones[0].indicesTrain)\n",
    "#clasificador.clasifica(dataset, validacion_simple.particiones[0].indicesTest)\n",
    "error1 = clasificador.validacion(validacion_simple,dataset,clasificador)\n",
    "print(\"El error del Clasificador NaiveBayes es: \"+str(error1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicciones utilizando validacion simple sin correcion de Laplace: [2. 2. 2. 2. 2.]\n",
      "Clases reales de la particion de text [2. 2. 0. 2. 0.]\n",
      "Predicciones utilizando validacion simple con correcion de Laplace: [2. 2. 2. 2. 2.]\n",
      "Clases reales de la particion de text [2. 2. 0. 2. 0.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pablo/anaconda3/lib/python3.7/site-packages/sklearn/preprocessing/_encoders.py:415: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/pablo/anaconda3/lib/python3.7/site-packages/sklearn/preprocessing/_encoders.py:451: DeprecationWarning: The 'categorical_features' keyword is deprecated in version 0.20 and will be removed in 0.22. You can use the ColumnTransformer instead.\n",
      "  \"use the ColumnTransformer instead.\", DeprecationWarning)\n",
      "/home/pablo/anaconda3/lib/python3.7/site-packages/sklearn/naive_bayes.py:485: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n",
      "/home/pablo/anaconda3/lib/python3.7/site-packages/sklearn/naive_bayes.py:485: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n"
     ]
    }
   ],
   "source": [
    "#SKLearn Validacion Cruzada\n",
    "# Hacemos el encode de los datos\n",
    "encAtributos = preprocessing.OneHotEncoder(categorical_features=dataset.nominalAtributos[:-1],sparse=False)\n",
    "X = encAtributos.fit_transform(dataset.datos[:,:-1])\n",
    "Y = dataset.datos[:,-1]\n",
    "\n",
    "# Partimos los datos en Train y Test\n",
    "x_train, x_test, y_train,y_test = train_test_split(X,Y,test_size=0.2)\n",
    "\n",
    "# Aplicamos Naive Bayes sin Laplace\n",
    "clf = MultinomialNB(alpha = 0,fit_prior=True, class_prior=None)\n",
    "predicciones=clf.fit(x_train,y_train).predict(x_test)\n",
    "\n",
    "# Print de los resultados\n",
    "print(\"Predicciones utilizando validacion simple sin correcion de Laplace:\", predicciones)\n",
    "print(\"Clases reales de la particion de text\", y_test)\n",
    "\n",
    "# Aplicamos Naive Bayes con Laplace\n",
    "clf = MultinomialNB(alpha = 0,fit_prior=True, class_prior=None)\n",
    "predicciones=clf.fit(x_train,y_train).predict(x_test)\n",
    "\n",
    "# Print de los resultados\n",
    "print(\"Predicciones utilizando validacion simple con correcion de Laplace:\", predicciones)\n",
    "print(\"Clases reales de la particion de text\", y_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicciones utilizando validacion simple sin correcion de Laplace: [2. 2. 2. 2. 2.]\n",
      "Clases reales de la particion de text [2. 2. 0. 2. 2.]\n",
      "Predicciones utilizando validacion simple con correcion de Laplace: [2. 2. 2. 2. 2.]\n",
      "Clases reales de la particion de text [2. 2. 0. 2. 2.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pablo/anaconda3/lib/python3.7/site-packages/sklearn/preprocessing/_encoders.py:415: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/pablo/anaconda3/lib/python3.7/site-packages/sklearn/preprocessing/_encoders.py:451: DeprecationWarning: The 'categorical_features' keyword is deprecated in version 0.20 and will be removed in 0.22. You can use the ColumnTransformer instead.\n",
      "  \"use the ColumnTransformer instead.\", DeprecationWarning)\n",
      "/home/pablo/anaconda3/lib/python3.7/site-packages/sklearn/naive_bayes.py:485: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n",
      "/home/pablo/anaconda3/lib/python3.7/site-packages/sklearn/naive_bayes.py:485: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n"
     ]
    }
   ],
   "source": [
    "# SKLearn Validacion Cruzada\n",
    "# Hacemos el encode de los datos\n",
    "encAtributos = preprocessing.OneHotEncoder(categorical_features=dataset.nominalAtributos[:-1],sparse=False)\n",
    "X = encAtributos.fit_transform(dataset.datos[:,:-1])\n",
    "Y = dataset.datos[:,-1]\n",
    "\n",
    "# Partimos los datos en Train y Test\n",
    "x_train, x_test, y_train,y_test = train_test_split(X,Y,test_size=0.2)\n",
    "\n",
    "# Aplicamos Naive Bayes sin Laplace\n",
    "clf = MultinomialNB(alpha = 0,fit_prior=True, class_prior=None)\n",
    "predicciones=clf.fit(x_train,y_train).predict(x_test)\n",
    "\n",
    "# Print de los resultados\n",
    "print(\"Predicciones utilizando validacion simple sin correcion de Laplace:\", predicciones)\n",
    "print(\"Clases reales de la particion de text\", y_test)\n",
    "\n",
    "# Aplicamos Naive Bayes con Laplace\n",
    "clf = MultinomialNB(alpha = 0,fit_prior=True, class_prior=None)\n",
    "predicciones=clf.fit(x_train,y_train).predict(x_test)\n",
    "\n",
    "# Print de los resultados\n",
    "print(\"Predicciones utilizando validacion simple con correcion de Laplace:\", predicciones)\n",
    "print(\"Clases reales de la particion de text\", y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
