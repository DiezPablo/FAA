{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/python3\n",
    "# -*- coding: utf-8 -*-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importamos Librerias\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from sortedcontainers import SortedDict\n",
    "import sklearn.naive_bayes as nb\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold, cross_val_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Datos:\n",
    "\n",
    "  TiposDeAtributos=('Continuo','Nominal')\n",
    "\n",
    "  # TODO: procesar el fichero para asignar correctamente las variables tipoAtributos, nombreAtributos, nominalAtributos, datos y diccionarios\n",
    "  # NOTA: No confundir TiposDeAtributos con tipoAtributos\n",
    "  def __init__(self, nombreFichero):\n",
    "\n",
    "      with open(nombreFichero, \"r\") as f:\n",
    "        # Guardamos el numero de datos que contiene el DataSet y esta en la primera linea\n",
    "        self.numDatos = int(f.readline())\n",
    "\n",
    "        # Guardamos el nombre de los atributos\n",
    "        self.nombreAtributos = f.readline().strip('\\n').split(',')\n",
    "        #print(self.nombreAtributos)\n",
    "\n",
    "        # Leemos el tipo de los atributos de las variables y eliminamos el ultimo que es un salto de linea\n",
    "        self.tipoAtributos = f.readline().strip('\\n').split(',')\n",
    "        #print(self.tipoAtributos)\n",
    "\n",
    "        # Comprobamos que todos los atributos sean Continuos o Nominales\n",
    "        if any(atr not in Datos.TiposDeAtributos for atr in self.tipoAtributos):\n",
    "            raise ValueError(\"Tipo de atributo erroneo\")\n",
    "\n",
    "        # Segun el atributo, asignamos True o False.\n",
    "        self.nominalAtributos = []\n",
    "\n",
    "        # Guardamos en la lista nominalAtributos en la posicion de cada uno si es o no Nominal\n",
    "        for tipo in self.tipoAtributos:\n",
    "            if tipo == self.TiposDeAtributos[0]:\n",
    "                self.nominalAtributos.append(False)\n",
    "            else:\n",
    "                self.nominalAtributos.append(True)\n",
    "        #print(self.nominalAtributos)\n",
    "\n",
    "        # Guardamos los datos del fichero y los formateamos, de tal forma que cada linea es una lista\n",
    "        datos = f.readlines()\n",
    "        datosFormat = []\n",
    "        for lista in datos:\n",
    "            datosFormat.append(lista.strip('\\n').split(','))\n",
    "\n",
    "        # print(set(sorted(datosFormat[0])))\n",
    "        listaDatosAtributos = []\n",
    "        for i in range(len(self.tipoAtributos)):\n",
    "            listaDatosAtributos.append([])\n",
    "\n",
    "        # Hacemos la traspuesta de los datos que guardamos para que cada lista de atributo guarde todos los datos\n",
    "        # de cada atributo.\n",
    "        for lista in datosFormat:\n",
    "            i = 0\n",
    "            for item in lista:\n",
    "                listaDatosAtributos[i].append(item)\n",
    "                i += 1\n",
    "\n",
    "        # Ordenamos y hacemos un set para eliminar repetidos.\n",
    "        i = 0\n",
    "        for item in listaDatosAtributos:\n",
    "            listaDatosAtributos[i] = sorted(set(item))\n",
    "            i += 1\n",
    "\n",
    "\n",
    "        # Creacion de lista diccionarios, en caso de que el atributo sea Continuo, el diccionario estara vacio\n",
    "        self.listaDicts = []\n",
    "        for i in range(len(self.tipoAtributos)):\n",
    "            self.listaDicts.append({})\n",
    "\n",
    "        # Creamos el diccionario tal y como se describe en las diapositivas, por orden y asignando valores numericos crecientes\n",
    "        i = 0\n",
    "        for atributo in listaDatosAtributos:\n",
    "            k = 0\n",
    "            if self.tipoAtributos[i] == \"Nominal\":\n",
    "                for dato in atributo:\n",
    "                    self.listaDicts[i][dato] = k\n",
    "                    k += 1\n",
    "            i += 1\n",
    "\n",
    "        # Creacion de la matriz de datos utilizando el diccionario para mapear los valores\n",
    "        # En primer lugar, creamos una matriz vacia de tamaña numero de atributos.\n",
    "        self.datos = np.empty((int(self.numDatos),int(len(self.tipoAtributos))))\n",
    "        i = 0\n",
    "        j = 0\n",
    "\n",
    "        # Metemos los datos en la matriz, mapeando con los diccionarios en el caso de que sean Nominales, y si son continuos normal.\n",
    "        for i in range(int(self.numDatos)):\n",
    "            for j in range(len(self.tipoAtributos)):\n",
    "                if self.tipoAtributos[j] == 'Nominal':\n",
    "                    self.datos[i][j] = self.listaDicts[j].get(str(datosFormat[i][j]))\n",
    "                else:\n",
    "                    self.datos[i][j] = datosFormat[i][j]\n",
    "        \n",
    "        print(self.nombreAtributos)\n",
    "        print(self.listaDicts)\n",
    "        f.close()\n",
    "\n",
    "\n",
    "  # TODO: implementar en la practica 1\n",
    "  def extraeDatos(self, idx):\n",
    "    return self.datos[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Age', 'Spectacle', 'Astigmatic', 'Tear', 'Class']\n",
      "[{'1': 0, '2': 1, '3': 2}, {'1': 0, '2': 1}, {'1': 0, '2': 1}, {'1': 0, '2': 1}, {'1': 0, '2': 1, '3': 2}]\n",
      "[[0. 0. 0. 0. 2.]\n",
      " [0. 0. 0. 1. 1.]\n",
      " [0. 0. 1. 0. 2.]\n",
      " [0. 0. 1. 1. 0.]\n",
      " [0. 1. 0. 0. 2.]\n",
      " [0. 1. 0. 1. 1.]\n",
      " [0. 1. 1. 0. 2.]\n",
      " [0. 1. 1. 1. 0.]\n",
      " [1. 0. 0. 0. 2.]\n",
      " [1. 0. 0. 1. 1.]\n",
      " [1. 0. 1. 0. 2.]\n",
      " [1. 0. 1. 1. 0.]\n",
      " [1. 1. 0. 0. 2.]\n",
      " [1. 1. 0. 1. 1.]\n",
      " [1. 1. 1. 0. 2.]\n",
      " [1. 1. 1. 1. 2.]\n",
      " [2. 0. 0. 0. 2.]\n",
      " [2. 0. 0. 1. 2.]\n",
      " [2. 0. 1. 0. 2.]\n",
      " [2. 0. 1. 1. 0.]\n",
      " [2. 1. 0. 0. 2.]\n",
      " [2. 1. 0. 1. 1.]\n",
      " [2. 1. 1. 0. 2.]\n",
      " [2. 1. 1. 1. 2.]]\n"
     ]
    }
   ],
   "source": [
    "dataset = Datos('lenses.data')\n",
    "\n",
    "print(dataset.datos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABCMeta,abstractmethod\n",
    "\n",
    "\n",
    "class Particion():\n",
    "\n",
    "  # Esta clase mantiene la lista de �ndices de Train y Test para cada partici�n del conjunto de particiones\n",
    "    def __init__(self,train=[],test=[]):\n",
    "        self.indicesTrain=train\n",
    "        self.indicesTest=test\n",
    "    \n",
    "    def __str__(self):\n",
    "        return \"Train: {}\\nTest:  {}\".format(str(self.indicesTrain),str(self.indicesTest)) \n",
    "\n",
    "#####################################################################################################\n",
    "\n",
    "class EstrategiaParticionado:\n",
    "\n",
    "    # Clase abstracta\n",
    "    __metaclass__ = ABCMeta\n",
    "\n",
    "    # Lista de las particiones\n",
    "    def __init__(self, nombre=\"\"):\n",
    "        self.nombreEstrategia = nombre\n",
    "        self.numeroParticiones = 0\n",
    "        self.particiones=[]\n",
    "\n",
    "    # Atributos: deben rellenarse adecuadamente para cada estrategia concreta: nombreEstrategia, numeroParticiones, listaParticiones. Se pasan en el constructor\n",
    "\n",
    "    @abstractmethod\n",
    "    # TODO: esta funcion deben ser implementadas en cada estrategia concreta\n",
    "    def creaParticiones(self,datos,seed=None):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValidacionSimple(EstrategiaParticionado):\n",
    "    \n",
    "    def __init__(self, porcentaje):\n",
    "        self.porcentaje = porcentaje\n",
    "        super().__init__(\"Validacion simple\")\n",
    "        \n",
    "  # Crea particiones segun el metodo tradicional de division de los datos segun el porcentaje deseado.\n",
    "  # Devuelve una lista de particiones (clase Particion)\n",
    "  # TODO: implementar\n",
    "    def creaParticiones(self,datos,seed=None):\n",
    "        np.random.seed(seed)\n",
    "\n",
    "        self.numeroParticiones = 1\n",
    "    \n",
    "        # Generamos una lista con todos los números de datos aleatorios   \n",
    "        indicesAleatorios = np.random.permutation(int(datos.numDatos))\n",
    "        \n",
    "        # Creamos la particion, en funcion del porcentaje especificado\n",
    "        self.particiones = [Particion(indicesAleatorios[:int(datos.numDatos*self.porcentaje)],\n",
    "                                      indicesAleatorios[int(datos.numDatos*self.porcentaje):])]\n",
    "        \n",
    "        return self.particiones\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [ 0 23  3  8 18  5  6  2 15 21  4  9 22 19 13 10 11 17]\n",
      "Test:  [14 16 20 12  7  1]\n"
     ]
    }
   ],
   "source": [
    "validacion_simple = ValidacionSimple(0.75)\n",
    "validacion_simple.creaParticiones(dataset)\n",
    "print(validacion_simple.particiones[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValidacionCruzada(EstrategiaParticionado):\n",
    "\n",
    "  # Crea particiones segun el metodo de validacion cruzada.\n",
    "  # El conjunto de entrenamiento se crea con las nfolds-1 particiones y el de test con la particion restante\n",
    "  # Esta funcion devuelve una lista de particiones (clase Particion)\n",
    "  # TODO: implementar\n",
    "    \n",
    "    def __init__(self, k):\n",
    "        self.k = k\n",
    "        super().__init__(\"Validacion cruzada\")\n",
    "\n",
    "    def creaParticiones(self,datos,seed=None):\n",
    "        np.random.seed(seed)\n",
    "        \n",
    "        self.numeroParticiones = self.k\n",
    "        \n",
    "        # Generamos una lista con todos los números de datos aleatorios   \n",
    "        indicesAleatorios = np.random.permutation(int(datos.numDatos))\n",
    "        \n",
    "        # Hallamos el tamaño de cada bloque\n",
    "        tamBloque = int(datos.numDatos/self.k)\n",
    "        \n",
    "        \n",
    "        datosSobran = datos.numDatos - (tamBloque*self.k)\n",
    "        count = 0\n",
    "        for i in range(self.k):\n",
    "            \n",
    "            train = np.delete(indicesAleatorios, range(i*tamBloque,(i+1)*tamBloque))\n",
    "            test =  indicesAleatorios[i*tamBloque:(i+1)*tamBloque]\n",
    "            \n",
    "            # Caso en el que la cuenta es justa\n",
    "            if datosSobran == 0:\n",
    "                self.particiones.append(Particion(train, test))\n",
    "                \n",
    "            # Contemplamos el caso de que la division para sacar el numero de subconjuntos no fuese entera\n",
    "            if datosSobran > 0:\n",
    "                count += 1\n",
    "                particionTest = np.append(test, train[(datos.numDatos - tamBloque)- i - 1])\n",
    "                particionTrain = np.delete(train, (datos.numDatos - tamBloque)- i - 1)\n",
    "                datosSobran -= 1\n",
    "                self.particiones.append(Particion(particionTrain, particionTest))\n",
    "                \n",
    "            \n",
    "           \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [ 0 14  6  8  2  1 10 21 16 20 17  3 15  9 22 13 11 19]\n",
      "Test:  [18  7  5  4 23 12]\n",
      "Train: [18  7  5  4 23 12 10 21 16 20 17  3 15  9 22 13 11 19]\n",
      "Test:  [ 0 14  6  8  2  1]\n",
      "Train: [18  7  5  4 23 12  0 14  6  8  2  1 15  9 22 13 11 19]\n",
      "Test:  [10 21 16 20 17  3]\n",
      "Train: [18  7  5  4 23 12  0 14  6  8  2  1 10 21 16 20 17  3]\n",
      "Test:  [15  9 22 13 11 19]\n"
     ]
    }
   ],
   "source": [
    "v_cruzada = ValidacionCruzada(4)\n",
    "v_cruzada.creaParticiones(dataset)\n",
    "\n",
    "for particion in v_cruzada.particiones:\n",
    "    print(particion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Clasificador:\n",
    "  \n",
    "    # Clase abstracta\n",
    "    __metaclass__ = ABCMeta\n",
    "\n",
    "    # Metodos abstractos que se implementan en casa clasificador concreto\n",
    "    @abstractmethod\n",
    "    # TODO: esta funcion debe ser implementada en cada clasificador concreto\n",
    "    # datosTrain: matriz numpy con los datos de entrenamiento\n",
    "    # atributosDiscretos: array bool con la indicatriz de los atributos nominales\n",
    "    # diccionario: array de diccionarios de la estructura Datos utilizados para la codificacion de variables discretas\n",
    "    def entrenamiento(self,datos,datosTrain,atributosDiscretos,diccionario):\n",
    "        pass\n",
    "\n",
    "\n",
    "    @abstractmethod\n",
    "    # TODO: esta funcion debe ser implementada en cada clasificador concreto\n",
    "    # devuelve un numpy array con las predicciones\n",
    "    def clasifica(self,datosTest,atributosDiscretos,diccionario):\n",
    "        pass\n",
    "\n",
    "\n",
    "    # Obtiene el numero de aciertos y errores para calcular la tasa de fallo\n",
    "    # TODO: implementar\n",
    "    def error(self,datos,pred):\n",
    "    # Aqui se compara la prediccion (pred) con las clases reales y se calcula el error    \n",
    "        i = 0\n",
    "        real = datos[:,-1]\n",
    "        error = 0\n",
    "        for i in range(len(real)):\n",
    "            if real[i] != pred[i]:\n",
    "                error += 1\n",
    "        err = (error)/(len(real)+0.0)\n",
    "        return err\n",
    "\n",
    "\n",
    "    # Realiza una clasificacion utilizando una estrategia de particionado determinada\n",
    "    # TODO: implementar esta funcion\n",
    "    def validacion(self,particionado,dataset,clasificador,seed=None):\n",
    "\n",
    "    # Creamos las particiones siguiendo la estrategia llamando a particionado.creaParticiones\n",
    "    # - Para validacion cruzada: en el bucle hasta nv entrenamos el clasificador con la particion de train i\n",
    "    # y obtenemos el error en la particion de test i\n",
    "    # - Para validacion simple (hold-out): entrenamos el clasificador con la particion de train\n",
    "    # y obtenemos el error en la particion test. Otra opci�n es repetir la validaci�n simple un n�mero especificado de veces, obteniendo en cada una un error. Finalmente se calcular�a la media.\n",
    "        errores = 0\n",
    "        #particionado.creaParticiones(dataset, seed)\n",
    "    # Comprobamos si es por validación cruzada o simple, por la longitud de la lista de particiones\n",
    "    # Validación Simple\n",
    "        if len(particionado.particiones) == 1:\n",
    "            clasificador.entrenamiento(dataset, particionado.particiones[0].indicesTrain)\n",
    "            pred = clasificador.clasifica(dataset,particionado.particiones[0].indicesTest)\n",
    "            ret = self.error(dataset.extraeDatos(particionado.particiones[0].indicesTest), pred)\n",
    "            if ret > 0:\n",
    "                return ret\n",
    "            else:\n",
    "                return 0\n",
    "            \n",
    "    # Validación Cruzada        \n",
    "        else:\n",
    "            for particion in particionado.particiones:\n",
    "                clasificador.entrenamiento(dataset, particion.indicesTrain)\n",
    "                pred = clasificador.clasifica(dataset,particion.indicesTest)\n",
    "                ret = self.error(dataset.extraeDatos(particion.indicesTest), pred)\n",
    "                errores += ret\n",
    "            error = errores/len(particionado.particiones)\n",
    "            #Devolucion de la media de los errores\n",
    "            return error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClasificadorNaiveBayes(Clasificador):\n",
    "    \n",
    "    def __init__(self, laplace):\n",
    "        self.laplace = laplace\n",
    "        \n",
    "    \n",
    "    def entrenamiento(self,dataset,datosTrain):\n",
    "     \n",
    "        # Cargamos todos los datos de la clase del dataset desde la matriz de datos\n",
    "        clasesTrain = dataset.extraeDatos(datosTrain)\n",
    "        self.numClases = clasesTrain[:,-1]\n",
    "        # Contamos las apariciones de cada uno para luego calcular la probabilidad a priori de cada clase\n",
    "        counter = Counter(self.numClases)\n",
    "        # Calculamos la probabilidad de la clase y lo metemos en un diccionario ordenado segun el numero \n",
    "        # correspondiente a cada clase asignado en el diccionario\n",
    "        self.dictPrioris={}\n",
    "        for k in counter:\n",
    "            k = int(k)\n",
    "            counter[k] = counter[k]/len(self.numClases)\n",
    "            self.dictPrioris[k] = counter[k]\n",
    "            \n",
    "        # Aqui ordenamos el diccionario para que esten en el mismo orden de como extraemos los datos del dataset\n",
    "        self.dictPrioris = SortedDict(self.dictPrioris)\n",
    "        \n",
    "        # Calcular tablas de probabilidades del entrenamiento. Tenemos que calcular por cada atributo una cuenta\n",
    "        # de las apariciones en cada clase\n",
    "        # Creamos una lista de matrices, donde vamos almacenar todos los datos que hemos obtenido en los datos de Test\n",
    "        self.posteriori = np.zeros(len(dataset.nombreAtributos)-1,dtype=object)\n",
    "        \n",
    "        # Recorremos todos los datos de la matriz sin llegar a la clase\n",
    "        for i in range(len(dataset.nombreAtributos) - 1):\n",
    "            \n",
    "            # Si el dato que obtenemos es Nominal haremos el recuento de todas las veces que sale la P(D|H)\n",
    "            if dataset.nominalAtributos[i] == True:\n",
    "                \n",
    "                # Creamos una matriz de tamaño X: Número de Atributos menos la clase Y: Número de clases\n",
    "                post = np.zeros((len(dataset.listaDicts[i]),len(dataset.listaDicts[-1])))\n",
    "               \n",
    "                # Aqui contamos todos las datos que queremos del datos Train para construir la matriz de entrenamiento\n",
    "                for c in range(len(dataset.listaDicts[-1])):\n",
    "                    datosEnt = dataset.extraeDatos(datosTrain)\n",
    "                    dat = datosEnt[:,i]\n",
    "                    repes = Counter(dat[datosEnt[:,-1] == c])\n",
    "                    for r in repes:\n",
    "                        post[int(r),c] = repes[r]\n",
    "                    if self.laplace == True:\n",
    "                        self.posteriori[i] = post +1\n",
    "                    else:\n",
    "                        self.posteriori[i] = post\n",
    "            \n",
    "            # Si el dato es Continuo obtendremos la media y la desviación tipica de la clase\n",
    "            else:\n",
    "                \n",
    "                # Creamos una matriz de X: Los datos de Media y Desivación típica Y: Número de clases\n",
    "                post = np.zeros((2,len(dataset.listaDicts[-1])))\n",
    "               \n",
    "                # Aqui obtenemos la media y desviación tipica de cada clase, despues de tener los datos de entrenamiento\n",
    "                for c in range(len(dataset.listaDicts[-1])):\n",
    "                    datosEnt = dataset.extraeDatos(datosTrain)\n",
    "                    dat = datosEnt[:,i]\n",
    "                    datos = dat[datosEnt[:,-1] == c]\n",
    "                    post[0][c] = np.mean(datos)\n",
    "                    post[1][c] = np.std(datos)\n",
    "                self.posteriori[i] = post\n",
    "            \n",
    "        #Calculamos los valores de los posteriori de todos las tablas anteriores\n",
    "        for j in range(len(dataset.nominalAtributos)-1):\n",
    "            if dataset.nominalAtributos[j] == True:\n",
    "                for i in range(len(dataset.listaDicts) - 1):\n",
    "                    self.posteriori[i] /= sum(self.posteriori[i])\n",
    "\n",
    "        \n",
    "    def clasifica(self,dataset,datosTest):\n",
    "        j = 0\n",
    "        aux = 1\n",
    "        self.prediccion = []\n",
    "        datTest = dataset.extraeDatos(datosTest)\n",
    "        #Ahora vamos a estudiar la probabilidad de la clase con los datos obtenidos en el entrenamiento\n",
    "        #Recorremos todos las datos de la matriz de los datos Test\n",
    "        \n",
    "        for dato in datTest:\n",
    "            mapa = []\n",
    "            #Aqui obtenemos los prioris de cada clase para poder obtener la probabilidad de cada una\n",
    "            for clase in range(len(self.dictPrioris)):\n",
    "            #Aqui obtenemos cada valor posteriori de nuestro entrenamiento de los datos, es decir, P(D|H)\n",
    "                for atributo in range(len(self.posteriori)):\n",
    "                    if dataset.nominalAtributos[j] == True:\n",
    "                        aux *= self.posteriori[atributo][int(dato[atributo])][clase]\n",
    "                    #Aqui obtenemos la siguiente probabilidad P(D|H)*P(H)\n",
    "                        aux2 = self.dictPrioris.get(clase)*aux\n",
    "                        aux = 1\n",
    "                \n",
    "                    #Lo añadimos a una lista para obtener la probabilidad de las diferentes clases\n",
    "                        mapa.append(aux2)\n",
    "            \n",
    "            #Aqui obtenemos la probabilidad de los atibutos continuos\n",
    "                    else:\n",
    "                        # Hacemos la formula de la distribucion normal\n",
    "                        exp1 = 1/(self.posteriori[atributo][0][clase]*math.sqrt(2*math.pi))\n",
    "                        exp2 = ((dato-self.posteriori[atributo][0][clase]) - dato[atributo]/self.posteriori[atributo][1][clase])\n",
    "                        exp3 = exp2 ** 2\n",
    "                        exp4 = math.exp((-1/2)* exp3)\n",
    "                        total = exp1 * exp4\n",
    "                        aux2 = total * self.dictPrioris.get(clase)\n",
    "                        mapa.append(aux2)\n",
    "            \n",
    "            if j == len(dataset.nominalAtributos)-1:\n",
    "                j = 0\n",
    "            j += 1\n",
    "            #Aqui obtenemos la predicción de mayor probabilidad y la guardamos en nuestra lista de predicciones\n",
    "            self.prediccion.append(np.argmax(mapa))\n",
    "        \n",
    "        #Devolvemos la lista con la predicción de nuestro clasifica   \n",
    "        return self.prediccion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El error del Clasificador NaiveBayes  para Validacion Simple es: 1.0\n",
      "El error del Clasificador NaiveBayes para Validacion Cruzada es: 1.0\n"
     ]
    }
   ],
   "source": [
    "clasificador = ClasificadorNaiveBayes(True)\n",
    "#clasificador.entrenamiento(dataset, validacion_simple.particiones[0].indicesTrain)\n",
    "#clasificador.clasifica(dataset, validacion_simple.particiones[0].indicesTest)\n",
    "error = clasificador.validacion(validacion_simple,dataset,clasificador)\n",
    "error1 = clasificador.validacion(v_cruzada,dataset,clasificador)\n",
    "print(\"El error del Clasificador NaiveBayes  para Validacion Simple es: \"+str(error))\n",
    "print(\"El error del Clasificador NaiveBayes para Validacion Cruzada es: \"+str(error1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['TLeftSq', 'TMidSq', 'TRightSq', 'MLeftSq', 'MMidSq', 'MRightSq', 'BLeftSq', 'BMidSq', 'BRightSq', 'Class']\n",
      "[{'b': 0, 'o': 1, 'x': 2}, {'b': 0, 'o': 1, 'x': 2}, {'b': 0, 'o': 1, 'x': 2}, {'b': 0, 'o': 1, 'x': 2}, {'b': 0, 'o': 1, 'x': 2}, {'b': 0, 'o': 1, 'x': 2}, {'b': 0, 'o': 1, 'x': 2}, {'b': 0, 'o': 1, 'x': 2}, {'b': 0, 'o': 1, 'x': 2}, {'negative': 0, 'positive': 1}]\n",
      "[[2. 2. 2. ... 1. 1. 1.]\n",
      " [2. 2. 2. ... 2. 1. 1.]\n",
      " [2. 2. 2. ... 1. 2. 1.]\n",
      " ...\n",
      " [1. 2. 1. ... 1. 2. 0.]\n",
      " [1. 2. 1. ... 1. 2. 0.]\n",
      " [1. 1. 2. ... 2. 2. 0.]]\n"
     ]
    }
   ],
   "source": [
    "dataset = Datos('tic-tac-toe.data')\n",
    "print(dataset.datos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VALIDACION SIMPLE Train: [227 479 228 218 727 419 585 224 453 518 405 643 908 769 226 336 304  15\n",
      " 310  65 524 752  81 870 618  97  24 849 679 142 324 584 530 262  86 313\n",
      " 906 730 881  58 536 354 754 493 233 559 701 297 534   9  22 834 764 705\n",
      " 582 484 388 683 529 501 496 938 285 673 873  46 467 888 790 465 554 954\n",
      "  88 540 422   4 179 709 334 383 605  83 443  84 393 855 661 668 604  96\n",
      " 252 217  92 659 751 332 273 319 266 716  35 437  63 377 404 429 538  85\n",
      " 132 731 167 199 788 749 244 694 251 741 653 478 434 586 560 507 774 662\n",
      " 482 597 394 577 298 939 574 678 519 674 689 426 126 386 204 119 671 136\n",
      " 675 160 804 114 145 278 495 883 348 838 169 463 579 905 856 787 885 197\n",
      " 372 923 865 124 315 949 338 430 658 628  73 580 381 223 477 190 522 464\n",
      "  49 107 314 106 164 370 311 161 935 309 508 677 216 175 594  53 431 713\n",
      "  69 186 652 466 123 206 492  91 957 494 592 146 414   6  94 205 836 531\n",
      " 547 891 104 916 743 410  11 707 125 552 686 317 940 735  41 328 392 601\n",
      " 403 509 789 818 640  89 942 843 808 685 396 567  42 212 526 408 444 139\n",
      " 841 177 203 210  76 644 337 260 913 250 375  74 208 575 286 645 761 546\n",
      " 345 475 822 807 919 436 265 904 539 718 832  18 420 688 520 180  33  21\n",
      " 149  80  64 902 748 473 725  37 363 454 320 331 616 441 837 442 264 634\n",
      " 929  34 738 625 240 893  25 826 932 669 451 418 889 358 178 852   2 877\n",
      " 230 553 213 920  61 168 869 506 133 936 766 620  82 172 786 953 903 176\n",
      " 280 371  71 739 641 109 379 861 630 564 153 665 657 326 360 811 417 516\n",
      " 202  17   8 416 810 693 527  14 421 187 860 237 840  44 340 502 452 895\n",
      " 746 445 664 308 275 461 619 829 795 772 222 945 898 292  59 274 844 562\n",
      " 397 700 745 854  52 598 859 692 543 858 347 198 590 143 702  78 469 472\n",
      "  31 103 115 122 296 617 166 946 312 721 270 851 344 148 196 704 368 847\n",
      " 261  29 621 248 651 583 353 155 733  16 446 779 785 350 770 663 110 941\n",
      " 711 424 943 947 511 886 907 607 323 864 120  66 765 299   0 183 163 485\n",
      "   3 793 720 256 918  23 734 648 875 717 305 712 128 259 325 930 613 828\n",
      " 490  47 229 255 642 105 815 171 623 341 474 596 351 342 896 359 384 211\n",
      "  12 165   5 427 489 138 130 111 293 162 239 595 150 129 283 366 719 480\n",
      "  57 378  60 955 459 728 924 636 928 279 400 225 483 909 151 235 576 722\n",
      " 606 468 656 462 654 322 333 876 736 380 357 753 343 505 412  45 385 915\n",
      " 549 767 937 825 152 646 792 708 599  30 470  19 639 884 158 450 395 775\n",
      " 593 131 603 622 140 697 629 409 406 156 681 933 894 778 848 750 221 846\n",
      " 500 352 513 269 626 276 819 791 777 236 901 867 715  55 633 390 215 232\n",
      " 880 710 756 231 561 159 850  93 209 887 447 195 455 435 108 747 267 556\n",
      " 863 327 780 670 491 438 737 773  48 696 193 137 486 820 950 755 425 174\n",
      " 391  28 440 871 882 290 899 558 476 555 307 135 682 925  87 487 569 784\n",
      " 921 301  68 802 246  70 812 303 892 382 900 245 423 878 537 401 402 504\n",
      " 800 243 698 566 116 589  90 768 182 824 191 523 680 134 389  51 339 637\n",
      "  10 300 723 127 862 112 550 330 823 703 355 676 695  27 542 439]\n",
      "Test:  [ 39 349 407  13 667 220 374 141 742 845 214 302 926  38 627  20 428 194\n",
      " 356 609 760 912 545 842 794 635 369 624 373 835 726 799  98 821 289 184\n",
      " 432 481 591 471 113 655 757 951 931 201 532 874  50 587 806 399 517 691\n",
      "  95 948 497 797 238 192 189 207   1 615 181 411 535 288 660 503 449 796\n",
      " 263 872 460 257 284 512 783  54  26  40 814 254 514 666 910 219 857 253\n",
      " 282 544 291 247 321 170 816 714  32 853 571 488 608  36 173 185 364 944\n",
      " 803 147 457 335 672 612 367 563 578 911 638 157 610 200 927 805 687 361\n",
      " 581  67  99 548 515 281 798 771  43 706 249 433 101 801 376 456 234 362\n",
      " 740 744 827 346 521 600 498 271 277 897 631 890 117 448  79 458 551 387\n",
      " 287 568 879   7 588 258 602 318 241  75 102 914 632 499 316 922 242 831\n",
      "  56 952 729 782 690 118  77 759 272 398 533 762 724 763 813 365 611 839\n",
      " 306 572 528 510  62 917 830 758 525 776 699 295 934 956 781 188 647 650\n",
      " 570 565 866 144 868 329 100 649 268 415 294 684 541 573 817 121 732 413\n",
      " 154 614 557 833 809  72]\n",
      "VALIDACION CRUZADA\n",
      "Train: [146 889 780 748 293 448  85 682  69 539  23 601 634 243 162 919 936 583\n",
      " 323 792 359 178 608 121 728 342 466 709 827 119 607 369 397  99 326 690\n",
      " 903 115 867 580 140 136 775 779 108 560 691 547 957  97 338 883 147 546\n",
      " 113 661 910 487 475 700 207 319 444 245 177  74 205 167 271 823 713 694\n",
      " 102 744 170 368 593 599  15 590 627 183 802 922 237 274 888 152 461 534\n",
      " 142 623 486  86 906 708 333 357  96 558 196  84 273 236 680 190 760 639\n",
      "  95 631 417 277 428 912 225 900 832 429 309 751 764  46 750 752 520 406\n",
      "  79 395 813 386 250  21  51 554 839 688 443 269 383 385 304 107 266 696\n",
      " 916 932 645 176 180 281 482  33 168 877 396 540 435 620 829 686 644 624\n",
      " 759 513 211 529 567  40 305 462 169  30  11 557 454 545 335 425 847 582\n",
      " 651 130 897 148 561 232 615  42 766 214  52 298 470 501 613 657 793 201\n",
      " 649 769   4 926   6 575 616 465 716 853 334 366 584 144 360  80 531 158\n",
      " 840 668 450 852  63 296 743 757 819 439 445 724 161 937 587  41 317 351\n",
      " 565 538 522 457 340 652 198 490  18 299 440 579 772 468 878 774 156 950\n",
      " 536 515 745 473 185  20 379 489 213 908  14 364  35 275 741 941 532 373\n",
      " 265 285  78 491 678 313 508 917 307  39 226 660 761 219 689 799  94 355\n",
      " 814 413  57 114 512 258 785 242 478 749 502 882 718 505 376 571 471 864\n",
      " 542 372 401 717 511 276  88 569 118  61 288 946 484 474 614 122 815 325\n",
      " 197 890 736 791  38 400 704 252 110 453 516 283 188 626  62 610 742 598\n",
      " 798  25 206 441 481 734 424   0 452 314 157 353 410 391 523 770 876 655\n",
      " 535 725  10 866 469 367 849 216 699 731 589  55 229 324 572 222 621 431\n",
      " 312 541 758 350 264 344 928   7 860 328 677 290  73 464 203  58 828 215\n",
      " 221 648 149 220 782 684 195 695 553 240 393 132 472 911 166  92 123 336\n",
      " 617   3 729 548 409 662 658 822 796 451 550 811 138 637 525 933 297 278\n",
      " 855 851 286 809 854 633   8 687 735 788 251 836 381 101 494 930 762 865\n",
      " 228 934 921 901 818 859 795 835 378 404 398 204 767 248 103 712 330 153\n",
      " 485 192 249 710 705  89 503 145 570  83 817 524 701 756 172 904 654 498\n",
      " 676 679 632 787 824 805 754 885 544  29 675 521 234 856 844 830 133 504\n",
      " 238 405 697 282  81 263 143 671 765   9 703 646 858 723  47 886 611 306\n",
      " 596 261 380 120 267  77 647 918 320 477 246 187 374 771 262  87 449 348\n",
      " 137 483 210 253 692 730 816 597 388 527 224 193 354 820 528 673 495 260\n",
      " 845 165 447 361 931 739 595 233 230 422 789 564 134 722 693  13 895  64\n",
      " 349 530 279 480 434 300 389 559 199 923 111 909 292  12 920 807 126 794\n",
      " 551 720 727  98 442 943   5 642 935 939 685 954 773 124 902 585 568  53\n",
      "  93 803 555 578 163 665 826 332 341 315 810 873 174 726 666 863 537 887\n",
      " 500 112  24  72  50  31 492 622  75 496 562 804   2  76 698 650 838 884\n",
      " 141 345 740 879 459 173 628  66 721 871 131  17 467 294 603 476 894 747\n",
      " 460 506 604 945  65 753 212 231 808 874 382 790 346 833 861 182 289 427\n",
      " 670 303 781 556 499 707 763 606 549 456 834 776 375 899  43 412 106 437\n",
      " 135  60 664  22 421 711 683  90  68 843 194 191 244 732 208 618]\n",
      "Test:  [821 128 100 947 479 209 488 105 784 898 602 719 755 955 797 870 339 925\n",
      " 327 415 638 837 159 371 629 875 436 801  45  48 394 612 913 857 600 706\n",
      " 493 938 862 778 329 308 200 419 636 733 241 672 218 356   1 179  36 321\n",
      " 175 390  32 674 573 257 850 316 800 953 280 117 247 868 924 586  54 433\n",
      " 641 812 635 892 418 358 416 669 407  71 738 363 715 630 268  67 310 952\n",
      "  82 272  27 160 533 458 783 154 526 322  56 663 577  44 259 625 217 681\n",
      " 927 576 842 563  49 841 116 514 295 846 402 806 825  28 384 109 446 408\n",
      " 592 942  19 129 609 518 566 254  59 104 848 284 948 256 370 223 150 768\n",
      " 139  91 171 411 399 423 907 574 543 343 831 786 127 640 189 914 331  70\n",
      " 181 872 202 581 737 656 905 164 430 588 893 301 287 414 239 869 186 929\n",
      " 184 420 510 426 291 365 302 227 235 438 915 552 517 403 949 432 659 519\n",
      " 125 714  34 377  26 746 881 951 463 880 337 944 653 643 507 352  16  37\n",
      " 497 270 667 255 151 891 702 777 940 362 311 387 956 455 594 509 591 619\n",
      " 605 896 318 392 155 347]\n",
      "Train: [821 128 100 947 479 209 488 105 784 898 602 719 755 955 797 870 339 925\n",
      " 327 415 638 837 159 371 629 875 436 801  45  48 394 612 913 857 600 706\n",
      " 493 938 862 778 329 308 200 419 636 733 241 672 218 356   1 179  36 321\n",
      " 175 390  32 674 573 257 850 316 800 953 280 117 247 868 924 586  54 433\n",
      " 641 812 635 892 418 358 416 669 407  71 738 363 715 630 268  67 310 952\n",
      "  82 272  27 160 533 458 783 154 526 322  56 663 577  44 259 625 217 681\n",
      " 927 576 842 563  49 841 116 514 295 846 402 806 825  28 384 109 446 408\n",
      " 592 942  19 129 609 518 566 254  59 104 848 284 948 256 370 223 150 768\n",
      " 139  91 171 411 399 423 907 574 543 343 831 786 127 640 189 914 331  70\n",
      " 181 872 202 581 737 656 905 164 430 588 893 301 287 414 239 869 186 929\n",
      " 184 420 510 426 291 365 302 227 235 438 915 552 517 403 949 432 659 519\n",
      " 125 714  34 377  26 746 881 951 463 880 337 944 653 643 507 352  16  37\n",
      " 497 270 667 255 151 891 702 777 940 362 311 387 956 455 594 509 591 619\n",
      " 605 896 318 392 155 652 198 490  18 299 440 579 772 468 878 774 156 950\n",
      " 536 515 745 473 185  20 379 489 213 908  14 364  35 275 741 941 532 373\n",
      " 265 285  78 491 678 313 508 917 307  39 226 660 761 219 689 799  94 355\n",
      " 814 413  57 114 512 258 785 242 478 749 502 882 718 505 376 571 471 864\n",
      " 542 372 401 717 511 276  88 569 118  61 288 946 484 474 614 122 815 325\n",
      " 197 890 736 791  38 400 704 252 110 453 516 283 188 626  62 610 742 598\n",
      " 798  25 206 441 481 734 424   0 452 314 157 353 410 391 523 770 876 655\n",
      " 535 725  10 866 469 367 849 216 699 731 589  55 229 324 572 222 621 431\n",
      " 312 541 758 350 264 344 928   7 860 328 677 290  73 464 203  58 828 215\n",
      " 221 648 149 220 782 684 195 695 553 240 393 132 472 911 166  92 123 336\n",
      " 617   3 729 548 409 662 658 822 796 451 550 811 138 637 525 933 297 278\n",
      " 855 851 286 809 854 633   8 687 735 788 251 836 381 101 494 930 762 865\n",
      " 228 934 921 901 818 859 795 835 378 404 398 204 767 248 103 712 330 153\n",
      " 485 192 249 710 705  89 503 145 570  83 817 524 701 756 172 904 654 498\n",
      " 676 679 632 787 824 805 754 885 544  29 675 521 234 856 844 830 133 504\n",
      " 238 405 697 282  81 263 143 671 765   9 703 646 858 723  47 886 611 306\n",
      " 596 261 380 120 267  77 647 918 320 477 246 187 374 771 262  87 449 348\n",
      " 137 483 210 253 692 730 816 597 388 527 224 193 354 820 528 673 495 260\n",
      " 845 165 447 361 931 739 595 233 230 422 789 564 134 722 693  13 895  64\n",
      " 349 530 279 480 434 300 389 559 199 923 111 909 292  12 920 807 126 794\n",
      " 551 720 727  98 442 943   5 642 935 939 685 954 773 124 902 585 568  53\n",
      "  93 803 555 578 163 665 826 332 341 315 810 873 174 726 666 863 537 887\n",
      " 500 112  24  72  50  31 492 622  75 496 562 804   2  76 698 650 838 884\n",
      " 141 345 740 879 459 173 628  66 721 871 131  17 467 294 603 476 894 747\n",
      " 460 506 604 945  65 753 212 231 808 874 382 790 346 833 861 182 289 427\n",
      " 670 303 781 556 499 707 763 606 549 456 834 776 375 899  43 412 106 437\n",
      " 135  60 664  22 421 711 683  90  68 843 194 191 244 732 208 347]\n",
      "Test:  [146 889 780 748 293 448  85 682  69 539  23 601 634 243 162 919 936 583\n",
      " 323 792 359 178 608 121 728 342 466 709 827 119 607 369 397  99 326 690\n",
      " 903 115 867 580 140 136 775 779 108 560 691 547 957  97 338 883 147 546\n",
      " 113 661 910 487 475 700 207 319 444 245 177  74 205 167 271 823 713 694\n",
      " 102 744 170 368 593 599  15 590 627 183 802 922 237 274 888 152 461 534\n",
      " 142 623 486  86 906 708 333 357  96 558 196  84 273 236 680 190 760 639\n",
      "  95 631 417 277 428 912 225 900 832 429 309 751 764  46 750 752 520 406\n",
      "  79 395 813 386 250  21  51 554 839 688 443 269 383 385 304 107 266 696\n",
      " 916 932 645 176 180 281 482  33 168 877 396 540 435 620 829 686 644 624\n",
      " 759 513 211 529 567  40 305 462 169  30  11 557 454 545 335 425 847 582\n",
      " 651 130 897 148 561 232 615  42 766 214  52 298 470 501 613 657 793 201\n",
      " 649 769   4 926   6 575 616 465 716 853 334 366 584 144 360  80 531 158\n",
      " 840 668 450 852  63 296 743 757 819 439 445 724 161 937 587  41 317 351\n",
      " 565 538 522 457 340 618]\n",
      "Train: [821 128 100 947 479 209 488 105 784 898 602 719 755 955 797 870 339 925\n",
      " 327 415 638 837 159 371 629 875 436 801  45  48 394 612 913 857 600 706\n",
      " 493 938 862 778 329 308 200 419 636 733 241 672 218 356   1 179  36 321\n",
      " 175 390  32 674 573 257 850 316 800 953 280 117 247 868 924 586  54 433\n",
      " 641 812 635 892 418 358 416 669 407  71 738 363 715 630 268  67 310 952\n",
      "  82 272  27 160 533 458 783 154 526 322  56 663 577  44 259 625 217 681\n",
      " 927 576 842 563  49 841 116 514 295 846 402 806 825  28 384 109 446 408\n",
      " 592 942  19 129 609 518 566 254  59 104 848 284 948 256 370 223 150 768\n",
      " 139  91 171 411 399 423 907 574 543 343 831 786 127 640 189 914 331  70\n",
      " 181 872 202 581 737 656 905 164 430 588 893 301 287 414 239 869 186 929\n",
      " 184 420 510 426 291 365 302 227 235 438 915 552 517 403 949 432 659 519\n",
      " 125 714  34 377  26 746 881 951 463 880 337 944 653 643 507 352  16  37\n",
      " 497 270 667 255 151 891 702 777 940 362 311 387 956 455 594 509 591 619\n",
      " 605 896 318 392 155 146 889 780 748 293 448  85 682  69 539  23 601 634\n",
      " 243 162 919 936 583 323 792 359 178 608 121 728 342 466 709 827 119 607\n",
      " 369 397  99 326 690 903 115 867 580 140 136 775 779 108 560 691 547 957\n",
      "  97 338 883 147 546 113 661 910 487 475 700 207 319 444 245 177  74 205\n",
      " 167 271 823 713 694 102 744 170 368 593 599  15 590 627 183 802 922 237\n",
      " 274 888 152 461 534 142 623 486  86 906 708 333 357  96 558 196  84 273\n",
      " 236 680 190 760 639  95 631 417 277 428 912 225 900 832 429 309 751 764\n",
      "  46 750 752 520 406  79 395 813 386 250  21  51 554 839 688 443 269 383\n",
      " 385 304 107 266 696 916 932 645 176 180 281 482  33 168 877 396 540 435\n",
      " 620 829 686 644 624 759 513 211 529 567  40 305 462 169  30  11 557 454\n",
      " 545 335 425 847 582 651 130 897 148 561 232 615  42 766 214  52 298 470\n",
      " 501 613 657 793 201 649 769   4 926   6 575 616 465 716 853 334 366 584\n",
      " 144 360  80 531 158 840 668 450 852  63 296 743 757 819 439 445 724 161\n",
      " 937 587  41 317 351 565 538 522 457 340 817 524 701 756 172 904 654 498\n",
      " 676 679 632 787 824 805 754 885 544  29 675 521 234 856 844 830 133 504\n",
      " 238 405 697 282  81 263 143 671 765   9 703 646 858 723  47 886 611 306\n",
      " 596 261 380 120 267  77 647 918 320 477 246 187 374 771 262  87 449 348\n",
      " 137 483 210 253 692 730 816 597 388 527 224 193 354 820 528 673 495 260\n",
      " 845 165 447 361 931 739 595 233 230 422 789 564 134 722 693  13 895  64\n",
      " 349 530 279 480 434 300 389 559 199 923 111 909 292  12 920 807 126 794\n",
      " 551 720 727  98 442 943   5 642 935 939 685 954 773 124 902 585 568  53\n",
      "  93 803 555 578 163 665 826 332 341 315 810 873 174 726 666 863 537 887\n",
      " 500 112  24  72  50  31 492 622  75 496 562 804   2  76 698 650 838 884\n",
      " 141 345 740 879 459 173 628  66 721 871 131  17 467 294 603 476 894 747\n",
      " 460 506 604 945  65 753 212 231 808 874 382 790 346 833 861 182 289 427\n",
      " 670 303 781 556 499 707 763 606 549 456 834 776 375 899  43 412 106 437\n",
      " 135  60 664  22 421 711 683  90  68 843 194 191 244 732 208 618 347]\n",
      "Test:  [652 198 490  18 299 440 579 772 468 878 774 156 950 536 515 745 473 185\n",
      "  20 379 489 213 908  14 364  35 275 741 941 532 373 265 285  78 491 678\n",
      " 313 508 917 307  39 226 660 761 219 689 799  94 355 814 413  57 114 512\n",
      " 258 785 242 478 749 502 882 718 505 376 571 471 864 542 372 401 717 511\n",
      " 276  88 569 118  61 288 946 484 474 614 122 815 325 197 890 736 791  38\n",
      " 400 704 252 110 453 516 283 188 626  62 610 742 598 798  25 206 441 481\n",
      " 734 424   0 452 314 157 353 410 391 523 770 876 655 535 725  10 866 469\n",
      " 367 849 216 699 731 589  55 229 324 572 222 621 431 312 541 758 350 264\n",
      " 344 928   7 860 328 677 290  73 464 203  58 828 215 221 648 149 220 782\n",
      " 684 195 695 553 240 393 132 472 911 166  92 123 336 617   3 729 548 409\n",
      " 662 658 822 796 451 550 811 138 637 525 933 297 278 855 851 286 809 854\n",
      " 633   8 687 735 788 251 836 381 101 494 930 762 865 228 934 921 901 818\n",
      " 859 795 835 378 404 398 204 767 248 103 712 330 153 485 192 249 710 705\n",
      "  89 503 145 570  83]\n",
      "Train: [821 128 100 947 479 209 488 105 784 898 602 719 755 955 797 870 339 925\n",
      " 327 415 638 837 159 371 629 875 436 801  45  48 394 612 913 857 600 706\n",
      " 493 938 862 778 329 308 200 419 636 733 241 672 218 356   1 179  36 321\n",
      " 175 390  32 674 573 257 850 316 800 953 280 117 247 868 924 586  54 433\n",
      " 641 812 635 892 418 358 416 669 407  71 738 363 715 630 268  67 310 952\n",
      "  82 272  27 160 533 458 783 154 526 322  56 663 577  44 259 625 217 681\n",
      " 927 576 842 563  49 841 116 514 295 846 402 806 825  28 384 109 446 408\n",
      " 592 942  19 129 609 518 566 254  59 104 848 284 948 256 370 223 150 768\n",
      " 139  91 171 411 399 423 907 574 543 343 831 786 127 640 189 914 331  70\n",
      " 181 872 202 581 737 656 905 164 430 588 893 301 287 414 239 869 186 929\n",
      " 184 420 510 426 291 365 302 227 235 438 915 552 517 403 949 432 659 519\n",
      " 125 714  34 377  26 746 881 951 463 880 337 944 653 643 507 352  16  37\n",
      " 497 270 667 255 151 891 702 777 940 362 311 387 956 455 594 509 591 619\n",
      " 605 896 318 392 155 146 889 780 748 293 448  85 682  69 539  23 601 634\n",
      " 243 162 919 936 583 323 792 359 178 608 121 728 342 466 709 827 119 607\n",
      " 369 397  99 326 690 903 115 867 580 140 136 775 779 108 560 691 547 957\n",
      "  97 338 883 147 546 113 661 910 487 475 700 207 319 444 245 177  74 205\n",
      " 167 271 823 713 694 102 744 170 368 593 599  15 590 627 183 802 922 237\n",
      " 274 888 152 461 534 142 623 486  86 906 708 333 357  96 558 196  84 273\n",
      " 236 680 190 760 639  95 631 417 277 428 912 225 900 832 429 309 751 764\n",
      "  46 750 752 520 406  79 395 813 386 250  21  51 554 839 688 443 269 383\n",
      " 385 304 107 266 696 916 932 645 176 180 281 482  33 168 877 396 540 435\n",
      " 620 829 686 644 624 759 513 211 529 567  40 305 462 169  30  11 557 454\n",
      " 545 335 425 847 582 651 130 897 148 561 232 615  42 766 214  52 298 470\n",
      " 501 613 657 793 201 649 769   4 926   6 575 616 465 716 853 334 366 584\n",
      " 144 360  80 531 158 840 668 450 852  63 296 743 757 819 439 445 724 161\n",
      " 937 587  41 317 351 565 538 522 457 340 652 198 490  18 299 440 579 772\n",
      " 468 878 774 156 950 536 515 745 473 185  20 379 489 213 908  14 364  35\n",
      " 275 741 941 532 373 265 285  78 491 678 313 508 917 307  39 226 660 761\n",
      " 219 689 799  94 355 814 413  57 114 512 258 785 242 478 749 502 882 718\n",
      " 505 376 571 471 864 542 372 401 717 511 276  88 569 118  61 288 946 484\n",
      " 474 614 122 815 325 197 890 736 791  38 400 704 252 110 453 516 283 188\n",
      " 626  62 610 742 598 798  25 206 441 481 734 424   0 452 314 157 353 410\n",
      " 391 523 770 876 655 535 725  10 866 469 367 849 216 699 731 589  55 229\n",
      " 324 572 222 621 431 312 541 758 350 264 344 928   7 860 328 677 290  73\n",
      " 464 203  58 828 215 221 648 149 220 782 684 195 695 553 240 393 132 472\n",
      " 911 166  92 123 336 617   3 729 548 409 662 658 822 796 451 550 811 138\n",
      " 637 525 933 297 278 855 851 286 809 854 633   8 687 735 788 251 836 381\n",
      " 101 494 930 762 865 228 934 921 901 818 859 795 835 378 404 398 204 767\n",
      " 248 103 712 330 153 485 192 249 710 705  89 503 145 570  83 618 347]\n",
      "Test:  [817 524 701 756 172 904 654 498 676 679 632 787 824 805 754 885 544  29\n",
      " 675 521 234 856 844 830 133 504 238 405 697 282  81 263 143 671 765   9\n",
      " 703 646 858 723  47 886 611 306 596 261 380 120 267  77 647 918 320 477\n",
      " 246 187 374 771 262  87 449 348 137 483 210 253 692 730 816 597 388 527\n",
      " 224 193 354 820 528 673 495 260 845 165 447 361 931 739 595 233 230 422\n",
      " 789 564 134 722 693  13 895  64 349 530 279 480 434 300 389 559 199 923\n",
      " 111 909 292  12 920 807 126 794 551 720 727  98 442 943   5 642 935 939\n",
      " 685 954 773 124 902 585 568  53  93 803 555 578 163 665 826 332 341 315\n",
      " 810 873 174 726 666 863 537 887 500 112  24  72  50  31 492 622  75 496\n",
      " 562 804   2  76 698 650 838 884 141 345 740 879 459 173 628  66 721 871\n",
      " 131  17 467 294 603 476 894 747 460 506 604 945  65 753 212 231 808 874\n",
      " 382 790 346 833 861 182 289 427 670 303 781 556 499 707 763 606 549 456\n",
      " 834 776 375 899  43 412 106 437 135  60 664  22 421 711 683  90  68 843\n",
      " 194 191 244 732 208]\n"
     ]
    }
   ],
   "source": [
    "validacion_simple = ValidacionSimple(0.75)\n",
    "validacion_simple.creaParticiones(dataset)\n",
    "v_cruzada = ValidacionCruzada(4)\n",
    "v_cruzada.creaParticiones(dataset)\n",
    "\n",
    "print(\"VALIDACION SIMPLE\",validacion_simple.particiones[0])\n",
    "\n",
    "print(\"VALIDACION CRUZADA\")\n",
    "for particion in v_cruzada.particiones:\n",
    "    print(particion)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El error del Clasificador NaiveBayes  para Validacion Simple es: 1.0\n",
      "El error del Clasificador NaiveBayes para Validacion Cruzada es: 1.0\n"
     ]
    }
   ],
   "source": [
    "clasificador = ClasificadorNaiveBayes(True)\n",
    "#clasificador.entrenamiento(dataset, validacion_simple.particiones[0].indicesTrain)\n",
    "#clasificador.clasifica(dataset, validacion_simple.particiones[0].indicesTest)\n",
    "error = clasificador.validacion(validacion_simple,dataset,clasificador)\n",
    "error1 = clasificador.validacion(v_cruzada,dataset,clasificador)\n",
    "print(\"El error del Clasificador NaiveBayes  para Validacion Simple es: \"+str(error))\n",
    "print(\"El error del Clasificador NaiveBayes para Validacion Cruzada es: \"+str(error1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A1', 'A2', 'A3', 'A4', 'A5', 'A6', 'A7', 'A8', 'A9', 'A10', 'A11', 'A12', 'A13', 'A14', 'A15', 'A16', 'A17', 'A18', 'A19', 'A20', 'Class']\n",
      "[{'A11': 0, 'A12': 1, 'A13': 2, 'A14': 3}, {}, {'A30': 0, 'A31': 1, 'A32': 2, 'A33': 3, 'A34': 4}, {'A40': 0, 'A41': 1, 'A410': 2, 'A42': 3, 'A43': 4, 'A44': 5, 'A45': 6, 'A46': 7, 'A48': 8, 'A49': 9}, {}, {'A61': 0, 'A62': 1, 'A63': 2, 'A64': 3, 'A65': 4}, {'A71': 0, 'A72': 1, 'A73': 2, 'A74': 3, 'A75': 4}, {}, {'A91': 0, 'A92': 1, 'A93': 2, 'A94': 3}, {'A101': 0, 'A102': 1, 'A103': 2}, {}, {'A121': 0, 'A122': 1, 'A123': 2, 'A124': 3}, {}, {'A141': 0, 'A142': 1, 'A143': 2}, {'A151': 0, 'A152': 1, 'A153': 2}, {}, {'A171': 0, 'A172': 1, 'A173': 2, 'A174': 3}, {}, {'A191': 0, 'A192': 1}, {'A201': 0, 'A202': 1}, {'1': 0, '2': 1}]\n",
      "[[ 0.  6.  4. ...  1.  0.  0.]\n",
      " [ 1. 48.  2. ...  0.  0.  1.]\n",
      " [ 3. 12.  4. ...  0.  0.  0.]\n",
      " ...\n",
      " [ 3. 12.  2. ...  0.  0.  0.]\n",
      " [ 0. 45.  2. ...  1.  0.  1.]\n",
      " [ 1. 45.  4. ...  0.  0.  0.]]\n",
      "[True, False, True, True, False, True, True, False, True, True, False, True, False, True, True, False, True, False, True, True, True]\n"
     ]
    }
   ],
   "source": [
    "dataset = Datos('german.data')\n",
    "print(dataset.datos)\n",
    "print(dataset.nominalAtributos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VALIDACION SIMPLE Train: [556 424 113 366 899 824 920 727 657 829 272 975 595 776 454  84  97 624\n",
      " 941 904  16  85 372 623 983 560 418 482 958 537 340 652 495  23  41 307\n",
      " 441 298 622 356 122 872 338 955 485 720 369 875 804 878 788 959 426 404\n",
      " 643 195 979 827 382 936 263 526 479 896 668 685 371 970  42 114 501 852\n",
      " 473 193 554 583 751 391  39 508  37 187 339 331 384 393  55  58 124 990\n",
      " 448 478 680 741 999  24 949 591 779 512 582 841 188 565 268 907 269 662\n",
      " 996 164 316 414  53 573 480 948 768 880  68 194 810 594 690 212 992 605\n",
      " 173 604 229 249 763 474 795 354 295 864 155 695 957 815 288 535 980 927\n",
      " 823 488  48 782 457 400 754 234 412 405 974 184 575  90  94 147 202 529\n",
      " 325 532 171 808 401 506 489  45 491 497 586 944 675 218 891 830 149 879\n",
      " 601 800 109 310 805 237 230 490 525 165 378  47 647 765 505 898 281 803\n",
      " 545 469 398 425 598 527 886 928 172 283 129 245 335 259 395  88 678 730\n",
      "  86 704 661 274 687 241 219 121 845 504 150 199 261 503 689 885 183 244\n",
      "  66 243 481 518 458 464 576 761 910 952 159 577  12 725 645 118 452 674\n",
      " 862 747 993  21 710  27 973 309 584 278 729 634 328 352 161 216 297 227\n",
      " 521 608  63 392 755 750 334 785 180 293 179 625 642 646 255 797 888  43\n",
      " 894 691 592 673 189 411 931 796 840  17  99 361  32 737 251 348 256 394\n",
      " 692 900 599 995 665 206 326 550 370 825 534 846 522 344 265 619 686 235\n",
      " 264 621  18 154 191   1 286 775 208 389 126 820 433 574 893 718  34 667\n",
      " 769 658 981 210 417 871 321 921 971  31 714 876 260 744 628 925 819 538\n",
      " 982 381 543  75  65 943 934 890 564 421 460 185 451 420 100 985 831 349\n",
      " 498 715 735 102   5 416  76 555 786  46 438  93 881 587 177 447 783 915\n",
      " 207 991 520  33 453 868 551  83 780 476 792  59 127  71 299 146 422  19\n",
      " 542 280 327 972 978 205 343 660  13 322 162 148 304 429 320 445 123 513\n",
      "  54 956   3 540 640 225  89 536 449 314 300 922 799 867 651 832 302 817\n",
      " 238 806 802 719 116 175 105 713 572 728 859 969 672 468  29 125 330 961\n",
      " 262 726 450 289 346 702 688 568 383 140 459 134 632 613 143   8 873 110\n",
      " 313 215 517 636  26 739 101 950 962 231  70 157 694  14 167 380 266 315\n",
      " 839 998  20 930 752  36 282 929 618 627 903 347 858 836 918 767 333 656\n",
      " 848 826 850 917 905 214 233 865 211 654 547 557 287   9 528  92 367 128\n",
      " 388 296 631  72 471   4  25 432 360 745 463 317 912 732   7 717 144 376\n",
      " 549 842 676 461 933 664 908 701 659 192 757 821 477 644 467 341  82 758\n",
      " 724 563 226 569 854 597 106 633 176 585 828 837 909 377 612 379 614 516\n",
      " 967 224 406 200 267 511  35 781 611 916 197   2 319 650 847 801 139 593\n",
      " 753 682 603 350 437 994 131 363  67 273 439 707 455 519 740 104 254 774\n",
      " 813 787 653 443 523 373 699 552 524 965 332 935  91 607 120 486 156 708\n",
      " 462 887 743 178 358 940 914 777 884 816 515 203 964 271 132 365 553 415\n",
      " 812 571 470 773 860  15 428 756 133 791 509 655 849 541 609 408 324 684\n",
      " 240 638 626 617 861 703 276 835 138 679 649 874 305 209 857 942 984 811\n",
      " 530 834 883 158 487 499 375 579 311  78 403 510  64 410 635 166 923 151\n",
      " 610 863 329 851 213 581 444 919 409  96 764 246  52 953 766  77 170 639\n",
      " 997  57 174 153 252 798 760 892 242 336 539 937]\n",
      "Test:  [629 588 838 770 723 484 822 492 951 419  80 567 946  98 924 789 152  51\n",
      " 562 396 136 232 722 683 359 762 963 902 814 111 696 257 431 285 407 531\n",
      " 578 386 374 279 145 897 472 793 306 456 390  79 590 794 301 670 362 709\n",
      " 711 290 436 544 901 681 869 190 986 221 856 548 435 877 533 807 434 706\n",
      " 277 423 693 630 117 734 700  62  60 294 945 606 475  69 938 648 500 168\n",
      " 427 196 430 671 778 580 291 966  22 413 130 446 911 677 115 103 882 275\n",
      " 502 895 866  44 987   6 141 112 107 669  81 843 759 698 186 663 926 559\n",
      " 721 855 705 960 442 250 303 602 712 236 220 228 137 507 697 248 771 247\n",
      "  10 853 558 733 483 323 637 387 736 201  49 954 169  30 790 977  61 641\n",
      " 466 182 493 308 397 615 181 947 239 345 253  74 784 353  56 976 589 198\n",
      " 357 142 222 494 932 844 258 312 616 746 913 738 270 160 399 108 818 716\n",
      " 870 666 337 292 318 561 465 217  50 939 772 284  28 749 833  73   0 742\n",
      " 989 570  87 351 402 496 440 514 163 223 368 355  38 748 596 731 889  11\n",
      " 342 385 809 119 906 546  95 204 600 968 135 364  40 988 620 566]\n",
      "VALIDACION CRUZADA\n",
      "Train: [833 116 340 221 994  50 777  54 200 797 135 308 479 829 201 850 808 803\n",
      " 522 556 600 399 269 890 409 882 503 215 188 780 974 881 964 168 877  48\n",
      " 506 965  38 581 198 172 676 566  22  40 897 299   6 419 309 289 207 136\n",
      " 723 140   2 124 757 687 811 962 369 775 467 513 517 454 932 404 425 694\n",
      " 484 920 641  24 819 418 335 181 837 989 112 546 531  90 794 247 635 574\n",
      " 487 971 282 633 603  79 940 991 888 667  97 436 501  49 883 682 709 253\n",
      " 514  81 312 392 332 669 987 938  29 210 186 235 545 853 121 155 764 241\n",
      "  20 610 598 311 744  25 182 651 526 981 321 410 445  31  44 783 106  66\n",
      " 588 294 621 557  95 697 438 798 638 233 502 874  30 180 645 417 346 771\n",
      " 103 209 716 792 441 475 594  12 926 245 553 529  68  47 512 939 631 122\n",
      " 961   4 543 128 590 206 302 387 359 515 705 461 160 444 693 119 715 593\n",
      " 378 585 173 466 508 451 967 900 365 691 132  87 123 183 748 169 708 117\n",
      " 326 146 821 944 628 789 562 534 617  63 732 648 727 554 992 272 714 832\n",
      " 358 591 801 267 446 923 268 490 153 655 876 519 765 258 154 465 620 681\n",
      " 751 165 398 120 951 864 558 596 901 375 320  46 396 458 303 533 453 187\n",
      " 371 830 300 986  35 661 379 959 202 107 297 150  23 527 909 455 403 381\n",
      " 822 430 243 196  67 688 721 931 277 525   1  53 942 935 763  17 240  60\n",
      " 799 949 283 919 348 736 360 463 271 536 439 164 827 211 569  41 480 718\n",
      " 197 725 141 969 114 457 265 802 791  28 471 474 353 166 459 421 477 464\n",
      " 644  65 374 306 834 963 933 913 483 184 152 717  37 422 194 257 278 510\n",
      " 205 846 341 315 325 493  74 179 779 384 232 908  88 385 912 816 785 968\n",
      " 167 872  14 195 735 793 452 109 314 960 818   0 786 579 386 174 862 685\n",
      " 823  57 118 947 860 281 902 129 228 623 226 854 328 565 580 213 564  59\n",
      " 486 542 354 887 301 507 629 389  16 115 624  33 343 351 322  85 810 704\n",
      " 576 337 550 219 595 185 907 429 766 587 844 637 904 488 162 841 143  58\n",
      " 563 104 275 504  43 906 943 254 790 807 323 236 642 953 719 248 190 496\n",
      " 521 440 528 495 978 108 431 782 684 845 886 817 891  92 357 126 849 852\n",
      " 411 678 279 800   9 227 954 741  39 754 673 975 147 560 632 958 139 679\n",
      " 142 604 319 192 296 237 993 280 689 349 997 767 607 432 795 584 400 390\n",
      " 813 449 643 683 653 656 996  42 601 434 476 573 229 433 858  84 539 290\n",
      " 686 424 614 910 485 734 937 137 263 842 893 470 760 511 773 851 559 189\n",
      " 578 497 805 130 362 288 231 193 572 917  76 342 295 972 157 377 316 538\n",
      " 979 284 318 394 339 242 224 784 778 505  21 437 611 273 156 177 769 899\n",
      " 712  72 261 761 941 110 203 285 875 551  86 699  15 313 355 608 393 814\n",
      " 460 619 701 660 344 921 649 293 363 547 548 654 983 575  75 859 976 520\n",
      " 925 730 101 889 324 870 857 225 927 382  73 133 570 627 367 540 722  99\n",
      " 413 214 212 670 310 415 426 911  70 222 873 334 636 469 999 397 863 820\n",
      " 498 286 952 707  77 266 861 255 995 176 838 747 640 423 930 674 634 696\n",
      " 848 125 916 703 828  45 530 880 781 327 350 924 331 606 665 292 729 532\n",
      " 745 549 720 364 518 843 905 171  52 774 435 680 957 329 259 898 338 985\n",
      " 145 405 256 746 998 672 804  82 866 552 406 298 728 835 586 879 840 915\n",
      " 666 847 304 127 776 589 948  62 826 368 970 758]\n",
      "Test:  [482 509 630 356 677 408 984 420  96  32  71  11 276 806 336 407 492  83\n",
      " 768  91 914 650 208  27 895 287 878 249  94 239 347 695 170 966 138 260\n",
      " 456 380 973 710 896 478 622 615 885 524 856 352 702 251 252 740 262  36\n",
      " 743   7 537 839 567 220 223 934 738 597 675 762 652   8 373 868 582  26\n",
      " 698 178  78 658 894 442 815 395 592  34 330 401 825 204 733 711   3 871\n",
      " 274 749 731 609 602 657 345 836 750 809 788 787 414 366 427 955 647 489\n",
      " 113 903 234 605 468 472 946 724 713 216 317 990 159 361 759 706 664 175\n",
      "   5 494 383 739 217 737 626 616  18 370 448 982 865 618 246 568 700 131\n",
      " 892 770 668 333 305 671  98 980 163 149 577  64 250 307 583 918 218 372\n",
      " 884 523 929  13 443 516 659  89 755 613 662 945 199 191 450 599 388 752\n",
      " 402  69  51 447 535 612 544 690  80 855 922 772 491 161 928 416 462 144\n",
      " 244 812 796 753 726 270  61 264 238 869 102 105 151  56 134  55 111 376\n",
      " 824 428 646 756 541 663 692 936 500 158 571 499 391 291 100 473 742  19\n",
      "  10 988 867 639 481 230 950 956 555 625 561 148 412  93 831 977]\n",
      "Train: [482 509 630 356 677 408 984 420  96  32  71  11 276 806 336 407 492  83\n",
      " 768  91 914 650 208  27 895 287 878 249  94 239 347 695 170 966 138 260\n",
      " 456 380 973 710 896 478 622 615 885 524 856 352 702 251 252 740 262  36\n",
      " 743   7 537 839 567 220 223 934 738 597 675 762 652   8 373 868 582  26\n",
      " 698 178  78 658 894 442 815 395 592  34 330 401 825 204 733 711   3 871\n",
      " 274 749 731 609 602 657 345 836 750 809 788 787 414 366 427 955 647 489\n",
      " 113 903 234 605 468 472 946 724 713 216 317 990 159 361 759 706 664 175\n",
      "   5 494 383 739 217 737 626 616  18 370 448 982 865 618 246 568 700 131\n",
      " 892 770 668 333 305 671  98 980 163 149 577  64 250 307 583 918 218 372\n",
      " 884 523 929  13 443 516 659  89 755 613 662 945 199 191 450 599 388 752\n",
      " 402  69  51 447 535 612 544 690  80 855 922 772 491 161 928 416 462 144\n",
      " 244 812 796 753 726 270  61 264 238 869 102 105 151  56 134  55 111 376\n",
      " 824 428 646 756 541 663 692 936 500 158 571 499 391 291 100 473 742  19\n",
      "  10 988 867 639 481 230 950 956 555 625 561 148 412  93 831 977 620 681\n",
      " 751 165 398 120 951 864 558 596 901 375 320  46 396 458 303 533 453 187\n",
      " 371 830 300 986  35 661 379 959 202 107 297 150  23 527 909 455 403 381\n",
      " 822 430 243 196  67 688 721 931 277 525   1  53 942 935 763  17 240  60\n",
      " 799 949 283 919 348 736 360 463 271 536 439 164 827 211 569  41 480 718\n",
      " 197 725 141 969 114 457 265 802 791  28 471 474 353 166 459 421 477 464\n",
      " 644  65 374 306 834 963 933 913 483 184 152 717  37 422 194 257 278 510\n",
      " 205 846 341 315 325 493  74 179 779 384 232 908  88 385 912 816 785 968\n",
      " 167 872  14 195 735 793 452 109 314 960 818   0 786 579 386 174 862 685\n",
      " 823  57 118 947 860 281 902 129 228 623 226 854 328 565 580 213 564  59\n",
      " 486 542 354 887 301 507 629 389  16 115 624  33 343 351 322  85 810 704\n",
      " 576 337 550 219 595 185 907 429 766 587 844 637 904 488 162 841 143  58\n",
      " 563 104 275 504  43 906 943 254 790 807 323 236 642 953 719 248 190 496\n",
      " 521 440 528 495 978 108 431 782 684 845 886 817 891  92 357 126 849 852\n",
      " 411 678 279 800   9 227 954 741  39 754 673 975 147 560 632 958 139 679\n",
      " 142 604 319 192 296 237 993 280 689 349 997 767 607 432 795 584 400 390\n",
      " 813 449 643 683 653 656 996  42 601 434 476 573 229 433 858  84 539 290\n",
      " 686 424 614 910 485 734 937 137 263 842 893 470 760 511 773 851 559 189\n",
      " 578 497 805 130 362 288 231 193 572 917  76 342 295 972 157 377 316 538\n",
      " 979 284 318 394 339 242 224 784 778 505  21 437 611 273 156 177 769 899\n",
      " 712  72 261 761 941 110 203 285 875 551  86 699  15 313 355 608 393 814\n",
      " 460 619 701 660 344 921 649 293 363 547 548 654 983 575  75 859 976 520\n",
      " 925 730 101 889 324 870 857 225 927 382  73 133 570 627 367 540 722  99\n",
      " 413 214 212 670 310 415 426 911  70 222 873 334 636 469 999 397 863 820\n",
      " 498 286 952 707  77 266 861 255 995 176 838 747 640 423 930 674 634 696\n",
      " 848 125 916 703 828  45 530 880 781 327 350 924 331 606 665 292 729 532\n",
      " 745 549 720 364 518 843 905 171  52 774 435 680 957 329 259 898 338 985\n",
      " 145 405 256 746 998 672 804  82 866 552 406 298 728 835 586 879 840 915\n",
      " 666 847 304 127 776 589 948  62 826 368 970 758]\n",
      "Test:  [833 116 340 221 994  50 777  54 200 797 135 308 479 829 201 850 808 803\n",
      " 522 556 600 399 269 890 409 882 503 215 188 780 974 881 964 168 877  48\n",
      " 506 965  38 581 198 172 676 566  22  40 897 299   6 419 309 289 207 136\n",
      " 723 140   2 124 757 687 811 962 369 775 467 513 517 454 932 404 425 694\n",
      " 484 920 641  24 819 418 335 181 837 989 112 546 531  90 794 247 635 574\n",
      " 487 971 282 633 603  79 940 991 888 667  97 436 501  49 883 682 709 253\n",
      " 514  81 312 392 332 669 987 938  29 210 186 235 545 853 121 155 764 241\n",
      "  20 610 598 311 744  25 182 651 526 981 321 410 445  31  44 783 106  66\n",
      " 588 294 621 557  95 697 438 798 638 233 502 874  30 180 645 417 346 771\n",
      " 103 209 716 792 441 475 594  12 926 245 553 529  68  47 512 939 631 122\n",
      " 961   4 543 128 590 206 302 387 359 515 705 461 160 444 693 119 715 593\n",
      " 378 585 173 466 508 451 967 900 365 691 132  87 123 183 748 169 708 117\n",
      " 326 146 821 944 628 789 562 534 617  63 732 648 727 554 992 272 714 832\n",
      " 358 591 801 267 446 923 268 490 153 655 876 519 765 258 154 465]\n",
      "Train: [482 509 630 356 677 408 984 420  96  32  71  11 276 806 336 407 492  83\n",
      " 768  91 914 650 208  27 895 287 878 249  94 239 347 695 170 966 138 260\n",
      " 456 380 973 710 896 478 622 615 885 524 856 352 702 251 252 740 262  36\n",
      " 743   7 537 839 567 220 223 934 738 597 675 762 652   8 373 868 582  26\n",
      " 698 178  78 658 894 442 815 395 592  34 330 401 825 204 733 711   3 871\n",
      " 274 749 731 609 602 657 345 836 750 809 788 787 414 366 427 955 647 489\n",
      " 113 903 234 605 468 472 946 724 713 216 317 990 159 361 759 706 664 175\n",
      "   5 494 383 739 217 737 626 616  18 370 448 982 865 618 246 568 700 131\n",
      " 892 770 668 333 305 671  98 980 163 149 577  64 250 307 583 918 218 372\n",
      " 884 523 929  13 443 516 659  89 755 613 662 945 199 191 450 599 388 752\n",
      " 402  69  51 447 535 612 544 690  80 855 922 772 491 161 928 416 462 144\n",
      " 244 812 796 753 726 270  61 264 238 869 102 105 151  56 134  55 111 376\n",
      " 824 428 646 756 541 663 692 936 500 158 571 499 391 291 100 473 742  19\n",
      "  10 988 867 639 481 230 950 956 555 625 561 148 412  93 831 977 833 116\n",
      " 340 221 994  50 777  54 200 797 135 308 479 829 201 850 808 803 522 556\n",
      " 600 399 269 890 409 882 503 215 188 780 974 881 964 168 877  48 506 965\n",
      "  38 581 198 172 676 566  22  40 897 299   6 419 309 289 207 136 723 140\n",
      "   2 124 757 687 811 962 369 775 467 513 517 454 932 404 425 694 484 920\n",
      " 641  24 819 418 335 181 837 989 112 546 531  90 794 247 635 574 487 971\n",
      " 282 633 603  79 940 991 888 667  97 436 501  49 883 682 709 253 514  81\n",
      " 312 392 332 669 987 938  29 210 186 235 545 853 121 155 764 241  20 610\n",
      " 598 311 744  25 182 651 526 981 321 410 445  31  44 783 106  66 588 294\n",
      " 621 557  95 697 438 798 638 233 502 874  30 180 645 417 346 771 103 209\n",
      " 716 792 441 475 594  12 926 245 553 529  68  47 512 939 631 122 961   4\n",
      " 543 128 590 206 302 387 359 515 705 461 160 444 693 119 715 593 378 585\n",
      " 173 466 508 451 967 900 365 691 132  87 123 183 748 169 708 117 326 146\n",
      " 821 944 628 789 562 534 617  63 732 648 727 554 992 272 714 832 358 591\n",
      " 801 267 446 923 268 490 153 655 876 519 765 258 154 465 632 958 139 679\n",
      " 142 604 319 192 296 237 993 280 689 349 997 767 607 432 795 584 400 390\n",
      " 813 449 643 683 653 656 996  42 601 434 476 573 229 433 858  84 539 290\n",
      " 686 424 614 910 485 734 937 137 263 842 893 470 760 511 773 851 559 189\n",
      " 578 497 805 130 362 288 231 193 572 917  76 342 295 972 157 377 316 538\n",
      " 979 284 318 394 339 242 224 784 778 505  21 437 611 273 156 177 769 899\n",
      " 712  72 261 761 941 110 203 285 875 551  86 699  15 313 355 608 393 814\n",
      " 460 619 701 660 344 921 649 293 363 547 548 654 983 575  75 859 976 520\n",
      " 925 730 101 889 324 870 857 225 927 382  73 133 570 627 367 540 722  99\n",
      " 413 214 212 670 310 415 426 911  70 222 873 334 636 469 999 397 863 820\n",
      " 498 286 952 707  77 266 861 255 995 176 838 747 640 423 930 674 634 696\n",
      " 848 125 916 703 828  45 530 880 781 327 350 924 331 606 665 292 729 532\n",
      " 745 549 720 364 518 843 905 171  52 774 435 680 957 329 259 898 338 985\n",
      " 145 405 256 746 998 672 804  82 866 552 406 298 728 835 586 879 840 915\n",
      " 666 847 304 127 776 589 948  62 826 368 970 758]\n",
      "Test:  [620 681 751 165 398 120 951 864 558 596 901 375 320  46 396 458 303 533\n",
      " 453 187 371 830 300 986  35 661 379 959 202 107 297 150  23 527 909 455\n",
      " 403 381 822 430 243 196  67 688 721 931 277 525   1  53 942 935 763  17\n",
      " 240  60 799 949 283 919 348 736 360 463 271 536 439 164 827 211 569  41\n",
      " 480 718 197 725 141 969 114 457 265 802 791  28 471 474 353 166 459 421\n",
      " 477 464 644  65 374 306 834 963 933 913 483 184 152 717  37 422 194 257\n",
      " 278 510 205 846 341 315 325 493  74 179 779 384 232 908  88 385 912 816\n",
      " 785 968 167 872  14 195 735 793 452 109 314 960 818   0 786 579 386 174\n",
      " 862 685 823  57 118 947 860 281 902 129 228 623 226 854 328 565 580 213\n",
      " 564  59 486 542 354 887 301 507 629 389  16 115 624  33 343 351 322  85\n",
      " 810 704 576 337 550 219 595 185 907 429 766 587 844 637 904 488 162 841\n",
      " 143  58 563 104 275 504  43 906 943 254 790 807 323 236 642 953 719 248\n",
      " 190 496 521 440 528 495 978 108 431 782 684 845 886 817 891  92 357 126\n",
      " 849 852 411 678 279 800   9 227 954 741  39 754 673 975 147 560]\n",
      "Train: [482 509 630 356 677 408 984 420  96  32  71  11 276 806 336 407 492  83\n",
      " 768  91 914 650 208  27 895 287 878 249  94 239 347 695 170 966 138 260\n",
      " 456 380 973 710 896 478 622 615 885 524 856 352 702 251 252 740 262  36\n",
      " 743   7 537 839 567 220 223 934 738 597 675 762 652   8 373 868 582  26\n",
      " 698 178  78 658 894 442 815 395 592  34 330 401 825 204 733 711   3 871\n",
      " 274 749 731 609 602 657 345 836 750 809 788 787 414 366 427 955 647 489\n",
      " 113 903 234 605 468 472 946 724 713 216 317 990 159 361 759 706 664 175\n",
      "   5 494 383 739 217 737 626 616  18 370 448 982 865 618 246 568 700 131\n",
      " 892 770 668 333 305 671  98 980 163 149 577  64 250 307 583 918 218 372\n",
      " 884 523 929  13 443 516 659  89 755 613 662 945 199 191 450 599 388 752\n",
      " 402  69  51 447 535 612 544 690  80 855 922 772 491 161 928 416 462 144\n",
      " 244 812 796 753 726 270  61 264 238 869 102 105 151  56 134  55 111 376\n",
      " 824 428 646 756 541 663 692 936 500 158 571 499 391 291 100 473 742  19\n",
      "  10 988 867 639 481 230 950 956 555 625 561 148 412  93 831 977 833 116\n",
      " 340 221 994  50 777  54 200 797 135 308 479 829 201 850 808 803 522 556\n",
      " 600 399 269 890 409 882 503 215 188 780 974 881 964 168 877  48 506 965\n",
      "  38 581 198 172 676 566  22  40 897 299   6 419 309 289 207 136 723 140\n",
      "   2 124 757 687 811 962 369 775 467 513 517 454 932 404 425 694 484 920\n",
      " 641  24 819 418 335 181 837 989 112 546 531  90 794 247 635 574 487 971\n",
      " 282 633 603  79 940 991 888 667  97 436 501  49 883 682 709 253 514  81\n",
      " 312 392 332 669 987 938  29 210 186 235 545 853 121 155 764 241  20 610\n",
      " 598 311 744  25 182 651 526 981 321 410 445  31  44 783 106  66 588 294\n",
      " 621 557  95 697 438 798 638 233 502 874  30 180 645 417 346 771 103 209\n",
      " 716 792 441 475 594  12 926 245 553 529  68  47 512 939 631 122 961   4\n",
      " 543 128 590 206 302 387 359 515 705 461 160 444 693 119 715 593 378 585\n",
      " 173 466 508 451 967 900 365 691 132  87 123 183 748 169 708 117 326 146\n",
      " 821 944 628 789 562 534 617  63 732 648 727 554 992 272 714 832 358 591\n",
      " 801 267 446 923 268 490 153 655 876 519 765 258 154 465 620 681 751 165\n",
      " 398 120 951 864 558 596 901 375 320  46 396 458 303 533 453 187 371 830\n",
      " 300 986  35 661 379 959 202 107 297 150  23 527 909 455 403 381 822 430\n",
      " 243 196  67 688 721 931 277 525   1  53 942 935 763  17 240  60 799 949\n",
      " 283 919 348 736 360 463 271 536 439 164 827 211 569  41 480 718 197 725\n",
      " 141 969 114 457 265 802 791  28 471 474 353 166 459 421 477 464 644  65\n",
      " 374 306 834 963 933 913 483 184 152 717  37 422 194 257 278 510 205 846\n",
      " 341 315 325 493  74 179 779 384 232 908  88 385 912 816 785 968 167 872\n",
      "  14 195 735 793 452 109 314 960 818   0 786 579 386 174 862 685 823  57\n",
      " 118 947 860 281 902 129 228 623 226 854 328 565 580 213 564  59 486 542\n",
      " 354 887 301 507 629 389  16 115 624  33 343 351 322  85 810 704 576 337\n",
      " 550 219 595 185 907 429 766 587 844 637 904 488 162 841 143  58 563 104\n",
      " 275 504  43 906 943 254 790 807 323 236 642 953 719 248 190 496 521 440\n",
      " 528 495 978 108 431 782 684 845 886 817 891  92 357 126 849 852 411 678\n",
      " 279 800   9 227 954 741  39 754 673 975 147 560]\n",
      "Test:  [632 958 139 679 142 604 319 192 296 237 993 280 689 349 997 767 607 432\n",
      " 795 584 400 390 813 449 643 683 653 656 996  42 601 434 476 573 229 433\n",
      " 858  84 539 290 686 424 614 910 485 734 937 137 263 842 893 470 760 511\n",
      " 773 851 559 189 578 497 805 130 362 288 231 193 572 917  76 342 295 972\n",
      " 157 377 316 538 979 284 318 394 339 242 224 784 778 505  21 437 611 273\n",
      " 156 177 769 899 712  72 261 761 941 110 203 285 875 551  86 699  15 313\n",
      " 355 608 393 814 460 619 701 660 344 921 649 293 363 547 548 654 983 575\n",
      "  75 859 976 520 925 730 101 889 324 870 857 225 927 382  73 133 570 627\n",
      " 367 540 722  99 413 214 212 670 310 415 426 911  70 222 873 334 636 469\n",
      " 999 397 863 820 498 286 952 707  77 266 861 255 995 176 838 747 640 423\n",
      " 930 674 634 696 848 125 916 703 828  45 530 880 781 327 350 924 331 606\n",
      " 665 292 729 532 745 549 720 364 518 843 905 171  52 774 435 680 957 329\n",
      " 259 898 338 985 145 405 256 746 998 672 804  82 866 552 406 298 728 835\n",
      " 586 879 840 915 666 847 304 127 776 589 948  62 826 368 970 758]\n"
     ]
    }
   ],
   "source": [
    "validacion_simple = ValidacionSimple(0.75)\n",
    "validacion_simple.creaParticiones(dataset)\n",
    "v_cruzada = ValidacionCruzada(4)\n",
    "v_cruzada.creaParticiones(dataset)\n",
    "\n",
    "print(\"VALIDACION SIMPLE\",validacion_simple.particiones[0])\n",
    "\n",
    "print(\"VALIDACION CRUZADA\")\n",
    "for particion in v_cruzada.particiones:\n",
    "    print(particion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 9 is out of bounds for axis 0 with size 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-180-177411b8a88b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#clasificador.entrenamiento(dataset, validacion_simple.particiones[0].indicesTrain)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#clasificador.clasifica(dataset, validacion_simple.particiones[0].indicesTest)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0merror\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclasificador\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidacion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidacion_simple\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mclasificador\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0merror1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclasificador\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidacion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv_cruzada\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mclasificador\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"El error del Clasificador NaiveBayes  para Validacion Simple es: \"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-138-fc3769d437ab>\u001b[0m in \u001b[0;36mvalidacion\u001b[0;34m(self, particionado, dataset, clasificador, seed)\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparticionado\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparticiones\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mclasificador\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mentrenamiento\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparticionado\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparticiones\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindicesTrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m             \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclasificador\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasifica\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mparticionado\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparticiones\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindicesTest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextraeDatos\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparticionado\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparticiones\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindicesTest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-172-757431d12f5a>\u001b[0m in \u001b[0;36mclasifica\u001b[0;34m(self, dataset, datosTest)\u001b[0m\n\u001b[1;32m     87\u001b[0m                     \u001b[0;32mfor\u001b[0m \u001b[0matributo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mposteriori\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m                         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0matributo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m                         \u001b[0maux\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mposteriori\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0matributo\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdato\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0matributo\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mclase\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m                     \u001b[0;31m#Aqui obtenemos la siguiente probabilidad P(D|H)*P(H)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 9 is out of bounds for axis 0 with size 2"
     ]
    }
   ],
   "source": [
    "clasificador = ClasificadorNaiveBayes(True)\n",
    "#clasificador.entrenamiento(dataset, validacion_simple.particiones[0].indicesTrain)\n",
    "#clasificador.clasifica(dataset, validacion_simple.particiones[0].indicesTest)\n",
    "error = clasificador.validacion(validacion_simple,dataset,clasificador)\n",
    "error1 = clasificador.validacion(v_cruzada,dataset,clasificador)\n",
    "print(\"El error del Clasificador NaiveBayes  para Validacion Simple es: \"+str(error))\n",
    "print(\"El error del Clasificador NaiveBayes para Validacion Cruzada es: \"+str(error1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aalcala/anaconda3/lib/python3.7/site-packages/sklearn/preprocessing/_encoders.py:415: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/aalcala/anaconda3/lib/python3.7/site-packages/sklearn/preprocessing/_encoders.py:451: DeprecationWarning: The 'categorical_features' keyword is deprecated in version 0.20 and will be removed in 0.22. You can use the ColumnTransformer instead.\n",
      "  \"use the ColumnTransformer instead.\", DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicciones utilizando validacion simple sin correcion de Laplace: [0. 0. 0. 1. 0. 0. 1. 1. 1. 1. 0. 1. 1. 0. 0. 0. 1. 0. 0. 1. 0. 1. 1. 0.\n",
      " 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1. 1. 0. 1. 0. 1. 1.\n",
      " 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 0. 1. 1.\n",
      " 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 1. 0. 1. 1. 0. 1. 0. 1. 0. 0. 0. 0. 1.\n",
      " 0. 0. 0. 1. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 1. 0. 1. 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0.\n",
      " 0. 0. 1. 1. 0. 1. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 1. 0. 0. 1. 1. 1. 1. 0.\n",
      " 0. 0. 1. 0. 1. 0. 1. 0.]\n",
      "Clases reales de la particion de text [0. 0. 0. 1. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 1. 0. 1. 0. 1. 1. 1. 1. 0. 0. 0. 1. 1.\n",
      " 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 1.\n",
      " 1. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 1.\n",
      " 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0.\n",
      " 1. 0. 1. 0. 1. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 1. 1. 1. 0. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 1. 0.\n",
      " 0. 0. 0. 1. 0. 1. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Predicciones utilizando validacion simple con correcion de Laplace: [0. 0. 0. 1. 0. 0. 1. 1. 1. 1. 0. 1. 1. 0. 0. 0. 1. 0. 0. 1. 0. 1. 1. 0.\n",
      " 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1. 1. 0. 1. 0. 1. 1.\n",
      " 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 0. 1. 1.\n",
      " 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 1. 0. 1. 1. 0. 1. 0. 1. 0. 0. 0. 0. 1.\n",
      " 0. 0. 0. 1. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 1. 0. 1. 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0.\n",
      " 0. 0. 1. 1. 0. 1. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 1. 0. 0. 1. 1. 1. 1. 0.\n",
      " 0. 0. 1. 0. 1. 0. 1. 0.]\n",
      "Clases reales de la particion de text [0. 0. 0. 1. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 1. 0. 1. 0. 1. 1. 1. 1. 0. 0. 0. 1. 1.\n",
      " 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 1.\n",
      " 1. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 1.\n",
      " 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0.\n",
      " 1. 0. 1. 0. 1. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 1. 1. 1. 0. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 1. 0.\n",
      " 0. 0. 0. 1. 0. 1. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aalcala/anaconda3/lib/python3.7/site-packages/sklearn/naive_bayes.py:485: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n",
      "/home/aalcala/anaconda3/lib/python3.7/site-packages/sklearn/naive_bayes.py:485: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n"
     ]
    }
   ],
   "source": [
    "#SKLearn Validacion Cruzada\n",
    "# Hacemos el encode de los datos\n",
    "encAtributos = preprocessing.OneHotEncoder(categorical_features=dataset.nominalAtributos[:-1],sparse=False)\n",
    "X = encAtributos.fit_transform(dataset.datos[:,:-1])\n",
    "Y = dataset.datos[:,-1]\n",
    "\n",
    "# Partimos los datos en Train y Test\n",
    "x_train, x_test, y_train,y_test = train_test_split(X,Y,test_size=0.2)\n",
    "\n",
    "# Aplicamos Naive Bayes sin Laplace\n",
    "clf = MultinomialNB(alpha = 0,fit_prior=True, class_prior=None)\n",
    "predicciones=clf.fit(x_train,y_train).predict(x_test)\n",
    "\n",
    "# Print de los resultados\n",
    "print(\"Predicciones utilizando validacion simple sin correcion de Laplace:\", predicciones)\n",
    "print(\"Clases reales de la particion de text\", y_test)\n",
    "\n",
    "# Aplicamos Naive Bayes con Laplace\n",
    "clf = MultinomialNB(alpha = 0,fit_prior=True, class_prior=None)\n",
    "predicciones=clf.fit(x_train,y_train).predict(x_test)\n",
    "\n",
    "# Print de los resultados\n",
    "print(\"Predicciones utilizando validacion simple con correcion de Laplace:\", predicciones)\n",
    "print(\"Clases reales de la particion de text\", y_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicciones utilizando validacion simple sin correcion de Laplace: [1. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 0. 1. 0. 1. 1. 1. 1. 1. 0. 1. 0. 1. 1.\n",
      " 0. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 0. 0. 1. 0. 0. 0. 1. 1. 1. 0. 1. 1. 0.\n",
      " 1. 1. 1. 0. 0. 1. 0. 1. 1. 0. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1.\n",
      " 0. 1. 1. 1. 0. 1. 0. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 0. 0. 0. 1. 0.\n",
      " 1. 1. 0. 0. 1. 1. 1. 0. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1.\n",
      " 1. 1. 0. 1. 1. 1. 0. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 0. 0. 1. 0. 0. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1.]\n",
      "Clases reales de la particion de text [1. 0. 1. 0. 0. 1. 1. 0. 0. 1. 1. 0. 0. 1. 1. 1. 0. 0. 1. 1. 1. 0. 1. 1.\n",
      " 1. 1. 1. 1. 0. 0. 0. 1. 1. 0. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 0. 0. 1. 1.\n",
      " 1. 0. 1. 0. 1. 1. 0. 0. 1. 1. 1. 1. 1. 0. 1. 1. 0. 0. 1. 1. 1. 0. 0. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 0. 1. 1. 0. 0. 1. 1. 1. 1.\n",
      " 1. 1. 1. 0. 1. 0. 1. 1. 1. 1. 0. 0. 1. 1. 0. 1. 1. 0. 1. 1. 0. 0. 1. 1.\n",
      " 0. 1. 1. 0. 0. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 0.\n",
      " 1. 1. 0. 1. 1. 1. 1. 0. 0. 1. 1. 0. 0. 0. 1. 1. 0. 0. 0. 1. 0. 1. 1. 1.\n",
      " 1. 1. 0. 0. 1. 1. 0. 1. 1. 1. 0. 0. 0. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1. 0.]\n",
      "Predicciones utilizando validacion simple con correcion de Laplace: [1. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 0. 1. 0. 1. 1. 1. 1. 1. 0. 1. 0. 1. 1.\n",
      " 0. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 0. 0. 1. 0. 0. 0. 1. 1. 1. 0. 1. 1. 0.\n",
      " 1. 1. 1. 0. 0. 1. 0. 1. 1. 0. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1.\n",
      " 0. 1. 1. 1. 0. 1. 0. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 0. 0. 0. 1. 0.\n",
      " 1. 1. 0. 0. 1. 1. 1. 0. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1.\n",
      " 1. 1. 0. 1. 1. 1. 0. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 0. 0. 1. 0. 0. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1.]\n",
      "Clases reales de la particion de text [1. 0. 1. 0. 0. 1. 1. 0. 0. 1. 1. 0. 0. 1. 1. 1. 0. 0. 1. 1. 1. 0. 1. 1.\n",
      " 1. 1. 1. 1. 0. 0. 0. 1. 1. 0. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 0. 0. 1. 1.\n",
      " 1. 0. 1. 0. 1. 1. 0. 0. 1. 1. 1. 1. 1. 0. 1. 1. 0. 0. 1. 1. 1. 0. 0. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 0. 1. 1. 0. 0. 1. 1. 1. 1.\n",
      " 1. 1. 1. 0. 1. 0. 1. 1. 1. 1. 0. 0. 1. 1. 0. 1. 1. 0. 1. 1. 0. 0. 1. 1.\n",
      " 0. 1. 1. 0. 0. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 0.\n",
      " 1. 1. 0. 1. 1. 1. 1. 0. 0. 1. 1. 0. 0. 0. 1. 1. 0. 0. 0. 1. 0. 1. 1. 1.\n",
      " 1. 1. 0. 0. 1. 1. 0. 1. 1. 1. 0. 0. 0. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1. 0.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aalcala/anaconda3/lib/python3.7/site-packages/sklearn/preprocessing/_encoders.py:415: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/aalcala/anaconda3/lib/python3.7/site-packages/sklearn/preprocessing/_encoders.py:451: DeprecationWarning: The 'categorical_features' keyword is deprecated in version 0.20 and will be removed in 0.22. You can use the ColumnTransformer instead.\n",
      "  \"use the ColumnTransformer instead.\", DeprecationWarning)\n",
      "/home/aalcala/anaconda3/lib/python3.7/site-packages/sklearn/naive_bayes.py:485: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n",
      "/home/aalcala/anaconda3/lib/python3.7/site-packages/sklearn/naive_bayes.py:485: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n"
     ]
    }
   ],
   "source": [
    "# SKLearn Validacion Cruzada\n",
    "# Hacemos el encode de los datos\n",
    "encAtributos = preprocessing.OneHotEncoder(categorical_features=dataset.nominalAtributos[:-1],sparse=False)\n",
    "X = encAtributos.fit_transform(dataset.datos[:,:-1])\n",
    "Y = dataset.datos[:,-1]\n",
    "\n",
    "# Partimos los datos en Train y Test\n",
    "x_train, x_test, y_train,y_test = train_test_split(X,Y,test_size=0.2)\n",
    "\n",
    "# Aplicamos Naive Bayes sin Laplace\n",
    "clf = MultinomialNB(alpha = 0,fit_prior=True, class_prior=None)\n",
    "predicciones=clf.fit(x_train,y_train).predict(x_test)\n",
    "\n",
    "# Print de los resultados\n",
    "print(\"Predicciones utilizando validacion simple sin correcion de Laplace:\", predicciones)\n",
    "print(\"Clases reales de la particion de text\", y_test)\n",
    "\n",
    "# Aplicamos Naive Bayes con Laplace\n",
    "clf = MultinomialNB(alpha = 0,fit_prior=True, class_prior=None)\n",
    "predicciones=clf.fit(x_train,y_train).predict(x_test)\n",
    "\n",
    "# Print de los resultados\n",
    "print(\"Predicciones utilizando validacion simple con correcion de Laplace:\", predicciones)\n",
    "print(\"Clases reales de la particion de text\", y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
