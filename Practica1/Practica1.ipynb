{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/python3\n",
    "# -*- coding: utf-8 -*-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importamos Librerias\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from sortedcontainers import SortedDict\n",
    "import sklearn.naive_bayes as nb\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold, cross_val_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Datos:\n",
    "\n",
    "  TiposDeAtributos=('Continuo','Nominal')\n",
    "\n",
    "  # TODO: procesar el fichero para asignar correctamente las variables tipoAtributos, nombreAtributos, nominalAtributos, datos y diccionarios\n",
    "  # NOTA: No confundir TiposDeAtributos con tipoAtributos\n",
    "  def __init__(self, nombreFichero):\n",
    "\n",
    "      with open(nombreFichero, \"r\") as f:\n",
    "        # Guardamos el numero de datos que contiene el DataSet y esta en la primera linea\n",
    "        self.numDatos = int(f.readline())\n",
    "\n",
    "        # Guardamos el nombre de los atributos\n",
    "        self.nombreAtributos = f.readline().strip('\\n').split(',')\n",
    "        #print(self.nombreAtributos)\n",
    "\n",
    "        # Leemos el tipo de los atributos de las variables y eliminamos el ultimo que es un salto de linea\n",
    "        self.tipoAtributos = f.readline().strip('\\n').split(',')\n",
    "        #print(self.tipoAtributos)\n",
    "\n",
    "        # Comprobamos que todos los atributos sean Continuos o Nominales\n",
    "        if any(atr not in Datos.TiposDeAtributos for atr in self.tipoAtributos):\n",
    "            raise ValueError(\"Tipo de atributo erroneo\")\n",
    "\n",
    "        # Segun el atributo, asignamos True o False.\n",
    "        self.nominalAtributos = []\n",
    "\n",
    "        # Guardamos en la lista nominalAtributos en la posicion de cada uno si es o no Nominal\n",
    "        for tipo in self.tipoAtributos:\n",
    "            if tipo == self.TiposDeAtributos[0]:\n",
    "                self.nominalAtributos.append(False)\n",
    "            else:\n",
    "                self.nominalAtributos.append(True)\n",
    "        #print(self.nominalAtributos)\n",
    "\n",
    "        # Guardamos los datos del fichero y los formateamos, de tal forma que cada linea es una lista\n",
    "        datos = f.readlines()\n",
    "        datosFormat = []\n",
    "        for lista in datos:\n",
    "            datosFormat.append(lista.strip('\\n').split(','))\n",
    "\n",
    "        # print(set(sorted(datosFormat[0])))\n",
    "        listaDatosAtributos = []\n",
    "        for i in range(len(self.tipoAtributos)):\n",
    "            listaDatosAtributos.append([])\n",
    "\n",
    "        # Hacemos la traspuesta de los datos que guardamos para que cada lista de atributo guarde todos los datos\n",
    "        # de cada atributo.\n",
    "        for lista in datosFormat:\n",
    "            i = 0\n",
    "            for item in lista:\n",
    "                listaDatosAtributos[i].append(item)\n",
    "                i += 1\n",
    "\n",
    "        # Ordenamos y hacemos un set para eliminar repetidos.\n",
    "        i = 0\n",
    "        for item in listaDatosAtributos:\n",
    "            listaDatosAtributos[i] = sorted(set(item))\n",
    "            i += 1\n",
    "\n",
    "\n",
    "        # Creacion de lista diccionarios, en caso de que el atributo sea Continuo, el diccionario estara vacio\n",
    "        self.listaDicts = []\n",
    "        for i in range(len(self.tipoAtributos)):\n",
    "            self.listaDicts.append({})\n",
    "\n",
    "        # Creamos el diccionario tal y como se describe en las diapositivas, por orden y asignando valores numericos crecientes\n",
    "        i = 0\n",
    "        for atributo in listaDatosAtributos:\n",
    "            k = 0\n",
    "            if self.tipoAtributos[i] == \"Nominal\":\n",
    "                for dato in atributo:\n",
    "                    self.listaDicts[i][dato] = k\n",
    "                    k += 1\n",
    "            i += 1\n",
    "\n",
    "        # Creacion de la matriz de datos utilizando el diccionario para mapear los valores\n",
    "        # En primer lugar, creamos una matriz vacia de tamaña numero de atributos.\n",
    "        self.datos = np.empty((int(self.numDatos),int(len(self.tipoAtributos))))\n",
    "        i = 0\n",
    "        j = 0\n",
    "\n",
    "        # Metemos los datos en la matriz, mapeando con los diccionarios en el caso de que sean Nominales, y si son continuos normal.\n",
    "        for i in range(int(self.numDatos)):\n",
    "            for j in range(len(self.tipoAtributos)):\n",
    "                if self.tipoAtributos[j] == 'Nominal':\n",
    "                    self.datos[i][j] = self.listaDicts[j].get(str(datosFormat[i][j]))\n",
    "                else:\n",
    "                    self.datos[i][j] = datosFormat[i][j]\n",
    "        \n",
    "        print(self.nombreAtributos)\n",
    "        print(self.listaDicts)\n",
    "        f.close()\n",
    "\n",
    "\n",
    "  # TODO: implementar en la practica 1\n",
    "  def extraeDatos(self, idx):\n",
    "    return self.datos[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Age', 'Spectacle', 'Astigmatic', 'Tear', 'Class']\n",
      "[{'1': 0, '2': 1, '3': 2}, {'1': 0, '2': 1}, {'1': 0, '2': 1}, {'1': 0, '2': 1}, {'1': 0, '2': 1, '3': 2}]\n",
      "[[0. 0. 0. 0. 2.]\n",
      " [0. 0. 0. 1. 1.]\n",
      " [0. 0. 1. 0. 2.]\n",
      " [0. 0. 1. 1. 0.]\n",
      " [0. 1. 0. 0. 2.]\n",
      " [0. 1. 0. 1. 1.]\n",
      " [0. 1. 1. 0. 2.]\n",
      " [0. 1. 1. 1. 0.]\n",
      " [1. 0. 0. 0. 2.]\n",
      " [1. 0. 0. 1. 1.]\n",
      " [1. 0. 1. 0. 2.]\n",
      " [1. 0. 1. 1. 0.]\n",
      " [1. 1. 0. 0. 2.]\n",
      " [1. 1. 0. 1. 1.]\n",
      " [1. 1. 1. 0. 2.]\n",
      " [1. 1. 1. 1. 2.]\n",
      " [2. 0. 0. 0. 2.]\n",
      " [2. 0. 0. 1. 2.]\n",
      " [2. 0. 1. 0. 2.]\n",
      " [2. 0. 1. 1. 0.]\n",
      " [2. 1. 0. 0. 2.]\n",
      " [2. 1. 0. 1. 1.]\n",
      " [2. 1. 1. 0. 2.]\n",
      " [2. 1. 1. 1. 2.]]\n"
     ]
    }
   ],
   "source": [
    "dataset = Datos('lenses.data')\n",
    "\n",
    "print(dataset.datos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABCMeta,abstractmethod\n",
    "\n",
    "\n",
    "class Particion():\n",
    "\n",
    "  # Esta clase mantiene la lista de �ndices de Train y Test para cada partici�n del conjunto de particiones\n",
    "    def __init__(self,train=[],test=[]):\n",
    "        self.indicesTrain=train\n",
    "        self.indicesTest=test\n",
    "    \n",
    "    def __str__(self):\n",
    "        return \"Train: {}\\nTest:  {}\".format(str(self.indicesTrain),str(self.indicesTest)) \n",
    "\n",
    "#####################################################################################################\n",
    "\n",
    "class EstrategiaParticionado:\n",
    "\n",
    "    # Clase abstracta\n",
    "    __metaclass__ = ABCMeta\n",
    "\n",
    "    # Lista de las particiones\n",
    "    def __init__(self, nombre=\"\"):\n",
    "        self.nombreEstrategia = nombre\n",
    "        self.numeroParticiones = 0\n",
    "        self.particiones=[]\n",
    "\n",
    "    # Atributos: deben rellenarse adecuadamente para cada estrategia concreta: nombreEstrategia, numeroParticiones, listaParticiones. Se pasan en el constructor\n",
    "\n",
    "    @abstractmethod\n",
    "    # TODO: esta funcion deben ser implementadas en cada estrategia concreta\n",
    "    def creaParticiones(self,datos,seed=None):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValidacionSimple(EstrategiaParticionado):\n",
    "    \n",
    "    def __init__(self, porcentaje):\n",
    "        self.porcentaje = porcentaje\n",
    "        super().__init__(\"Validacion simple\")\n",
    "        \n",
    "  # Crea particiones segun el metodo tradicional de division de los datos segun el porcentaje deseado.\n",
    "  # Devuelve una lista de particiones (clase Particion)\n",
    "  # TODO: implementar\n",
    "    def creaParticiones(self,datos,seed=None):\n",
    "        np.random.seed(seed)\n",
    "\n",
    "        self.numeroParticiones = 1\n",
    "    \n",
    "        # Generamos una lista con todos los números de datos aleatorios   \n",
    "        indicesAleatorios = np.random.permutation(int(datos.numDatos))\n",
    "        \n",
    "        # Creamos la particion, en funcion del porcentaje especificado\n",
    "        self.particiones = [Particion(indicesAleatorios[:int(datos.numDatos*self.porcentaje)],\n",
    "                                      indicesAleatorios[int(datos.numDatos*self.porcentaje):])]\n",
    "        \n",
    "        return self.particiones\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [ 4  6 23  9 14  5 13 15  7 10  1 21 20 22 16 11 18  3]\n",
      "Test:  [17  0 12  8  2 19]\n"
     ]
    }
   ],
   "source": [
    "validacion_simple = ValidacionSimple(0.75)\n",
    "validacion_simple.creaParticiones(dataset)\n",
    "print(validacion_simple.particiones[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValidacionCruzada(EstrategiaParticionado):\n",
    "\n",
    "  # Crea particiones segun el metodo de validacion cruzada.\n",
    "  # El conjunto de entrenamiento se crea con las nfolds-1 particiones y el de test con la particion restante\n",
    "  # Esta funcion devuelve una lista de particiones (clase Particion)\n",
    "  # TODO: implementar\n",
    "    \n",
    "    def __init__(self, k):\n",
    "        self.k = k\n",
    "        super().__init__(\"Validacion cruzada\")\n",
    "\n",
    "    def creaParticiones(self,datos,seed=None):\n",
    "        np.random.seed(seed)\n",
    "        \n",
    "        self.numeroParticiones = self.k\n",
    "        \n",
    "        # Generamos una lista con todos los números de datos aleatorios   \n",
    "        indicesAleatorios = np.random.permutation(int(datos.numDatos))\n",
    "        \n",
    "        # Hallamos el tamaño de cada bloque\n",
    "        tamBloque = int(datos.numDatos/self.k)\n",
    "        \n",
    "        \n",
    "        datosSobran = datos.numDatos - (tamBloque*self.k)\n",
    "        count = 0\n",
    "        for i in range(self.k):\n",
    "            \n",
    "            train = np.delete(indicesAleatorios, range(i*tamBloque,(i+1)*tamBloque))\n",
    "            test =  indicesAleatorios[i*tamBloque:(i+1)*tamBloque]\n",
    "            \n",
    "            # Caso en el que la cuenta es justa\n",
    "            if datosSobran == 0:\n",
    "                self.particiones.append(Particion(train, test))\n",
    "                \n",
    "            # Contemplamos el caso de que la division para sacar el numero de subconjuntos no fuese entera\n",
    "            if datosSobran > 0:\n",
    "                count += 1\n",
    "                particionTest = np.append(test, train[(datos.numDatos - tamBloque)- i - 1])\n",
    "                particionTrain = np.delete(train, (datos.numDatos - tamBloque)- i - 1)\n",
    "                datosSobran -= 1\n",
    "                self.particiones.append(Particion(particionTrain, particionTest))\n",
    "                \n",
    "            \n",
    "           \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [13  8  9 12 17 19 22  2 21  0 18 23 14  7 20  4  1  6]\n",
      "Test:  [16 15 11  3 10  5]\n",
      "Train: [16 15 11  3 10  5 22  2 21  0 18 23 14  7 20  4  1  6]\n",
      "Test:  [13  8  9 12 17 19]\n",
      "Train: [16 15 11  3 10  5 13  8  9 12 17 19 14  7 20  4  1  6]\n",
      "Test:  [22  2 21  0 18 23]\n",
      "Train: [16 15 11  3 10  5 13  8  9 12 17 19 22  2 21  0 18 23]\n",
      "Test:  [14  7 20  4  1  6]\n"
     ]
    }
   ],
   "source": [
    "v_cruzada = ValidacionCruzada(4)\n",
    "v_cruzada.creaParticiones(dataset)\n",
    "\n",
    "for particion in v_cruzada.particiones:\n",
    "    print(particion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Clasificador:\n",
    "  \n",
    "    # Clase abstracta\n",
    "    __metaclass__ = ABCMeta\n",
    "\n",
    "    # Metodos abstractos que se implementan en casa clasificador concreto\n",
    "    @abstractmethod\n",
    "    # TODO: esta funcion debe ser implementada en cada clasificador concreto\n",
    "    # datosTrain: matriz numpy con los datos de entrenamiento\n",
    "    # atributosDiscretos: array bool con la indicatriz de los atributos nominales\n",
    "    # diccionario: array de diccionarios de la estructura Datos utilizados para la codificacion de variables discretas\n",
    "    def entrenamiento(self,datos,datosTrain,atributosDiscretos,diccionario):\n",
    "        pass\n",
    "\n",
    "\n",
    "    @abstractmethod\n",
    "    # TODO: esta funcion debe ser implementada en cada clasificador concreto\n",
    "    # devuelve un numpy array con las predicciones\n",
    "    def clasifica(self,datosTest,atributosDiscretos,diccionario):\n",
    "        pass\n",
    "\n",
    "\n",
    "    # Obtiene el numero de aciertos y errores para calcular la tasa de fallo\n",
    "    # TODO: implementar\n",
    "    def error(self,datos,pred):\n",
    "    # Aqui se compara la prediccion (pred) con las clases reales y se calcula el error    \n",
    "        i = 0\n",
    "        real = datos[:,-1]\n",
    "        error = 0\n",
    "        for i in range(len(real)):\n",
    "            if real[i] != pred[i]:\n",
    "                error += 1\n",
    "        err = (error)/(len(real)+0.0)\n",
    "        return err\n",
    "\n",
    "\n",
    "    # Realiza una clasificacion utilizando una estrategia de particionado determinada\n",
    "    # TODO: implementar esta funcion\n",
    "    def validacion(self,particionado,dataset,clasificador,seed=None):\n",
    "\n",
    "    # Creamos las particiones siguiendo la estrategia llamando a particionado.creaParticiones\n",
    "    # - Para validacion cruzada: en el bucle hasta nv entrenamos el clasificador con la particion de train i\n",
    "    # y obtenemos el error en la particion de test i\n",
    "    # - Para validacion simple (hold-out): entrenamos el clasificador con la particion de train\n",
    "    # y obtenemos el error en la particion test. Otra opci�n es repetir la validaci�n simple un n�mero especificado de veces, obteniendo en cada una un error. Finalmente se calcular�a la media.\n",
    "        errores = 0\n",
    "        #particionado.creaParticiones(dataset, seed)\n",
    "    # Comprobamos si es por validación cruzada o simple, por la longitud de la lista de particiones\n",
    "    # Validación Simple\n",
    "        if len(particionado.particiones) == 1:\n",
    "            clasificador.entrenamiento(dataset, particionado.particiones[0].indicesTrain)\n",
    "            pred = clasificador.clasifica(dataset,particionado.particiones[0].indicesTest)\n",
    "            ret = self.error(dataset.extraeDatos(particionado.particiones[0].indicesTest), pred)\n",
    "            if ret > 0:\n",
    "                return ret\n",
    "            else:\n",
    "                return 0\n",
    "            \n",
    "    # Validación Cruzada        \n",
    "        else:\n",
    "            for particion in particionado.particiones:\n",
    "                clasificador.entrenamiento(dataset, particion.indicesTrain)\n",
    "                pred = clasificador.clasifica(dataset,particion.indicesTest)\n",
    "                ret = self.error(dataset.extraeDatos(particion.indicesTest), pred)\n",
    "                errores += ret\n",
    "            error = errores/len(particionado.particiones)\n",
    "            #Devolucion de la media de los errores\n",
    "            return error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClasificadorNaiveBayes(Clasificador):\n",
    "    \n",
    "    def __init__(self, laplace):\n",
    "        self.laplace = laplace\n",
    "        \n",
    "    \n",
    "    def entrenamiento(self,dataset,datosTrain):\n",
    "     \n",
    "        # Cargamos todos los datos de la clase del dataset desde la matriz de datos\n",
    "        clasesTrain = dataset.extraeDatos(datosTrain)\n",
    "        self.numClases = clasesTrain[:,-1]\n",
    "        # Contamos las apariciones de cada uno para luego calcular la probabilidad a priori de cada clase\n",
    "        counter = Counter(self.numClases)\n",
    "        # Calculamos la probabilidad de la clase y lo metemos en un diccionario ordenado segun el numero \n",
    "        # correspondiente a cada clase asignado en el diccionario\n",
    "        self.dictPrioris={}\n",
    "        for k in counter:\n",
    "            k = int(k)\n",
    "            counter[k] = counter[k]/len(self.numClases)\n",
    "            self.dictPrioris[k] = counter[k]\n",
    "            \n",
    "        # Aqui ordenamos el diccionario para que esten en el mismo orden de como extraemos los datos del dataset\n",
    "        self.dictPrioris = SortedDict(self.dictPrioris)\n",
    "        \n",
    "        # Calcular tablas de probabilidades del entrenamiento. Tenemos que calcular por cada atributo una cuenta\n",
    "        # de las apariciones en cada clase\n",
    "        # Creamos una lista de matrices, donde vamos almacenar todos los datos que hemos obtenido en los datos de Test\n",
    "        self.posteriori = np.zeros(len(dataset.nombreAtributos)-1,dtype=object)\n",
    "        \n",
    "        # Recorremos todos los datos de la matriz sin llegar a la clase\n",
    "        for i in range(len(dataset.nombreAtributos) - 1):\n",
    "            \n",
    "            # Si el dato que obtenemos es Nominal haremos el recuento de todas las veces que sale la P(D|H)\n",
    "            if dataset.nominalAtributos[i] == True:\n",
    "                \n",
    "                # Creamos una matriz de tamaño X: Número de Atributos menos la clase Y: Número de clases\n",
    "                post = np.zeros((len(dataset.listaDicts[i]),len(dataset.listaDicts[-1])))\n",
    "               \n",
    "                # Aqui contamos todos las datos que queremos del datos Train para construir la matriz de entrenamiento\n",
    "                for c in range(len(dataset.listaDicts[-1])):\n",
    "                    datosEnt = dataset.extraeDatos(datosTrain)\n",
    "                    dat = datosEnt[:,i]\n",
    "                    repes = Counter(dat[datosEnt[:,-1] == c])\n",
    "                    for r in repes:\n",
    "                        post[int(r),c] = repes[r]\n",
    "                    if self.laplace == True:\n",
    "                        self.posteriori[i] = post +1\n",
    "                    else:\n",
    "                        self.posteriori[i] = post\n",
    "            \n",
    "            # Si el dato es Continuo obtendremos la media y la desviación tipica de la clase\n",
    "            else:\n",
    "                \n",
    "                # Creamos una matriz de X: Los datos de Media y Desivación típica Y: Número de clases\n",
    "                post = np.zeros((2,len(dataset.listaDicts[-1])))\n",
    "               \n",
    "                # Aqui obtenemos la media y desviación tipica de cada clase, despues de tener los datos de entrenamiento\n",
    "                for c in range(len(dataset.listaDicts[-1])):\n",
    "                    datosEnt = dataset.extraeDatos(datosTrain)\n",
    "                    dat = datosEnt[:,i]\n",
    "                    datos = dat[datosEnt[:,-1] == c]\n",
    "                    post[0][c] = np.mean(datos)\n",
    "                    post[1][c] = np.std(datos)\n",
    "                self.posteriori[i] = post\n",
    "            \n",
    "        #Calculamos los valores de los posteriori de todos las tablas anteriores\n",
    "        for j in range(len(dataset.nominalAtributos)-1):\n",
    "            if dataset.nominalAtributos[j] == True:\n",
    "                for i in range(len(dataset.listaDicts) - 1):\n",
    "                    self.posteriori[i] /= sum(self.posteriori[i])\n",
    "\n",
    "        \n",
    "    def clasifica(self,dataset,datosTest):\n",
    "        j = 0\n",
    "        aux = 1\n",
    "        self.prediccion = []\n",
    "        datTest = dataset.extraeDatos(datosTest)\n",
    "        #Ahora vamos a estudiar la probabilidad de la clase con los datos obtenidos en el entrenamiento\n",
    "        #Recorremos todos las datos de la matriz de los datos Test\n",
    "        \n",
    "        print(self.posteriori)\n",
    "        for dato in datTest:\n",
    "            mapa = []\n",
    "            if dataset.nominalAtributos[j] == True:\n",
    "            #Aqui obtenemos los prioris de cada clase para poder obtener la probabilidad de cada una\n",
    "                for clase in range(len(self.dictPrioris)):\n",
    "                #Aqui obtenemos cada valor posteriori de nuestro entrenamiento de los datos, es decir, P(D|H)\n",
    "                    for atributo in range(len(self.posteriori)):\n",
    "                        aux *= self.posteriori[atributo][int(dato[atributo])][clase]\n",
    "                    \n",
    "                    #Aqui obtenemos la siguiente probabilidad P(D|H)*P(H)\n",
    "                    aux2 = self.dictPrioris.get(clase)*aux\n",
    "                    aux = 1\n",
    "                \n",
    "                    #Lo añadimos a una lista para obtener la probabilidad de las diferentes clases\n",
    "                    mapa.append(aux2)\n",
    "            \n",
    "            #Aqui obtenemos la probabilidad de los atibutos continuos\n",
    "            else:\n",
    "                for clase in range(len(self.dictPrioris)):\n",
    "                    for atributo in range(len(self.posteriori)):\n",
    "                        # Hacemos la formula de la distribucion normal\n",
    "                        exp1 = 1/(self.posteriori[atributo][0][clase]*math.sqrt(2*math.pi))\n",
    "                        exp2 = ((dato-self.posteriori[atributo][0][clase]) - dato[atributo]/self.posteriori[atributo][1][clase])\n",
    "                        exp3 = exp2 ** 2\n",
    "                        exp4 = math.exp((-1/2)* exp3)\n",
    "                        total = exp1 * exp4\n",
    "                    aux2 = total * self.dictPrioris.get(clase)\n",
    "                    mapa.append(aux2)\n",
    "            j +=1\n",
    "            if j == len(dataset.nominalAtributos)-1:\n",
    "                j = 0\n",
    "            \n",
    "            #Aqui obtenemos la predicción de mayor probabilidad y la guardamos en nuestra lista de predicciones\n",
    "            self.prediccion.append(np.argmax(mapa))\n",
    "        \n",
    "        #Devolvemos la lista con la predicción de nuestro clasifica   \n",
    "        return self.prediccion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[0.19083969, 0.46581197],\n",
      "       [0.22900763, 0.33760684],\n",
      "       [0.07442748, 0.04273504],\n",
      "       [0.50572519, 0.15384615]])\n",
      " array([[0.62575639, 0.65001934],\n",
      "       [0.37424361, 0.34998066]])\n",
      " array([[0.01904762, 0.07659574],\n",
      "       [0.03428571, 0.1106383 ],\n",
      "       [0.50666667, 0.5787234 ],\n",
      "       [0.08952381, 0.0893617 ],\n",
      "       [0.35047619, 0.14468085]])\n",
      " array([[0.21132075, 0.27916667],\n",
      "       [0.12075472, 0.06666667],\n",
      "       [0.01320755, 0.01666667],\n",
      "       [0.16981132, 0.19166667],\n",
      "       [0.31698113, 0.22083333],\n",
      "       [0.01509434, 0.01666667],\n",
      "       [0.01886792, 0.025     ],\n",
      "       [0.04150943, 0.06666667],\n",
      "       [0.01320755, 0.00833333],\n",
      "       [0.07924528, 0.10833333]])\n",
      " array([[0.55114423, 0.52730337],\n",
      "       [0.44885577, 0.47269663]])\n",
      " array([[0.56      , 0.71489362],\n",
      "       [0.09333333, 0.11489362],\n",
      "       [0.07619048, 0.04255319],\n",
      "       [0.05142857, 0.0212766 ],\n",
      "       [0.21904762, 0.10638298]])\n",
      " array([[0.05714286, 0.06382979],\n",
      "       [0.14666667, 0.25957447],\n",
      "       [0.33333333, 0.34042553],\n",
      "       [0.20761905, 0.11489362],\n",
      "       [0.2552381 , 0.2212766 ]])\n",
      " array([[0.71902637, 0.73738007],\n",
      "       [0.28097363, 0.26261993]])\n",
      " array([[0.04007634, 0.07692308],\n",
      "       [0.29007634, 0.36752137],\n",
      "       [0.57824427, 0.48290598],\n",
      "       [0.09160305, 0.07264957]])\n",
      " array([[0.91395793, 0.9055794 ],\n",
      "       [0.0248566 , 0.06008584],\n",
      "       [0.06118547, 0.03433476]])\n",
      " array([[0.72099681, 0.71920973],\n",
      "       [0.27900319, 0.28079027]])\n",
      " array([[0.3110687 , 0.1965812 ],\n",
      "       [0.23473282, 0.23504274],\n",
      "       [0.32633588, 0.33760684],\n",
      "       [0.1278626 , 0.23076923]])\n",
      " array([[0.76371978, 0.75091915],\n",
      "       [0.23628022, 0.24908085]])\n",
      " array([[0.10516252, 0.19742489],\n",
      "       [0.04206501, 0.06437768],\n",
      "       [0.85277247, 0.73819742]])\n",
      " array([[0.17017208, 0.25751073],\n",
      "       [0.73231358, 0.57939914],\n",
      "       [0.09751434, 0.16309013]])\n",
      " array([[0.70794925, 0.7120866 ],\n",
      "       [0.29205075, 0.2879134 ]])\n",
      " array([[0.01908397, 0.02991453],\n",
      "       [0.20038168, 0.17948718],\n",
      "       [0.64122137, 0.65384615],\n",
      "       [0.13931298, 0.13675214]])\n",
      " array([[0.7561454 , 0.76381523],\n",
      "       [0.2438546 , 0.23618477]])\n",
      " array([[0.57471264, 0.63362069],\n",
      "       [0.42528736, 0.36637931]])\n",
      " array([[0.95019157, 0.98706897],\n",
      "       [0.04980843, 0.01293103]])]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 48 is out of bounds for axis 0 with size 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-116-177411b8a88b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#clasificador.entrenamiento(dataset, validacion_simple.particiones[0].indicesTrain)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#clasificador.clasifica(dataset, validacion_simple.particiones[0].indicesTest)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0merror\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclasificador\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidacion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidacion_simple\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mclasificador\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0merror1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclasificador\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidacion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv_cruzada\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mclasificador\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"El error del Clasificador NaiveBayes  para Validacion Simple es: \"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-80-fc3769d437ab>\u001b[0m in \u001b[0;36mvalidacion\u001b[0;34m(self, particionado, dataset, clasificador, seed)\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparticionado\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparticiones\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mclasificador\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mentrenamiento\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparticionado\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparticiones\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindicesTrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m             \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclasificador\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasifica\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mparticionado\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparticiones\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindicesTest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextraeDatos\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparticionado\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparticiones\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindicesTest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-115-3a4a42f3c549>\u001b[0m in \u001b[0;36mclasifica\u001b[0;34m(self, dataset, datosTest)\u001b[0m\n\u001b[1;32m     87\u001b[0m                 \u001b[0;31m#Aqui obtenemos cada valor posteriori de nuestro entrenamiento de los datos, es decir, P(D|H)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m                     \u001b[0;32mfor\u001b[0m \u001b[0matributo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mposteriori\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m                         \u001b[0maux\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mposteriori\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0matributo\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdato\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0matributo\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mclase\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m                     \u001b[0;31m#Aqui obtenemos la siguiente probabilidad P(D|H)*P(H)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 48 is out of bounds for axis 0 with size 2"
     ]
    }
   ],
   "source": [
    "clasificador = ClasificadorNaiveBayes(True)\n",
    "#clasificador.entrenamiento(dataset, validacion_simple.particiones[0].indicesTrain)\n",
    "#clasificador.clasifica(dataset, validacion_simple.particiones[0].indicesTest)\n",
    "error = clasificador.validacion(validacion_simple,dataset,clasificador)\n",
    "error1 = clasificador.validacion(v_cruzada,dataset,clasificador)\n",
    "print(\"El error del Clasificador NaiveBayes  para Validacion Simple es: \"+str(error))\n",
    "print(\"El error del Clasificador NaiveBayes para Validacion Cruzada es: \"+str(error1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['TLeftSq', 'TMidSq', 'TRightSq', 'MLeftSq', 'MMidSq', 'MRightSq', 'BLeftSq', 'BMidSq', 'BRightSq', 'Class']\n",
      "[{'b': 0, 'o': 1, 'x': 2}, {'b': 0, 'o': 1, 'x': 2}, {'b': 0, 'o': 1, 'x': 2}, {'b': 0, 'o': 1, 'x': 2}, {'b': 0, 'o': 1, 'x': 2}, {'b': 0, 'o': 1, 'x': 2}, {'b': 0, 'o': 1, 'x': 2}, {'b': 0, 'o': 1, 'x': 2}, {'b': 0, 'o': 1, 'x': 2}, {'negative': 0, 'positive': 1}]\n",
      "[[2. 2. 2. ... 1. 1. 1.]\n",
      " [2. 2. 2. ... 2. 1. 1.]\n",
      " [2. 2. 2. ... 1. 2. 1.]\n",
      " ...\n",
      " [1. 2. 1. ... 1. 2. 0.]\n",
      " [1. 2. 1. ... 1. 2. 0.]\n",
      " [1. 1. 2. ... 2. 2. 0.]]\n"
     ]
    }
   ],
   "source": [
    "dataset = Datos('tic-tac-toe.data')\n",
    "print(dataset.datos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VALIDACION SIMPLE Train: [520 790 761 917 182 681 250 857 242 679 522 145 364 272 119 610 393 493\n",
      "  38 501 758 856 567 461 203 294 112 948 602 120 388 266 233 837 367 524\n",
      " 248 901 763 269 336 184 260 680 733 401 786 669 653 862 625 816 302 757\n",
      " 558 643 143 173 801 185 525 295 428  75  42 297 439 545  62 207 363 408\n",
      " 792 712 264 590 708 343 308 205 125 164 375 430 832 725 382 196 953 839\n",
      " 179 639 194 751 531 484 665 945 512 711 208 595 743 940 608 688   8 783\n",
      " 802 654 765 813 921 793  86 627 689 114 285 911 325 247 499 624 136 815\n",
      " 473 904 740 903  67 189 720 530 796 645 465 641 230  31 909 384  79 766\n",
      "  69 280 842  73 445 674 519 906 323 154 559 198 952 592 887 937 744 224\n",
      " 334 699 789 193 561  14 617 695 698 631 505 635 563 175 178 579 554 210\n",
      " 715 283 831 436 803 453 900 666 569 724 340 865 485 255 897 177  59 440\n",
      " 804 845 692 628 508 910  10 521  78 226 111 718 403 497 539 231 263 947\n",
      " 823 642 310 713 618 144 259   4 702 406 212 110 709 600 891 599 155 550\n",
      " 726 907 871 716 282 251 496 748 146 160 799 434  22 218 101 874 703 438\n",
      " 478 220 293 464 314 547 918 677 483 204 356 147 772  51 225 727 385  41\n",
      " 386 468 632 896 402 746  87 697 214 858 755 186 181 924  26 852  15 481\n",
      " 770 444 769 469 495  25 555 565 365 749 951 651 171  82 510 582 303 848\n",
      " 574 747 414 180  54 488 431 291 337  92 383 219 741 638  66 732 648 347\n",
      "   2 652 601 195 759 474  40 221 229  17 397 409 331 424   3 881 604 589\n",
      " 566 722 292 504 500 721 258 576 615 603 841 678 165 183 586 609 934 234\n",
      " 936 534  49  45 271 920  97 246 433 322  88 443 946 400 109 238  89 869\n",
      "  39 950 768 667 587  53 489 332 598 941 316  47 432 738 358 876 787 460\n",
      " 318 307 811 342 344 912 776 782 585 515 407  64 821 693 798 864 640 806\n",
      " 694 317 518 309 892 717 277 902 514 853 245 745 261 935 650  32 287  35\n",
      " 157 894 387 279  19 338 299 300 288 571 549 253 729 276 580 418 274 636\n",
      " 922 420 908 883  72 774 612 597 658 113 622 190 914 824 133 450 446 634\n",
      " 915 637 731 629 812 479 771 735 670 398 477 552 877 673 843 362 878 216\n",
      " 606 152 107 578 760 192 395 879 872 361 141 739 330   6 348 458 126 360\n",
      " 404 213  96 172 354 413 700 855 840 191 118 705  34 954 137 621 867 649\n",
      " 148 814   0 327 828 211 847 237 548 131 457 690  93 278  55 374 810 289\n",
      "  29 475 822 257 426 215 546 809   9 437 905 357 630 326 844 142 416  80\n",
      " 560 932 517 392 380 575 421 928 351  18 616 105 838 349 805 452 346  60\n",
      " 368 684 710  13 794 480 377 863 425 577 391 222 764 265  68 188 682 930\n",
      " 620 927 102 696 313 232 535 880 564 127 568 594 373 859  58 270 290 104\n",
      " 168 243 122  90 161 854 899 830 923 169 956 529 156 797 427 773 523 656\n",
      " 614 228 467 581 511  28 134 350 633 646 273 176 938 707 463 588 820 817\n",
      " 730  33 249 149 239 100 329 174 551 394 399 454 124 800 734 833 543 132\n",
      " 328 298 537 605 162 829 779 885 657 611 369 422 683 850 613 753 121 150\n",
      " 378 777 527 607 849 252 116 159 509 788 223 305   5 199 780 166 372 320\n",
      " 441 281 355 957 888 108 304 140 591 106 415 206 201  61 542 573]\n",
      "Test:  [  7 296 200 459 875 536 366 762 661 596 490 301  43 462 151 583 860 825\n",
      " 236 389 750 335 943 890 139  21 882 528 333 553 886 435 544 676 129  83\n",
      " 227 379 381 128  52  11 447  20 167 345 557 448 861 663 412 158 352   1\n",
      " 668 719 419 808 312 835 647 153  85  91  95  23  37 451 895 209  44 664\n",
      " 756 541 781 135 866 417  99 324 834 819 268 411 675 659 503 187 836 775\n",
      " 487 396  77 341 868 321 516 311 455 931 691 701  30 846  36 714 562 267\n",
      " 532 728 916 795 785  76 926 241 130  46 686 275 737 556 498 429 163  81\n",
      " 217 706  24 456 949 376 240  65 736 662 791 506 660 123 540  16  27 533\n",
      " 723 370 870 623  63 471 138 410 115 767 570 827 470 593  70 491  98 925\n",
      "  50 944 644 584 449 117 889 244 752 687 526 256 423 284 494 319 262 442\n",
      " 538 103 898 359 235 826 919  56 390 572 884 754 286 202 942 371 655 306\n",
      " 893 626 482 507 353 197 685 339 170 619 466 818  57 492  84 742 807 476\n",
      " 315 955 472 671 913 933 486 851  94  74 873 778 704 784 929 513  71 939\n",
      " 502  48 672 405 254  12]\n",
      "VALIDACION CRUZADA\n",
      "Train: [354 105 659 594  12 826   1 110 384 769 334 495 630 271 910  74 255 540\n",
      " 761 621 832 391 480 772 498  62 360 862 219 949 777 266 633 608 859  59\n",
      " 432 408 797 261 810 139 401 950 207 877 531 864 574 252 166 451 553 687\n",
      " 295 628 325 656 787 538 654  37 504 299 293 733 856 598 471 726 505  38\n",
      " 599 527  43 933 927 230 296 844 872 534 479 721 874 556  42 694 316 161\n",
      " 424 327 107 803 270 416 677 891 138 788 597 467 908 785 782  76 890 607\n",
      " 283 768 162  22 398 569 937 724 349 164 762 583 363 606 554  85 214 404\n",
      " 905 670 869  48 860 187 158 893 117 851 926 774 113 678 792 957 171 425\n",
      " 898 209 735 857 836 667 198 745 691 421 231 572 819 718 547 766 907 771\n",
      " 126 310 901 545 463 673 390 355 914 588 904 816 763 736 757 765 333 566\n",
      " 615 592 337 675 847 245  87 143 543 683 536 802 326 202 841 369 605 530\n",
      " 286 262 839 358 318 947 379 222 440  15 476 879 308 343 693 800 719 438\n",
      " 225 741 692 790 582 494  75 478 176 690 861 335 517 306 885 173 778 483\n",
      " 945 722 801  80 717 188  82 870  33 427 385 611 297   9 559  93 134 897\n",
      " 150 282 595  18 679 276 955 661 854 508 264 330 435 120 552 154 580  96\n",
      " 211   3 815 445 537 744 917 845 750  79 644  77 796 516 233 632 412 307\n",
      " 622 523   5 853 734 458 244 789 375 267  35 210 748 103 189 850 686 201\n",
      " 813  13 942 822 613  21 849 558 546 284 406 603 889  25 882 455 619 629\n",
      " 840 195 393 610 265 549  65 519 645 512 858 697 855 938 533 805 808 253\n",
      " 749 685 758 155 812 911 118 823 328 551  24 712 568 213 571  57 422 157\n",
      " 833 329 159 279 370 623 100 181 170 941 411 700 710 924 377 871 581 315\n",
      "   8  26 714 348  23 371 575 738 472 626 133  20 590 220 353 892 728 136\n",
      " 317 453 930 809 346 442 913 542 500  66  32 320 309 268 172 576 881 239\n",
      " 423 794 591 362 313 652 521 916 535 739 863 108 784 918 459 884 662 351\n",
      " 770 389  78 303 497 160 419 780   4 604 115 951  36 180 429 634  50 740\n",
      " 403  44 151  88 577 238 305 653 456  84 642 292 616 227 319 258 669 247\n",
      " 257 903 888  64 759 896 584 793 397 415 821 175 680 179  95 119 380 804\n",
      " 485 256 561 715 147  60  72 490 922 236  54 532 364 636 900 206  63 324\n",
      " 708 602 190 902 923 229 713 688 791 720 414 507 852 447 737 682 647 183\n",
      " 386  52 658 752 402 448  86 807 361  90 565  61 263 709 477 340 570 886\n",
      " 395 649 600 221 203 426 378 912 185 259  67  56 323 814  49  83 142   7\n",
      " 837  91 723 331 124 275 400 705 473  55 373 383 880 433  29 128 392 163\n",
      " 806 756 627 452 428 177 237 298 491 111 116 866 635 449 454  10 356 387\n",
      " 193 509 925 651 511 650 382 287 342 641 314 929 940 492 251 698 919 269\n",
      " 935 672  14 399  99 638 587 676 109 586  51 250 499 339 121 775 660 234\n",
      " 232 114 461 944 743 174 344 156  11 646 381 639 474 895 894 359 936 618\n",
      " 827 624 779  27 357 194 285  39  98 407 829 347 585 208  16 149 609 703\n",
      "  41 273 106 241 503 304 289 465 226 420 367 640 481 557 228 376 617 197\n",
      " 112 909 781 562 153 699 795 418 730 751 706  69 755 137 281  89  47 541\n",
      " 671 493 579 754 567 811 294 842 742 148 776 388 417 838 182 196]\n",
      "Test:  [952 747 224 920 487 550 291 365   0 539 374 469  30 135 288 760 725 746\n",
      " 248 943 906 614 254 524 544 520 439 277 311 783 820 192 817 612 260 681\n",
      " 122 410 668  34  73 278 413 731 828 345 506 948 773 939 931 522 242 368\n",
      " 515 830 732 846 152 873 883 199 824 664 184 596 145 131 665 246 631  81\n",
      " 235 689 674   6 168 648 302 125 409 240 799 695 625 529 798 932 332 786\n",
      " 753 528 711 169 322  70 434 663 372  58 589 301 300 466  19 123 141 953\n",
      " 460   2 767 127  17 707  28 848 704 878 564 130 204  97 249 312 366  40\n",
      " 716 450 290 104 431  94 394 272 501 525 212 513  31 129 887 518 457 865\n",
      " 727 684 405 601 563 338 489 436 215 637  68 146 578 223  71 462 657 956\n",
      " 655 764 341 443 510 441 350 144 666 243 548 928 484 555 444 482 946 701\n",
      " 430 216 867 729 526  45 831 186  46 200 843 437 702 921 488 496 218 818\n",
      " 868  92 875 352 899 573 954 280 825 468 620 336 205 643 835 876 696 101\n",
      " 178 140 132  53 834 217 486 502 191 321 396 165 446 464 274 593 167 470\n",
      " 102 915 934 560 514 475]\n",
      "Train: [952 747 224 920 487 550 291 365   0 539 374 469  30 135 288 760 725 746\n",
      " 248 943 906 614 254 524 544 520 439 277 311 783 820 192 817 612 260 681\n",
      " 122 410 668  34  73 278 413 731 828 345 506 948 773 939 931 522 242 368\n",
      " 515 830 732 846 152 873 883 199 824 664 184 596 145 131 665 246 631  81\n",
      " 235 689 674   6 168 648 302 125 409 240 799 695 625 529 798 932 332 786\n",
      " 753 528 711 169 322  70 434 663 372  58 589 301 300 466  19 123 141 953\n",
      " 460   2 767 127  17 707  28 848 704 878 564 130 204  97 249 312 366  40\n",
      " 716 450 290 104 431  94 394 272 501 525 212 513  31 129 887 518 457 865\n",
      " 727 684 405 601 563 338 489 436 215 637  68 146 578 223  71 462 657 956\n",
      " 655 764 341 443 510 441 350 144 666 243 548 928 484 555 444 482 946 701\n",
      " 430 216 867 729 526  45 831 186  46 200 843 437 702 921 488 496 218 818\n",
      " 868  92 875 352 899 573 954 280 825 468 620 336 205 643 835 876 696 101\n",
      " 178 140 132  53 834 217 486 502 191 321 396 165 446 464 274 593 167 470\n",
      " 102 915 934 560 514 188  82 870  33 427 385 611 297   9 559  93 134 897\n",
      " 150 282 595  18 679 276 955 661 854 508 264 330 435 120 552 154 580  96\n",
      " 211   3 815 445 537 744 917 845 750  79 644  77 796 516 233 632 412 307\n",
      " 622 523   5 853 734 458 244 789 375 267  35 210 748 103 189 850 686 201\n",
      " 813  13 942 822 613  21 849 558 546 284 406 603 889  25 882 455 619 629\n",
      " 840 195 393 610 265 549  65 519 645 512 858 697 855 938 533 805 808 253\n",
      " 749 685 758 155 812 911 118 823 328 551  24 712 568 213 571  57 422 157\n",
      " 833 329 159 279 370 623 100 181 170 941 411 700 710 924 377 871 581 315\n",
      "   8  26 714 348  23 371 575 738 472 626 133  20 590 220 353 892 728 136\n",
      " 317 453 930 809 346 442 913 542 500  66  32 320 309 268 172 576 881 239\n",
      " 423 794 591 362 313 652 521 916 535 739 863 108 784 918 459 884 662 351\n",
      " 770 389  78 303 497 160 419 780   4 604 115 951  36 180 429 634  50 740\n",
      " 403  44 151  88 577 238 305 653 456  84 642 292 616 227 319 258 669 247\n",
      " 257 903 888  64 759 896 584 793 397 415 821 175 680 179  95 119 380 804\n",
      " 485 256 561 715 147  60  72 490 922 236  54 532 364 636 900 206  63 324\n",
      " 708 602 190 902 923 229 713 688 791 720 414 507 852 447 737 682 647 183\n",
      " 386  52 658 752 402 448  86 807 361  90 565  61 263 709 477 340 570 886\n",
      " 395 649 600 221 203 426 378 912 185 259  67  56 323 814  49  83 142   7\n",
      " 837  91 723 331 124 275 400 705 473  55 373 383 880 433  29 128 392 163\n",
      " 806 756 627 452 428 177 237 298 491 111 116 866 635 449 454  10 356 387\n",
      " 193 509 925 651 511 650 382 287 342 641 314 929 940 492 251 698 919 269\n",
      " 935 672  14 399  99 638 587 676 109 586  51 250 499 339 121 775 660 234\n",
      " 232 114 461 944 743 174 344 156  11 646 381 639 474 895 894 359 936 618\n",
      " 827 624 779  27 357 194 285  39  98 407 829 347 585 208  16 149 609 703\n",
      "  41 273 106 241 503 304 289 465 226 420 367 640 481 557 228 376 617 197\n",
      " 112 909 781 562 153 699 795 418 730 751 706  69 755 137 281  89  47 541\n",
      " 671 493 579 754 567 811 294 842 742 148 776 388 417 838 182 475]\n",
      "Test:  [354 105 659 594  12 826   1 110 384 769 334 495 630 271 910  74 255 540\n",
      " 761 621 832 391 480 772 498  62 360 862 219 949 777 266 633 608 859  59\n",
      " 432 408 797 261 810 139 401 950 207 877 531 864 574 252 166 451 553 687\n",
      " 295 628 325 656 787 538 654  37 504 299 293 733 856 598 471 726 505  38\n",
      " 599 527  43 933 927 230 296 844 872 534 479 721 874 556  42 694 316 161\n",
      " 424 327 107 803 270 416 677 891 138 788 597 467 908 785 782  76 890 607\n",
      " 283 768 162  22 398 569 937 724 349 164 762 583 363 606 554  85 214 404\n",
      " 905 670 869  48 860 187 158 893 117 851 926 774 113 678 792 957 171 425\n",
      " 898 209 735 857 836 667 198 745 691 421 231 572 819 718 547 766 907 771\n",
      " 126 310 901 545 463 673 390 355 914 588 904 816 763 736 757 765 333 566\n",
      " 615 592 337 675 847 245  87 143 543 683 536 802 326 202 841 369 605 530\n",
      " 286 262 839 358 318 947 379 222 440  15 476 879 308 343 693 800 719 438\n",
      " 225 741 692 790 582 494  75 478 176 690 861 335 517 306 885 173 778 483\n",
      " 945 722 801  80 717 196]\n",
      "Train: [952 747 224 920 487 550 291 365   0 539 374 469  30 135 288 760 725 746\n",
      " 248 943 906 614 254 524 544 520 439 277 311 783 820 192 817 612 260 681\n",
      " 122 410 668  34  73 278 413 731 828 345 506 948 773 939 931 522 242 368\n",
      " 515 830 732 846 152 873 883 199 824 664 184 596 145 131 665 246 631  81\n",
      " 235 689 674   6 168 648 302 125 409 240 799 695 625 529 798 932 332 786\n",
      " 753 528 711 169 322  70 434 663 372  58 589 301 300 466  19 123 141 953\n",
      " 460   2 767 127  17 707  28 848 704 878 564 130 204  97 249 312 366  40\n",
      " 716 450 290 104 431  94 394 272 501 525 212 513  31 129 887 518 457 865\n",
      " 727 684 405 601 563 338 489 436 215 637  68 146 578 223  71 462 657 956\n",
      " 655 764 341 443 510 441 350 144 666 243 548 928 484 555 444 482 946 701\n",
      " 430 216 867 729 526  45 831 186  46 200 843 437 702 921 488 496 218 818\n",
      " 868  92 875 352 899 573 954 280 825 468 620 336 205 643 835 876 696 101\n",
      " 178 140 132  53 834 217 486 502 191 321 396 165 446 464 274 593 167 470\n",
      " 102 915 934 560 514 354 105 659 594  12 826   1 110 384 769 334 495 630\n",
      " 271 910  74 255 540 761 621 832 391 480 772 498  62 360 862 219 949 777\n",
      " 266 633 608 859  59 432 408 797 261 810 139 401 950 207 877 531 864 574\n",
      " 252 166 451 553 687 295 628 325 656 787 538 654  37 504 299 293 733 856\n",
      " 598 471 726 505  38 599 527  43 933 927 230 296 844 872 534 479 721 874\n",
      " 556  42 694 316 161 424 327 107 803 270 416 677 891 138 788 597 467 908\n",
      " 785 782  76 890 607 283 768 162  22 398 569 937 724 349 164 762 583 363\n",
      " 606 554  85 214 404 905 670 869  48 860 187 158 893 117 851 926 774 113\n",
      " 678 792 957 171 425 898 209 735 857 836 667 198 745 691 421 231 572 819\n",
      " 718 547 766 907 771 126 310 901 545 463 673 390 355 914 588 904 816 763\n",
      " 736 757 765 333 566 615 592 337 675 847 245  87 143 543 683 536 802 326\n",
      " 202 841 369 605 530 286 262 839 358 318 947 379 222 440  15 476 879 308\n",
      " 343 693 800 719 438 225 741 692 790 582 494  75 478 176 690 861 335 517\n",
      " 306 885 173 778 483 945 722 801  80 717 821 175 680 179  95 119 380 804\n",
      " 485 256 561 715 147  60  72 490 922 236  54 532 364 636 900 206  63 324\n",
      " 708 602 190 902 923 229 713 688 791 720 414 507 852 447 737 682 647 183\n",
      " 386  52 658 752 402 448  86 807 361  90 565  61 263 709 477 340 570 886\n",
      " 395 649 600 221 203 426 378 912 185 259  67  56 323 814  49  83 142   7\n",
      " 837  91 723 331 124 275 400 705 473  55 373 383 880 433  29 128 392 163\n",
      " 806 756 627 452 428 177 237 298 491 111 116 866 635 449 454  10 356 387\n",
      " 193 509 925 651 511 650 382 287 342 641 314 929 940 492 251 698 919 269\n",
      " 935 672  14 399  99 638 587 676 109 586  51 250 499 339 121 775 660 234\n",
      " 232 114 461 944 743 174 344 156  11 646 381 639 474 895 894 359 936 618\n",
      " 827 624 779  27 357 194 285  39  98 407 829 347 585 208  16 149 609 703\n",
      "  41 273 106 241 503 304 289 465 226 420 367 640 481 557 228 376 617 197\n",
      " 112 909 781 562 153 699 795 418 730 751 706  69 755 137 281  89  47 541\n",
      " 671 493 579 754 567 811 294 842 742 148 776 388 417 838 182 196 475]\n",
      "Test:  [188  82 870  33 427 385 611 297   9 559  93 134 897 150 282 595  18 679\n",
      " 276 955 661 854 508 264 330 435 120 552 154 580  96 211   3 815 445 537\n",
      " 744 917 845 750  79 644  77 796 516 233 632 412 307 622 523   5 853 734\n",
      " 458 244 789 375 267  35 210 748 103 189 850 686 201 813  13 942 822 613\n",
      "  21 849 558 546 284 406 603 889  25 882 455 619 629 840 195 393 610 265\n",
      " 549  65 519 645 512 858 697 855 938 533 805 808 253 749 685 758 155 812\n",
      " 911 118 823 328 551  24 712 568 213 571  57 422 157 833 329 159 279 370\n",
      " 623 100 181 170 941 411 700 710 924 377 871 581 315   8  26 714 348  23\n",
      " 371 575 738 472 626 133  20 590 220 353 892 728 136 317 453 930 809 346\n",
      " 442 913 542 500  66  32 320 309 268 172 576 881 239 423 794 591 362 313\n",
      " 652 521 916 535 739 863 108 784 918 459 884 662 351 770 389  78 303 497\n",
      " 160 419 780   4 604 115 951  36 180 429 634  50 740 403  44 151  88 577\n",
      " 238 305 653 456  84 642 292 616 227 319 258 669 247 257 903 888  64 759\n",
      " 896 584 793 397 415]\n",
      "Train: [952 747 224 920 487 550 291 365   0 539 374 469  30 135 288 760 725 746\n",
      " 248 943 906 614 254 524 544 520 439 277 311 783 820 192 817 612 260 681\n",
      " 122 410 668  34  73 278 413 731 828 345 506 948 773 939 931 522 242 368\n",
      " 515 830 732 846 152 873 883 199 824 664 184 596 145 131 665 246 631  81\n",
      " 235 689 674   6 168 648 302 125 409 240 799 695 625 529 798 932 332 786\n",
      " 753 528 711 169 322  70 434 663 372  58 589 301 300 466  19 123 141 953\n",
      " 460   2 767 127  17 707  28 848 704 878 564 130 204  97 249 312 366  40\n",
      " 716 450 290 104 431  94 394 272 501 525 212 513  31 129 887 518 457 865\n",
      " 727 684 405 601 563 338 489 436 215 637  68 146 578 223  71 462 657 956\n",
      " 655 764 341 443 510 441 350 144 666 243 548 928 484 555 444 482 946 701\n",
      " 430 216 867 729 526  45 831 186  46 200 843 437 702 921 488 496 218 818\n",
      " 868  92 875 352 899 573 954 280 825 468 620 336 205 643 835 876 696 101\n",
      " 178 140 132  53 834 217 486 502 191 321 396 165 446 464 274 593 167 470\n",
      " 102 915 934 560 514 354 105 659 594  12 826   1 110 384 769 334 495 630\n",
      " 271 910  74 255 540 761 621 832 391 480 772 498  62 360 862 219 949 777\n",
      " 266 633 608 859  59 432 408 797 261 810 139 401 950 207 877 531 864 574\n",
      " 252 166 451 553 687 295 628 325 656 787 538 654  37 504 299 293 733 856\n",
      " 598 471 726 505  38 599 527  43 933 927 230 296 844 872 534 479 721 874\n",
      " 556  42 694 316 161 424 327 107 803 270 416 677 891 138 788 597 467 908\n",
      " 785 782  76 890 607 283 768 162  22 398 569 937 724 349 164 762 583 363\n",
      " 606 554  85 214 404 905 670 869  48 860 187 158 893 117 851 926 774 113\n",
      " 678 792 957 171 425 898 209 735 857 836 667 198 745 691 421 231 572 819\n",
      " 718 547 766 907 771 126 310 901 545 463 673 390 355 914 588 904 816 763\n",
      " 736 757 765 333 566 615 592 337 675 847 245  87 143 543 683 536 802 326\n",
      " 202 841 369 605 530 286 262 839 358 318 947 379 222 440  15 476 879 308\n",
      " 343 693 800 719 438 225 741 692 790 582 494  75 478 176 690 861 335 517\n",
      " 306 885 173 778 483 945 722 801  80 717 188  82 870  33 427 385 611 297\n",
      "   9 559  93 134 897 150 282 595  18 679 276 955 661 854 508 264 330 435\n",
      " 120 552 154 580  96 211   3 815 445 537 744 917 845 750  79 644  77 796\n",
      " 516 233 632 412 307 622 523   5 853 734 458 244 789 375 267  35 210 748\n",
      " 103 189 850 686 201 813  13 942 822 613  21 849 558 546 284 406 603 889\n",
      "  25 882 455 619 629 840 195 393 610 265 549  65 519 645 512 858 697 855\n",
      " 938 533 805 808 253 749 685 758 155 812 911 118 823 328 551  24 712 568\n",
      " 213 571  57 422 157 833 329 159 279 370 623 100 181 170 941 411 700 710\n",
      " 924 377 871 581 315   8  26 714 348  23 371 575 738 472 626 133  20 590\n",
      " 220 353 892 728 136 317 453 930 809 346 442 913 542 500  66  32 320 309\n",
      " 268 172 576 881 239 423 794 591 362 313 652 521 916 535 739 863 108 784\n",
      " 918 459 884 662 351 770 389  78 303 497 160 419 780   4 604 115 951  36\n",
      " 180 429 634  50 740 403  44 151  88 577 238 305 653 456  84 642 292 616\n",
      " 227 319 258 669 247 257 903 888  64 759 896 584 793 397 415 196 475]\n",
      "Test:  [821 175 680 179  95 119 380 804 485 256 561 715 147  60  72 490 922 236\n",
      "  54 532 364 636 900 206  63 324 708 602 190 902 923 229 713 688 791 720\n",
      " 414 507 852 447 737 682 647 183 386  52 658 752 402 448  86 807 361  90\n",
      " 565  61 263 709 477 340 570 886 395 649 600 221 203 426 378 912 185 259\n",
      "  67  56 323 814  49  83 142   7 837  91 723 331 124 275 400 705 473  55\n",
      " 373 383 880 433  29 128 392 163 806 756 627 452 428 177 237 298 491 111\n",
      " 116 866 635 449 454  10 356 387 193 509 925 651 511 650 382 287 342 641\n",
      " 314 929 940 492 251 698 919 269 935 672  14 399  99 638 587 676 109 586\n",
      "  51 250 499 339 121 775 660 234 232 114 461 944 743 174 344 156  11 646\n",
      " 381 639 474 895 894 359 936 618 827 624 779  27 357 194 285  39  98 407\n",
      " 829 347 585 208  16 149 609 703  41 273 106 241 503 304 289 465 226 420\n",
      " 367 640 481 557 228 376 617 197 112 909 781 562 153 699 795 418 730 751\n",
      " 706  69 755 137 281  89  47 541 671 493 579 754 567 811 294 842 742 148\n",
      " 776 388 417 838 182]\n"
     ]
    }
   ],
   "source": [
    "validacion_simple = ValidacionSimple(0.75)\n",
    "validacion_simple.creaParticiones(dataset)\n",
    "v_cruzada = ValidacionCruzada(4)\n",
    "v_cruzada.creaParticiones(dataset)\n",
    "\n",
    "print(\"VALIDACION SIMPLE\",validacion_simple.particiones[0])\n",
    "\n",
    "print(\"VALIDACION CRUZADA\")\n",
    "for particion in v_cruzada.particiones:\n",
    "    print(particion)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[0.18650794, 0.2309322 ],\n",
      "       [0.4484127 , 0.29661017],\n",
      "       [0.36507937, 0.47245763]])\n",
      " array([[0.23809524, 0.29025424],\n",
      "       [0.30555556, 0.375     ],\n",
      "       [0.45634921, 0.33474576]])\n",
      " array([[0.18253968, 0.24788136],\n",
      "       [0.45238095, 0.30720339],\n",
      "       [0.36507937, 0.44491525]])\n",
      " array([[0.23412698, 0.27754237],\n",
      "       [0.28968254, 0.35805085],\n",
      "       [0.47619048, 0.36440678]])\n",
      " array([[0.14285714, 0.16101695],\n",
      "       [0.57539683, 0.24576271],\n",
      "       [0.28174603, 0.59322034]])\n",
      " array([[0.25      , 0.28601695],\n",
      "       [0.32539683, 0.35169492],\n",
      "       [0.42460317, 0.36228814]])\n",
      " array([[0.19444444, 0.22669492],\n",
      "       [0.4484127 , 0.3029661 ],\n",
      "       [0.35714286, 0.47033898]])\n",
      " array([[0.25793651, 0.28389831],\n",
      "       [0.27380952, 0.36652542],\n",
      "       [0.46825397, 0.34957627]])\n",
      " array([[0.20634921, 0.2309322 ],\n",
      "       [0.41269841, 0.28177966],\n",
      "       [0.38095238, 0.48728814]])]\n",
      "[array([[0.20233463, 0.22269807],\n",
      "       [0.43579767, 0.30835118],\n",
      "       [0.3618677 , 0.46895075]])\n",
      " array([[0.22957198, 0.26766595],\n",
      "       [0.29182879, 0.37044968],\n",
      "       [0.47859922, 0.36188437]])\n",
      " array([[0.18677043, 0.22912206],\n",
      "       [0.43190661, 0.29764454],\n",
      "       [0.38132296, 0.4732334 ]])\n",
      " array([[0.22178988, 0.28051392],\n",
      "       [0.29571984, 0.36830835],\n",
      "       [0.48249027, 0.35117773]])\n",
      " array([[0.13229572, 0.17987152],\n",
      "       [0.58754864, 0.25053533],\n",
      "       [0.28015564, 0.56959315]])\n",
      " array([[0.22178988, 0.27408994],\n",
      "       [0.30350195, 0.3640257 ],\n",
      "       [0.47470817, 0.36188437]])\n",
      " array([[0.18287938, 0.24197002],\n",
      "       [0.46303502, 0.27408994],\n",
      "       [0.3540856 , 0.48394004]])\n",
      " array([[0.24902724, 0.28265525],\n",
      "       [0.307393  , 0.37259101],\n",
      "       [0.44357977, 0.34475375]])\n",
      " array([[0.20233463, 0.19914347],\n",
      "       [0.45136187, 0.30835118],\n",
      "       [0.3463035 , 0.49250535]])]\n",
      "[array([[0.19915254, 0.21721311],\n",
      "       [0.41525424, 0.29303279],\n",
      "       [0.38559322, 0.4897541 ]])\n",
      " array([[0.23305085, 0.27254098],\n",
      "       [0.30508475, 0.36270492],\n",
      "       [0.46186441, 0.3647541 ]])\n",
      " array([[0.18644068, 0.22540984],\n",
      "       [0.44491525, 0.30737705],\n",
      "       [0.36864407, 0.46721311]])\n",
      " array([[0.22457627, 0.2807377 ],\n",
      "       [0.3220339 , 0.34836066],\n",
      "       [0.45338983, 0.37090164]])\n",
      " array([[0.13983051, 0.17418033],\n",
      "       [0.58050847, 0.23155738],\n",
      "       [0.27966102, 0.5942623 ]])\n",
      " array([[0.24152542, 0.28278689],\n",
      "       [0.32627119, 0.35655738],\n",
      "       [0.43220339, 0.36065574]])\n",
      " array([[0.21186441, 0.22540984],\n",
      "       [0.40677966, 0.32377049],\n",
      "       [0.38135593, 0.45081967]])\n",
      " array([[0.23728814, 0.2807377 ],\n",
      "       [0.33474576, 0.38114754],\n",
      "       [0.4279661 , 0.33811475]])\n",
      " array([[0.16949153, 0.24795082],\n",
      "       [0.41949153, 0.29508197],\n",
      "       [0.41101695, 0.45696721]])]\n",
      "[array([[0.1796875 , 0.2238806 ],\n",
      "       [0.43359375, 0.30916844],\n",
      "       [0.38671875, 0.46695096]])\n",
      " array([[0.23828125, 0.2771855 ],\n",
      "       [0.31640625, 0.36886994],\n",
      "       [0.4453125 , 0.35394456]])\n",
      " array([[0.1875    , 0.22814499],\n",
      "       [0.45703125, 0.30277186],\n",
      "       [0.35546875, 0.46908316]])\n",
      " array([[0.25      , 0.28571429],\n",
      "       [0.28515625, 0.37526652],\n",
      "       [0.46484375, 0.33901919]])\n",
      " array([[0.16796875, 0.18976546],\n",
      "       [0.5703125 , 0.2238806 ],\n",
      "       [0.26171875, 0.58635394]])\n",
      " array([[0.2421875 , 0.27292111],\n",
      "       [0.2890625 , 0.3901919 ],\n",
      "       [0.46875   , 0.33688699]])\n",
      " array([[0.18359375, 0.2238806 ],\n",
      "       [0.4375    , 0.29637527],\n",
      "       [0.37890625, 0.47974414]])\n",
      " array([[0.21875   , 0.26226013],\n",
      "       [0.2890625 , 0.34754797],\n",
      "       [0.4921875 , 0.3901919 ]])\n",
      " array([[0.2109375 , 0.22601279],\n",
      "       [0.45703125, 0.29424307],\n",
      "       [0.33203125, 0.47974414]])]\n",
      "[array([[0.18532819, 0.24678112],\n",
      "       [0.46718147, 0.29828326],\n",
      "       [0.34749035, 0.45493562]])\n",
      " array([[0.24324324, 0.2832618 ],\n",
      "       [0.30501931, 0.36051502],\n",
      "       [0.45173745, 0.35622318]])\n",
      " array([[0.2046332 , 0.22746781],\n",
      "       [0.42084942, 0.30042918],\n",
      "       [0.37451737, 0.472103  ]])\n",
      " array([[0.24710425, 0.25321888],\n",
      "       [0.31660232, 0.37124464],\n",
      "       [0.43629344, 0.37553648]])\n",
      " array([[0.14671815, 0.17596567],\n",
      "       [0.56370656, 0.24248927],\n",
      "       [0.28957529, 0.58154506]])\n",
      " array([[0.23938224, 0.27038627],\n",
      "       [0.3011583 , 0.35193133],\n",
      "       [0.45945946, 0.3776824 ]])\n",
      " array([[0.18918919, 0.21888412],\n",
      "       [0.44401544, 0.31330472],\n",
      "       [0.36679537, 0.46781116]])\n",
      " array([[0.23938224, 0.27467811],\n",
      "       [0.28957529, 0.36051502],\n",
      "       [0.47104247, 0.36480687]])\n",
      " array([[0.18146718, 0.2360515 ],\n",
      "       [0.42471042, 0.3111588 ],\n",
      "       [0.39382239, 0.4527897 ]])]\n",
      "El error del Clasificador NaiveBayes  para Validacion Simple es: 0.29583333333333334\n",
      "El error del Clasificador NaiveBayes para Validacion Cruzada es: 0.3047942817294282\n"
     ]
    }
   ],
   "source": [
    "clasificador = ClasificadorNaiveBayes(True)\n",
    "#clasificador.entrenamiento(dataset, validacion_simple.particiones[0].indicesTrain)\n",
    "#clasificador.clasifica(dataset, validacion_simple.particiones[0].indicesTest)\n",
    "error = clasificador.validacion(validacion_simple,dataset,clasificador)\n",
    "error1 = clasificador.validacion(v_cruzada,dataset,clasificador)\n",
    "print(\"El error del Clasificador NaiveBayes  para Validacion Simple es: \"+str(error))\n",
    "print(\"El error del Clasificador NaiveBayes para Validacion Cruzada es: \"+str(error1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A1', 'A2', 'A3', 'A4', 'A5', 'A6', 'A7', 'A8', 'A9', 'A10', 'A11', 'A12', 'A13', 'A14', 'A15', 'A16', 'A17', 'A18', 'A19', 'A20', 'Class']\n",
      "[{'A11': 0, 'A12': 1, 'A13': 2, 'A14': 3}, {}, {'A30': 0, 'A31': 1, 'A32': 2, 'A33': 3, 'A34': 4}, {'A40': 0, 'A41': 1, 'A410': 2, 'A42': 3, 'A43': 4, 'A44': 5, 'A45': 6, 'A46': 7, 'A48': 8, 'A49': 9}, {}, {'A61': 0, 'A62': 1, 'A63': 2, 'A64': 3, 'A65': 4}, {'A71': 0, 'A72': 1, 'A73': 2, 'A74': 3, 'A75': 4}, {}, {'A91': 0, 'A92': 1, 'A93': 2, 'A94': 3}, {'A101': 0, 'A102': 1, 'A103': 2}, {}, {'A121': 0, 'A122': 1, 'A123': 2, 'A124': 3}, {}, {'A141': 0, 'A142': 1, 'A143': 2}, {'A151': 0, 'A152': 1, 'A153': 2}, {}, {'A171': 0, 'A172': 1, 'A173': 2, 'A174': 3}, {}, {'A191': 0, 'A192': 1}, {'A201': 0, 'A202': 1}, {'1': 0, '2': 1}]\n",
      "[[ 0.  6.  4. ...  1.  0.  0.]\n",
      " [ 1. 48.  2. ...  0.  0.  1.]\n",
      " [ 3. 12.  4. ...  0.  0.  0.]\n",
      " ...\n",
      " [ 3. 12.  2. ...  0.  0.  0.]\n",
      " [ 0. 45.  2. ...  1.  0.  1.]\n",
      " [ 1. 45.  4. ...  0.  0.  0.]]\n",
      "[True, False, True, True, False, True, True, False, True, True, False, True, False, True, True, False, True, False, True, True, True]\n"
     ]
    }
   ],
   "source": [
    "dataset = Datos('german.data')\n",
    "print(dataset.datos)\n",
    "print(dataset.nominalAtributos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VALIDACION SIMPLE Train: [502 525  37 217 516 965 329 374 376 569 949 597 570   5 736 731 723 894\n",
      " 183 236 700 855  26 489 343 201 575  70 408 585 291 465  86 670 459 637\n",
      " 116 933 598 881 558 352   3 791 767 806 177 645  58 580 146 815 105 475\n",
      " 684  97 388 760 797 451 192 646 606  89 544 954 500  79 989 437 152 278\n",
      " 244 113 907   0 803 608 415 540 103 972 357 286 583 441 187 834 134 397\n",
      " 662 510 538 215 230 749 321 828  94 986 141 810 922  80 622  77 928 340\n",
      " 795 243 969 713 730 872 497 735 211 156 672 420 378  11 689 957 638 181\n",
      " 883   8 798 660 462 285 207 800 276 313  38 847 171 661  67 801 263 657\n",
      " 424  34 189 366  66 213 859 191 830 246 157 296 621 545 384 571 457 813\n",
      " 618 522 964 365 368  63 229 394 382 118 160 979 763 197 601 879 746 556\n",
      " 390  25 535 951   4 262  90 861 467 454 654 322 123 687 565 242 712 419\n",
      " 783 863 870 326 180  42  96 317 316  10 876 182 991 936 929 941 707 305\n",
      " 519 747 482 284 473 188 288 169 417 396 526 423 935 327 849 899 690 945\n",
      " 766 145 605 914 464 513 149 858 274 640 506 839 124 444 997 628 551 998\n",
      " 600 572 280 586 588 593 178  61 924 958 770   9 463 596 717 114 269 789\n",
      " 658  22 109  59 126  20 491 974 301 372  47 144 999 633 512 627 133 504\n",
      " 651 812 777  84 753  82 873 158 101 331 225  92 541 617  73 694 440 139\n",
      " 743 869 530 306 515 380 375 714 963 240 170 844 614 421 391 584  60 771\n",
      " 738 856 228 568 632 442 925 143 629 874 631 461 788 360  32 142 889 110\n",
      " 470 549 754 485  75  76 968 885 381 275 472 400 469 981 498 635 435 413\n",
      " 686 153 312 260 258 518 673 371  62 792 546 882 762 299  53 398 890 484\n",
      " 245  19 727 942 610 216 903 857 453 752 591 675 524 977 921 952 356 814\n",
      " 748 418 838 487 150 241 370 875 167 910 862 793 836 576 805 410 559 587\n",
      " 520 493 486  54 799  49 455 757 323 864 523 402 289 728 819 649 708 298\n",
      " 594 129 970 399 776 266 944 995 665 438 561 563 781 164 681 501 521 674\n",
      " 659 668 919 219 831 774 567 529 277 768 891 642 346 309 978 128 562 691\n",
      " 775 616 361 190 297 611 449 715 817 471 683 867  44  15 908 685 647 548\n",
      " 808 826 185 916 310 364 107 488 630 644 147 722 282 599 358 490 195 892\n",
      " 175 268 324 902 822 613 206 250 450 341 163 804 973 257 121  52 742 379\n",
      " 479 247 517  43 279 335 159  24 811 336 460 509 779 577 248 272 325 710\n",
      " 412 904  14 595 939 234 267 696 641 653 940 328 403 655 319 353 729 271\n",
      " 532 429 993 744 825  36 764 579 937  69 669 173 833 666 716 848 527 915\n",
      " 582 886 680 342 179 832 514 221 425 338 209 923 888 987 106 994 104 122\n",
      " 931 821 431 897 877 300 290 679 130 135 589 918 893 100 790  68 389 476\n",
      " 695 643 249  46 239 483 148 948 507 320 393 264  23 354  29 111 468 383\n",
      " 851  56 251 386 223 293 692 943 725 976 953 911 677 745 759 720 868 496\n",
      " 265 934  98 385  81 193 477  48 387 854 664 447 769 161 112  13  28 574\n",
      " 756 636 505 966  91 896 210 478 363 845 555 359 446 404 620 137 557 721\n",
      " 984 652 108  51 503 311 283 871 905 688 395 906 426 704 255 117 433  95\n",
      " 895 719 837 853 184 751 676 992 428  64 718   1 256 369 203 796 333 534\n",
      "  17 212  93   2 166 198 912 709 550 552 199 678 961 959 842 553 536 373\n",
      " 962 281 367 901 235 339 436 445 724 237 955 162]\n",
      "Test:  [176 155 492 765 273 603  35 345 560 841 186 238 983 456 913 434 315 884\n",
      " 466 878 332 377 737 204 930 667 427 174 448 772 947 761 732 287  83 711\n",
      " 865 706 956  31 138  27 741 537 900 702 120 318 623 132 609 850 102 222\n",
      " 495 698 566  40 547 648 920   7 350  57 208  41 254 740 852 967 337 220\n",
      " 996 430 334 950  18  45 866 508  50 406 119 511 125 909 392 233 533 307\n",
      " 699 639 227 840 927 615 802 411 663 292 693 612 818 499 843 932 416 975\n",
      " 165 697 401 703 168 355 980 494  71  88 304  99  55 303 344 624 807 294\n",
      " 202 607 539 480 625  39  74 308 785 592 226 705 809 362 626 542 443 773\n",
      " 452 194 604 127 474 196 758 528  33 131  65 270 656 543 314 682 701 990\n",
      " 786  12 531 827 860 261 232 982 259 602  16 330 784  30 824 458 140 224\n",
      "  72   6 829 846 734 151 348 578 787 750  21 794  78  85 422 739 780 988\n",
      " 650 409 218  87 778 671 823 938 835 351 971 115 887 820 295 432 926 726\n",
      " 946 172 302 733 347 898 439 554 205 985 960 782 755 349 252 481 253 407\n",
      " 405 231 619 917 136 214 414 634 581 200 154 564 880 590 816 573]\n",
      "VALIDACION CRUZADA\n",
      "Train: [680 229 444 555 772 684 613 634 925 455 398 123 857 961 278 570 906 504\n",
      " 316 780 828 744 803 460 100 234 439 709 122 242 148 562 936 514 176 289\n",
      " 356  47 106 778 692 456 133 411 843 927 863 304 790 203 149 923 596 814\n",
      " 888 147 855 907  56 317 391 997 127 188 569 646 180 838 301  64  59 152\n",
      " 522 489 509 760 462 714 650 951  11 952  25 560 866 960 166  88 169 774\n",
      " 478 992 844 589 490 351  81  44 437 648 554 754 481 597 698  92 576 190\n",
      " 660 950 607 172 751 638 306 498 731 674 669 183 713 651  77  22 564 546\n",
      " 162 994 246 919 598 556 461 448 302 132 742 515 473 421 529 506 656 770\n",
      " 449 288 661 799 902 413 901 758 882 757 768  35 161 204 119 363  72 307\n",
      " 474 787 868 775 141   0  30 663  31 273 971 349 225 201 600 486 869  39\n",
      " 875 430 387 199 294 308 957 707 470 753 458 368 524 640 577 727 404 989\n",
      " 550 584 847 441  43 341 521 412 893 428   8 548 277 602 885 237 878 711\n",
      " 829 832 223 262 841 977 361 637 929  17 867 450 315 282 980 373 189 949\n",
      " 970 254 339 416 268 124 103 614 493 642 217 633 345 617 845 798 314 699\n",
      " 291   9 747 500 776 850 109 429  76  20 708 305 748 442  49 624 619 563\n",
      " 587 689 528 126 221 606 257 860 649 482 670 240  79 704 683 947 275 782\n",
      " 454 244 216  12 922 678 110 256 687 333 365 447 671 319 417 616 734  45\n",
      "  96 390 728 654 966 377 756  28 956 241 590 385 255 615 469  86 184 209\n",
      " 691 547 986 685 452 468 338 767 849 479 807 113 200 811 628 604 574 328\n",
      " 647 494 227 212 357 335 279 655  94 114 808 941  29 236 230  54 378 371\n",
      " 720 895 697 955 583 745  58 591 703 140 533 608 586 839 573 186 151 534\n",
      " 686 331 561 599 251 400 290 174 104 272 610 379 926 168 766 187 243 202\n",
      " 865  65 629 431  16 115 353 940 139 463 376 609 676 723  32   2 822 342\n",
      " 694 507 382 969 732  98 343 636 173 359 107 916 372 899 197 566 344 631\n",
      " 743  78 360 467 931 938 198 582 817 908 420 340  36 764 443 195 354 414\n",
      " 334 942 426 352 877 666 516 988 896 862 972 311 904  15 102 388 870 593\n",
      " 134 759 737 535 222 213 581 894 137 794 207 603 477 783 536 567 410  52\n",
      " 733 248 580 565 890 944 214 287 987 665 299 408 163 657 401  57 781 819\n",
      " 131 484 939  55 485 508  99  97 879 158 348 153 975  34 724 121 160 876\n",
      " 812 395 144 525 310 419 898 910 886 182 539 804 881 466 802 621 322 303\n",
      " 232 472 945 252 827 538 142 320 265 900  14  42 861 769 990 312 405 823\n",
      " 329  26 518 476 953 984 973 962 432 208 777 245 858 505 403  50 545 889\n",
      "  71 639 260 846 833 205 912 541 502 112 276 231 537 722 389  41 851 418\n",
      " 825 948 675 325 542 995 191 892   1 920 667 810 954 903 116   7 471 374\n",
      " 982 681 167 688 434 716  33 837 821   3 327 269 965  70 362  89 157 178\n",
      "  13 618 179  67 111 459 543 571 605 393 530 652 601 806   6 645 784 101\n",
      " 913 932 761 626 274 155 211 611  80 991 125 120 226 440 261 864 513 809\n",
      " 937 796  46  62 557  63 145 715 423 164 579 741 519 726 130 350 830 749\n",
      " 998 370 915 641 235 464 930 171 185 717 746 859 856 985 228 531 831 672\n",
      " 517 117 706 824 520 996 523  95 427 129 135 791 625 612 653 740 729 739\n",
      " 658 297 258 154 259 933 730 380 346 355 924  48  93 718  69 266 510 934\n",
      " 765 738 406 165 386  61 264 280  40 220 233 143]\n",
      "Test:  [526 318 964 887  51 979 156 795 967 438 911 852 383  83 453 664  68 786\n",
      " 253 705 592 559 435 595 752 682 192 981 394 553 792 797 177 238 532 407\n",
      "  10 267 285 588 659 818 321 572 392 323 136 512 415 332 309 702 999 146\n",
      " 425 905 284 292  38 422 884 968 848 181  85 755 789 993 457 935 585  23\n",
      " 793 943 835 358 206 108 578 880 921 247 501   5 872 914 710 436  53 336\n",
      " 552 239 446 762 480 959 296 300 834 679  21 736 210  37 402 826 399 364\n",
      " 549 673 465 475  19 492 293  66 375 788 816 662 897 487 677 805 381 138\n",
      " 369 963 326 928 801 854  24 218 337 397 701 978 295 883 270 594 445 946\n",
      " 503 568 511 527 800 909 976 313   4 150 836 632 105 215 871 170 891 540\n",
      " 974 496  84  87 820 874 700 721 630 263 271 958 644 779 690 286 491 668\n",
      " 330 983 719  91 750 298 635 396  73 499 367 917 433  27 175  82 451 424\n",
      "  60 409  75 696 643 118 324 853 735 842 623 224 366 483 219 575 771 815\n",
      " 725 873 785 497 128 693 918 347 763 773  74 488 193 620 159 622 495  18\n",
      " 281 283  90 249 194 695 558 196 544 250 813 551 840 712 384 627]\n",
      "Train: [526 318 964 887  51 979 156 795 967 438 911 852 383  83 453 664  68 786\n",
      " 253 705 592 559 435 595 752 682 192 981 394 553 792 797 177 238 532 407\n",
      "  10 267 285 588 659 818 321 572 392 323 136 512 415 332 309 702 999 146\n",
      " 425 905 284 292  38 422 884 968 848 181  85 755 789 993 457 935 585  23\n",
      " 793 943 835 358 206 108 578 880 921 247 501   5 872 914 710 436  53 336\n",
      " 552 239 446 762 480 959 296 300 834 679  21 736 210  37 402 826 399 364\n",
      " 549 673 465 475  19 492 293  66 375 788 816 662 897 487 677 805 381 138\n",
      " 369 963 326 928 801 854  24 218 337 397 701 978 295 883 270 594 445 946\n",
      " 503 568 511 527 800 909 976 313   4 150 836 632 105 215 871 170 891 540\n",
      " 974 496  84  87 820 874 700 721 630 263 271 958 644 779 690 286 491 668\n",
      " 330 983 719  91 750 298 635 396  73 499 367 917 433  27 175  82 451 424\n",
      "  60 409  75 696 643 118 324 853 735 842 623 224 366 483 219 575 771 815\n",
      " 725 873 785 497 128 693 918 347 763 773  74 488 193 620 159 622 495  18\n",
      " 281 283  90 249 194 695 558 196 544 250 813 551 840 712 384 627 314 699\n",
      " 291   9 747 500 776 850 109 429  76  20 708 305 748 442  49 624 619 563\n",
      " 587 689 528 126 221 606 257 860 649 482 670 240  79 704 683 947 275 782\n",
      " 454 244 216  12 922 678 110 256 687 333 365 447 671 319 417 616 734  45\n",
      "  96 390 728 654 966 377 756  28 956 241 590 385 255 615 469  86 184 209\n",
      " 691 547 986 685 452 468 338 767 849 479 807 113 200 811 628 604 574 328\n",
      " 647 494 227 212 357 335 279 655  94 114 808 941  29 236 230  54 378 371\n",
      " 720 895 697 955 583 745  58 591 703 140 533 608 586 839 573 186 151 534\n",
      " 686 331 561 599 251 400 290 174 104 272 610 379 926 168 766 187 243 202\n",
      " 865  65 629 431  16 115 353 940 139 463 376 609 676 723  32   2 822 342\n",
      " 694 507 382 969 732  98 343 636 173 359 107 916 372 899 197 566 344 631\n",
      " 743  78 360 467 931 938 198 582 817 908 420 340  36 764 443 195 354 414\n",
      " 334 942 426 352 877 666 516 988 896 862 972 311 904  15 102 388 870 593\n",
      " 134 759 737 535 222 213 581 894 137 794 207 603 477 783 536 567 410  52\n",
      " 733 248 580 565 890 944 214 287 987 665 299 408 163 657 401  57 781 819\n",
      " 131 484 939  55 485 508  99  97 879 158 348 153 975  34 724 121 160 876\n",
      " 812 395 144 525 310 419 898 910 886 182 539 804 881 466 802 621 322 303\n",
      " 232 472 945 252 827 538 142 320 265 900  14  42 861 769 990 312 405 823\n",
      " 329  26 518 476 953 984 973 962 432 208 777 245 858 505 403  50 545 889\n",
      "  71 639 260 846 833 205 912 541 502 112 276 231 537 722 389  41 851 418\n",
      " 825 948 675 325 542 995 191 892   1 920 667 810 954 903 116   7 471 374\n",
      " 982 681 167 688 434 716  33 837 821   3 327 269 965  70 362  89 157 178\n",
      "  13 618 179  67 111 459 543 571 605 393 530 652 601 806   6 645 784 101\n",
      " 913 932 761 626 274 155 211 611  80 991 125 120 226 440 261 864 513 809\n",
      " 937 796  46  62 557  63 145 715 423 164 579 741 519 726 130 350 830 749\n",
      " 998 370 915 641 235 464 930 171 185 717 746 859 856 985 228 531 831 672\n",
      " 517 117 706 824 520 996 523  95 427 129 135 791 625 612 653 740 729 739\n",
      " 658 297 258 154 259 933 730 380 346 355 924  48  93 718  69 266 510 934\n",
      " 765 738 406 165 386  61 264 280  40 220 233 143]\n",
      "Test:  [680 229 444 555 772 684 613 634 925 455 398 123 857 961 278 570 906 504\n",
      " 316 780 828 744 803 460 100 234 439 709 122 242 148 562 936 514 176 289\n",
      " 356  47 106 778 692 456 133 411 843 927 863 304 790 203 149 923 596 814\n",
      " 888 147 855 907  56 317 391 997 127 188 569 646 180 838 301  64  59 152\n",
      " 522 489 509 760 462 714 650 951  11 952  25 560 866 960 166  88 169 774\n",
      " 478 992 844 589 490 351  81  44 437 648 554 754 481 597 698  92 576 190\n",
      " 660 950 607 172 751 638 306 498 731 674 669 183 713 651  77  22 564 546\n",
      " 162 994 246 919 598 556 461 448 302 132 742 515 473 421 529 506 656 770\n",
      " 449 288 661 799 902 413 901 758 882 757 768  35 161 204 119 363  72 307\n",
      " 474 787 868 775 141   0  30 663  31 273 971 349 225 201 600 486 869  39\n",
      " 875 430 387 199 294 308 957 707 470 753 458 368 524 640 577 727 404 989\n",
      " 550 584 847 441  43 341 521 412 893 428   8 548 277 602 885 237 878 711\n",
      " 829 832 223 262 841 977 361 637 929  17 867 450 315 282 980 373 189 949\n",
      " 970 254 339 416 268 124 103 614 493 642 217 633 345 617 845 798]\n",
      "Train: [526 318 964 887  51 979 156 795 967 438 911 852 383  83 453 664  68 786\n",
      " 253 705 592 559 435 595 752 682 192 981 394 553 792 797 177 238 532 407\n",
      "  10 267 285 588 659 818 321 572 392 323 136 512 415 332 309 702 999 146\n",
      " 425 905 284 292  38 422 884 968 848 181  85 755 789 993 457 935 585  23\n",
      " 793 943 835 358 206 108 578 880 921 247 501   5 872 914 710 436  53 336\n",
      " 552 239 446 762 480 959 296 300 834 679  21 736 210  37 402 826 399 364\n",
      " 549 673 465 475  19 492 293  66 375 788 816 662 897 487 677 805 381 138\n",
      " 369 963 326 928 801 854  24 218 337 397 701 978 295 883 270 594 445 946\n",
      " 503 568 511 527 800 909 976 313   4 150 836 632 105 215 871 170 891 540\n",
      " 974 496  84  87 820 874 700 721 630 263 271 958 644 779 690 286 491 668\n",
      " 330 983 719  91 750 298 635 396  73 499 367 917 433  27 175  82 451 424\n",
      "  60 409  75 696 643 118 324 853 735 842 623 224 366 483 219 575 771 815\n",
      " 725 873 785 497 128 693 918 347 763 773  74 488 193 620 159 622 495  18\n",
      " 281 283  90 249 194 695 558 196 544 250 813 551 840 712 384 627 680 229\n",
      " 444 555 772 684 613 634 925 455 398 123 857 961 278 570 906 504 316 780\n",
      " 828 744 803 460 100 234 439 709 122 242 148 562 936 514 176 289 356  47\n",
      " 106 778 692 456 133 411 843 927 863 304 790 203 149 923 596 814 888 147\n",
      " 855 907  56 317 391 997 127 188 569 646 180 838 301  64  59 152 522 489\n",
      " 509 760 462 714 650 951  11 952  25 560 866 960 166  88 169 774 478 992\n",
      " 844 589 490 351  81  44 437 648 554 754 481 597 698  92 576 190 660 950\n",
      " 607 172 751 638 306 498 731 674 669 183 713 651  77  22 564 546 162 994\n",
      " 246 919 598 556 461 448 302 132 742 515 473 421 529 506 656 770 449 288\n",
      " 661 799 902 413 901 758 882 757 768  35 161 204 119 363  72 307 474 787\n",
      " 868 775 141   0  30 663  31 273 971 349 225 201 600 486 869  39 875 430\n",
      " 387 199 294 308 957 707 470 753 458 368 524 640 577 727 404 989 550 584\n",
      " 847 441  43 341 521 412 893 428   8 548 277 602 885 237 878 711 829 832\n",
      " 223 262 841 977 361 637 929  17 867 450 315 282 980 373 189 949 970 254\n",
      " 339 416 268 124 103 614 493 642 217 633 345 617 845 798 401  57 781 819\n",
      " 131 484 939  55 485 508  99  97 879 158 348 153 975  34 724 121 160 876\n",
      " 812 395 144 525 310 419 898 910 886 182 539 804 881 466 802 621 322 303\n",
      " 232 472 945 252 827 538 142 320 265 900  14  42 861 769 990 312 405 823\n",
      " 329  26 518 476 953 984 973 962 432 208 777 245 858 505 403  50 545 889\n",
      "  71 639 260 846 833 205 912 541 502 112 276 231 537 722 389  41 851 418\n",
      " 825 948 675 325 542 995 191 892   1 920 667 810 954 903 116   7 471 374\n",
      " 982 681 167 688 434 716  33 837 821   3 327 269 965  70 362  89 157 178\n",
      "  13 618 179  67 111 459 543 571 605 393 530 652 601 806   6 645 784 101\n",
      " 913 932 761 626 274 155 211 611  80 991 125 120 226 440 261 864 513 809\n",
      " 937 796  46  62 557  63 145 715 423 164 579 741 519 726 130 350 830 749\n",
      " 998 370 915 641 235 464 930 171 185 717 746 859 856 985 228 531 831 672\n",
      " 517 117 706 824 520 996 523  95 427 129 135 791 625 612 653 740 729 739\n",
      " 658 297 258 154 259 933 730 380 346 355 924  48  93 718  69 266 510 934\n",
      " 765 738 406 165 386  61 264 280  40 220 233 143]\n",
      "Test:  [314 699 291   9 747 500 776 850 109 429  76  20 708 305 748 442  49 624\n",
      " 619 563 587 689 528 126 221 606 257 860 649 482 670 240  79 704 683 947\n",
      " 275 782 454 244 216  12 922 678 110 256 687 333 365 447 671 319 417 616\n",
      " 734  45  96 390 728 654 966 377 756  28 956 241 590 385 255 615 469  86\n",
      " 184 209 691 547 986 685 452 468 338 767 849 479 807 113 200 811 628 604\n",
      " 574 328 647 494 227 212 357 335 279 655  94 114 808 941  29 236 230  54\n",
      " 378 371 720 895 697 955 583 745  58 591 703 140 533 608 586 839 573 186\n",
      " 151 534 686 331 561 599 251 400 290 174 104 272 610 379 926 168 766 187\n",
      " 243 202 865  65 629 431  16 115 353 940 139 463 376 609 676 723  32   2\n",
      " 822 342 694 507 382 969 732  98 343 636 173 359 107 916 372 899 197 566\n",
      " 344 631 743  78 360 467 931 938 198 582 817 908 420 340  36 764 443 195\n",
      " 354 414 334 942 426 352 877 666 516 988 896 862 972 311 904  15 102 388\n",
      " 870 593 134 759 737 535 222 213 581 894 137 794 207 603 477 783 536 567\n",
      " 410  52 733 248 580 565 890 944 214 287 987 665 299 408 163 657]\n",
      "Train: [526 318 964 887  51 979 156 795 967 438 911 852 383  83 453 664  68 786\n",
      " 253 705 592 559 435 595 752 682 192 981 394 553 792 797 177 238 532 407\n",
      "  10 267 285 588 659 818 321 572 392 323 136 512 415 332 309 702 999 146\n",
      " 425 905 284 292  38 422 884 968 848 181  85 755 789 993 457 935 585  23\n",
      " 793 943 835 358 206 108 578 880 921 247 501   5 872 914 710 436  53 336\n",
      " 552 239 446 762 480 959 296 300 834 679  21 736 210  37 402 826 399 364\n",
      " 549 673 465 475  19 492 293  66 375 788 816 662 897 487 677 805 381 138\n",
      " 369 963 326 928 801 854  24 218 337 397 701 978 295 883 270 594 445 946\n",
      " 503 568 511 527 800 909 976 313   4 150 836 632 105 215 871 170 891 540\n",
      " 974 496  84  87 820 874 700 721 630 263 271 958 644 779 690 286 491 668\n",
      " 330 983 719  91 750 298 635 396  73 499 367 917 433  27 175  82 451 424\n",
      "  60 409  75 696 643 118 324 853 735 842 623 224 366 483 219 575 771 815\n",
      " 725 873 785 497 128 693 918 347 763 773  74 488 193 620 159 622 495  18\n",
      " 281 283  90 249 194 695 558 196 544 250 813 551 840 712 384 627 680 229\n",
      " 444 555 772 684 613 634 925 455 398 123 857 961 278 570 906 504 316 780\n",
      " 828 744 803 460 100 234 439 709 122 242 148 562 936 514 176 289 356  47\n",
      " 106 778 692 456 133 411 843 927 863 304 790 203 149 923 596 814 888 147\n",
      " 855 907  56 317 391 997 127 188 569 646 180 838 301  64  59 152 522 489\n",
      " 509 760 462 714 650 951  11 952  25 560 866 960 166  88 169 774 478 992\n",
      " 844 589 490 351  81  44 437 648 554 754 481 597 698  92 576 190 660 950\n",
      " 607 172 751 638 306 498 731 674 669 183 713 651  77  22 564 546 162 994\n",
      " 246 919 598 556 461 448 302 132 742 515 473 421 529 506 656 770 449 288\n",
      " 661 799 902 413 901 758 882 757 768  35 161 204 119 363  72 307 474 787\n",
      " 868 775 141   0  30 663  31 273 971 349 225 201 600 486 869  39 875 430\n",
      " 387 199 294 308 957 707 470 753 458 368 524 640 577 727 404 989 550 584\n",
      " 847 441  43 341 521 412 893 428   8 548 277 602 885 237 878 711 829 832\n",
      " 223 262 841 977 361 637 929  17 867 450 315 282 980 373 189 949 970 254\n",
      " 339 416 268 124 103 614 493 642 217 633 345 617 845 798 314 699 291   9\n",
      " 747 500 776 850 109 429  76  20 708 305 748 442  49 624 619 563 587 689\n",
      " 528 126 221 606 257 860 649 482 670 240  79 704 683 947 275 782 454 244\n",
      " 216  12 922 678 110 256 687 333 365 447 671 319 417 616 734  45  96 390\n",
      " 728 654 966 377 756  28 956 241 590 385 255 615 469  86 184 209 691 547\n",
      " 986 685 452 468 338 767 849 479 807 113 200 811 628 604 574 328 647 494\n",
      " 227 212 357 335 279 655  94 114 808 941  29 236 230  54 378 371 720 895\n",
      " 697 955 583 745  58 591 703 140 533 608 586 839 573 186 151 534 686 331\n",
      " 561 599 251 400 290 174 104 272 610 379 926 168 766 187 243 202 865  65\n",
      " 629 431  16 115 353 940 139 463 376 609 676 723  32   2 822 342 694 507\n",
      " 382 969 732  98 343 636 173 359 107 916 372 899 197 566 344 631 743  78\n",
      " 360 467 931 938 198 582 817 908 420 340  36 764 443 195 354 414 334 942\n",
      " 426 352 877 666 516 988 896 862 972 311 904  15 102 388 870 593 134 759\n",
      " 737 535 222 213 581 894 137 794 207 603 477 783 536 567 410  52 733 248\n",
      " 580 565 890 944 214 287 987 665 299 408 163 657]\n",
      "Test:  [401  57 781 819 131 484 939  55 485 508  99  97 879 158 348 153 975  34\n",
      " 724 121 160 876 812 395 144 525 310 419 898 910 886 182 539 804 881 466\n",
      " 802 621 322 303 232 472 945 252 827 538 142 320 265 900  14  42 861 769\n",
      " 990 312 405 823 329  26 518 476 953 984 973 962 432 208 777 245 858 505\n",
      " 403  50 545 889  71 639 260 846 833 205 912 541 502 112 276 231 537 722\n",
      " 389  41 851 418 825 948 675 325 542 995 191 892   1 920 667 810 954 903\n",
      " 116   7 471 374 982 681 167 688 434 716  33 837 821   3 327 269 965  70\n",
      " 362  89 157 178  13 618 179  67 111 459 543 571 605 393 530 652 601 806\n",
      "   6 645 784 101 913 932 761 626 274 155 211 611  80 991 125 120 226 440\n",
      " 261 864 513 809 937 796  46  62 557  63 145 715 423 164 579 741 519 726\n",
      " 130 350 830 749 998 370 915 641 235 464 930 171 185 717 746 859 856 985\n",
      " 228 531 831 672 517 117 706 824 520 996 523  95 427 129 135 791 625 612\n",
      " 653 740 729 739 658 297 258 154 259 933 730 380 346 355 924  48  93 718\n",
      "  69 266 510 934 765 738 406 165 386  61 264 280  40 220 233 143]\n"
     ]
    }
   ],
   "source": [
    "validacion_simple = ValidacionSimple(0.75)\n",
    "validacion_simple.creaParticiones(dataset)\n",
    "v_cruzada = ValidacionCruzada(4)\n",
    "v_cruzada.creaParticiones(dataset)\n",
    "\n",
    "print(\"VALIDACION SIMPLE\",validacion_simple.particiones[0])\n",
    "\n",
    "print(\"VALIDACION CRUZADA\")\n",
    "for particion in v_cruzada.particiones:\n",
    "    print(particion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[0.18939394, 0.45217391],\n",
      "       [0.24621212, 0.32173913],\n",
      "       [0.06628788, 0.04347826],\n",
      "       [0.49810606, 0.1826087 ]])\n",
      " array([[0.634436  , 0.65110325],\n",
      "       [0.365564  , 0.34889675]])\n",
      " array([[0.02268431, 0.09090909],\n",
      "       [0.03024575, 0.0995671 ],\n",
      "       [0.51795841, 0.54112554],\n",
      "       [0.0831758 , 0.1038961 ],\n",
      "       [0.34593573, 0.16450216]])\n",
      " array([[0.20786517, 0.28389831],\n",
      "       [0.11985019, 0.05508475],\n",
      "       [0.00749064, 0.02542373],\n",
      "       [0.17602996, 0.1779661 ],\n",
      "       [0.31460674, 0.20762712],\n",
      "       [0.01123596, 0.01694915],\n",
      "       [0.02621723, 0.03389831],\n",
      "       [0.04307116, 0.05932203],\n",
      "       [0.01498127, 0.00847458],\n",
      "       [0.07865169, 0.13135593]])\n",
      " array([[0.55564549, 0.52586983],\n",
      "       [0.44435451, 0.47413017]])\n",
      " array([[0.54820416, 0.71428571],\n",
      "       [0.10018904, 0.11688312],\n",
      "       [0.06994329, 0.03896104],\n",
      "       [0.05860113, 0.02164502],\n",
      "       [0.22306238, 0.10822511]])\n",
      " array([[0.05293006, 0.0995671 ],\n",
      "       [0.15122873, 0.24242424],\n",
      "       [0.31758034, 0.32900433],\n",
      "       [0.20415879, 0.10822511],\n",
      "       [0.27410208, 0.22077922]])\n",
      " array([[0.72307819, 0.73581702],\n",
      "       [0.27692181, 0.26418298]])\n",
      " array([[0.03409091, 0.07391304],\n",
      "       [0.28977273, 0.37391304],\n",
      "       [0.57575758, 0.46521739],\n",
      "       [0.10037879, 0.08695652]])\n",
      " array([[0.89753321, 0.89956332],\n",
      "       [0.0398482 , 0.07423581],\n",
      "       [0.0626186 , 0.02620087]])\n",
      " array([[0.72424462, 0.72073924],\n",
      "       [0.27575538, 0.27926076]])\n",
      " array([[0.31439394, 0.21304348],\n",
      "       [0.22727273, 0.24347826],\n",
      "       [0.33712121, 0.34782609],\n",
      "       [0.12121212, 0.19565217]])\n",
      " array([[0.75762003, 0.75378321],\n",
      "       [0.24237997, 0.24621679]])\n",
      " array([[0.10246679, 0.18340611],\n",
      "       [0.03036053, 0.069869  ],\n",
      "       [0.86717268, 0.74672489]])\n",
      " array([[0.16508539, 0.2489083 ],\n",
      "       [0.74573055, 0.62445415],\n",
      "       [0.08918406, 0.12663755]])\n",
      " array([[0.71443487, 0.70766773],\n",
      "       [0.28556513, 0.29233227]])\n",
      " array([[0.02272727, 0.02608696],\n",
      "       [0.18939394, 0.19565217],\n",
      "       [0.67045455, 0.59130435],\n",
      "       [0.11742424, 0.18695652]])\n",
      " array([[0.76346003, 0.76773804],\n",
      "       [0.23653997, 0.23226196]])\n",
      " array([[0.58555133, 0.61403509],\n",
      "       [0.41444867, 0.38596491]])\n",
      " array([[0.9486692 , 0.98684211],\n",
      "       [0.0513308 , 0.01315789]])]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 12 is out of bounds for axis 0 with size 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-122-177411b8a88b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#clasificador.entrenamiento(dataset, validacion_simple.particiones[0].indicesTrain)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#clasificador.clasifica(dataset, validacion_simple.particiones[0].indicesTest)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0merror\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclasificador\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidacion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidacion_simple\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mclasificador\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0merror1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclasificador\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidacion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv_cruzada\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mclasificador\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"El error del Clasificador NaiveBayes  para Validacion Simple es: \"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-80-fc3769d437ab>\u001b[0m in \u001b[0;36mvalidacion\u001b[0;34m(self, particionado, dataset, clasificador, seed)\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparticionado\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparticiones\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mclasificador\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mentrenamiento\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparticionado\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparticiones\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindicesTrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m             \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclasificador\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasifica\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mparticionado\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparticiones\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindicesTest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextraeDatos\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparticionado\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparticiones\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindicesTest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-115-3a4a42f3c549>\u001b[0m in \u001b[0;36mclasifica\u001b[0;34m(self, dataset, datosTest)\u001b[0m\n\u001b[1;32m     87\u001b[0m                 \u001b[0;31m#Aqui obtenemos cada valor posteriori de nuestro entrenamiento de los datos, es decir, P(D|H)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m                     \u001b[0;32mfor\u001b[0m \u001b[0matributo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mposteriori\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m                         \u001b[0maux\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mposteriori\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0matributo\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdato\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0matributo\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mclase\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m                     \u001b[0;31m#Aqui obtenemos la siguiente probabilidad P(D|H)*P(H)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 12 is out of bounds for axis 0 with size 2"
     ]
    }
   ],
   "source": [
    "clasificador = ClasificadorNaiveBayes(True)\n",
    "#clasificador.entrenamiento(dataset, validacion_simple.particiones[0].indicesTrain)\n",
    "#clasificador.clasifica(dataset, validacion_simple.particiones[0].indicesTest)\n",
    "error = clasificador.validacion(validacion_simple,dataset,clasificador)\n",
    "error1 = clasificador.validacion(v_cruzada,dataset,clasificador)\n",
    "print(\"El error del Clasificador NaiveBayes  para Validacion Simple es: \"+str(error))\n",
    "print(\"El error del Clasificador NaiveBayes para Validacion Cruzada es: \"+str(error1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aalcala/anaconda3/lib/python3.7/site-packages/sklearn/preprocessing/_encoders.py:415: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/aalcala/anaconda3/lib/python3.7/site-packages/sklearn/preprocessing/_encoders.py:451: DeprecationWarning: The 'categorical_features' keyword is deprecated in version 0.20 and will be removed in 0.22. You can use the ColumnTransformer instead.\n",
      "  \"use the ColumnTransformer instead.\", DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicciones utilizando validacion simple sin correcion de Laplace: [0. 0. 0. 1. 0. 0. 1. 1. 1. 1. 0. 1. 1. 0. 0. 0. 1. 0. 0. 1. 0. 1. 1. 0.\n",
      " 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1. 1. 0. 1. 0. 1. 1.\n",
      " 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 0. 1. 1.\n",
      " 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 1. 0. 1. 1. 0. 1. 0. 1. 0. 0. 0. 0. 1.\n",
      " 0. 0. 0. 1. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 1. 0. 1. 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0.\n",
      " 0. 0. 1. 1. 0. 1. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 1. 0. 0. 1. 1. 1. 1. 0.\n",
      " 0. 0. 1. 0. 1. 0. 1. 0.]\n",
      "Clases reales de la particion de text [0. 0. 0. 1. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 1. 0. 1. 0. 1. 1. 1. 1. 0. 0. 0. 1. 1.\n",
      " 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 1.\n",
      " 1. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 1.\n",
      " 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0.\n",
      " 1. 0. 1. 0. 1. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 1. 1. 1. 0. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 1. 0.\n",
      " 0. 0. 0. 1. 0. 1. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Predicciones utilizando validacion simple con correcion de Laplace: [0. 0. 0. 1. 0. 0. 1. 1. 1. 1. 0. 1. 1. 0. 0. 0. 1. 0. 0. 1. 0. 1. 1. 0.\n",
      " 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1. 1. 0. 1. 0. 1. 1.\n",
      " 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 0. 1. 1.\n",
      " 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 1. 0. 1. 1. 0. 1. 0. 1. 0. 0. 0. 0. 1.\n",
      " 0. 0. 0. 1. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 1. 0. 1. 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0.\n",
      " 0. 0. 1. 1. 0. 1. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 1. 0. 0. 1. 1. 1. 1. 0.\n",
      " 0. 0. 1. 0. 1. 0. 1. 0.]\n",
      "Clases reales de la particion de text [0. 0. 0. 1. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 1. 0. 1. 0. 1. 1. 1. 1. 0. 0. 0. 1. 1.\n",
      " 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 1.\n",
      " 1. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 1.\n",
      " 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0.\n",
      " 1. 0. 1. 0. 1. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 1. 1. 1. 0. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 1. 0.\n",
      " 0. 0. 0. 1. 0. 1. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aalcala/anaconda3/lib/python3.7/site-packages/sklearn/naive_bayes.py:485: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n",
      "/home/aalcala/anaconda3/lib/python3.7/site-packages/sklearn/naive_bayes.py:485: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n"
     ]
    }
   ],
   "source": [
    "#SKLearn Validacion Cruzada\n",
    "# Hacemos el encode de los datos\n",
    "encAtributos = preprocessing.OneHotEncoder(categorical_features=dataset.nominalAtributos[:-1],sparse=False)\n",
    "X = encAtributos.fit_transform(dataset.datos[:,:-1])\n",
    "Y = dataset.datos[:,-1]\n",
    "\n",
    "# Partimos los datos en Train y Test\n",
    "x_train, x_test, y_train,y_test = train_test_split(X,Y,test_size=0.2)\n",
    "\n",
    "# Aplicamos Naive Bayes sin Laplace\n",
    "clf = MultinomialNB(alpha = 0,fit_prior=True, class_prior=None)\n",
    "predicciones=clf.fit(x_train,y_train).predict(x_test)\n",
    "\n",
    "# Print de los resultados\n",
    "print(\"Predicciones utilizando validacion simple sin correcion de Laplace:\", predicciones)\n",
    "print(\"Clases reales de la particion de text\", y_test)\n",
    "\n",
    "# Aplicamos Naive Bayes con Laplace\n",
    "clf = MultinomialNB(alpha = 0,fit_prior=True, class_prior=None)\n",
    "predicciones=clf.fit(x_train,y_train).predict(x_test)\n",
    "\n",
    "# Print de los resultados\n",
    "print(\"Predicciones utilizando validacion simple con correcion de Laplace:\", predicciones)\n",
    "print(\"Clases reales de la particion de text\", y_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicciones utilizando validacion simple sin correcion de Laplace: [1. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 0. 1. 0. 1. 1. 1. 1. 1. 0. 1. 0. 1. 1.\n",
      " 0. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 0. 0. 1. 0. 0. 0. 1. 1. 1. 0. 1. 1. 0.\n",
      " 1. 1. 1. 0. 0. 1. 0. 1. 1. 0. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1.\n",
      " 0. 1. 1. 1. 0. 1. 0. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 0. 0. 0. 1. 0.\n",
      " 1. 1. 0. 0. 1. 1. 1. 0. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1.\n",
      " 1. 1. 0. 1. 1. 1. 0. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 0. 0. 1. 0. 0. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1.]\n",
      "Clases reales de la particion de text [1. 0. 1. 0. 0. 1. 1. 0. 0. 1. 1. 0. 0. 1. 1. 1. 0. 0. 1. 1. 1. 0. 1. 1.\n",
      " 1. 1. 1. 1. 0. 0. 0. 1. 1. 0. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 0. 0. 1. 1.\n",
      " 1. 0. 1. 0. 1. 1. 0. 0. 1. 1. 1. 1. 1. 0. 1. 1. 0. 0. 1. 1. 1. 0. 0. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 0. 1. 1. 0. 0. 1. 1. 1. 1.\n",
      " 1. 1. 1. 0. 1. 0. 1. 1. 1. 1. 0. 0. 1. 1. 0. 1. 1. 0. 1. 1. 0. 0. 1. 1.\n",
      " 0. 1. 1. 0. 0. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 0.\n",
      " 1. 1. 0. 1. 1. 1. 1. 0. 0. 1. 1. 0. 0. 0. 1. 1. 0. 0. 0. 1. 0. 1. 1. 1.\n",
      " 1. 1. 0. 0. 1. 1. 0. 1. 1. 1. 0. 0. 0. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1. 0.]\n",
      "Predicciones utilizando validacion simple con correcion de Laplace: [1. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 0. 1. 0. 1. 1. 1. 1. 1. 0. 1. 0. 1. 1.\n",
      " 0. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 0. 0. 1. 0. 0. 0. 1. 1. 1. 0. 1. 1. 0.\n",
      " 1. 1. 1. 0. 0. 1. 0. 1. 1. 0. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1.\n",
      " 0. 1. 1. 1. 0. 1. 0. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 0. 0. 0. 1. 0.\n",
      " 1. 1. 0. 0. 1. 1. 1. 0. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1.\n",
      " 1. 1. 0. 1. 1. 1. 0. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 0. 0. 1. 0. 0. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1.]\n",
      "Clases reales de la particion de text [1. 0. 1. 0. 0. 1. 1. 0. 0. 1. 1. 0. 0. 1. 1. 1. 0. 0. 1. 1. 1. 0. 1. 1.\n",
      " 1. 1. 1. 1. 0. 0. 0. 1. 1. 0. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 0. 0. 1. 1.\n",
      " 1. 0. 1. 0. 1. 1. 0. 0. 1. 1. 1. 1. 1. 0. 1. 1. 0. 0. 1. 1. 1. 0. 0. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 0. 1. 1. 0. 0. 1. 1. 1. 1.\n",
      " 1. 1. 1. 0. 1. 0. 1. 1. 1. 1. 0. 0. 1. 1. 0. 1. 1. 0. 1. 1. 0. 0. 1. 1.\n",
      " 0. 1. 1. 0. 0. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 0.\n",
      " 1. 1. 0. 1. 1. 1. 1. 0. 0. 1. 1. 0. 0. 0. 1. 1. 0. 0. 0. 1. 0. 1. 1. 1.\n",
      " 1. 1. 0. 0. 1. 1. 0. 1. 1. 1. 0. 0. 0. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1. 0.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aalcala/anaconda3/lib/python3.7/site-packages/sklearn/preprocessing/_encoders.py:415: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/aalcala/anaconda3/lib/python3.7/site-packages/sklearn/preprocessing/_encoders.py:451: DeprecationWarning: The 'categorical_features' keyword is deprecated in version 0.20 and will be removed in 0.22. You can use the ColumnTransformer instead.\n",
      "  \"use the ColumnTransformer instead.\", DeprecationWarning)\n",
      "/home/aalcala/anaconda3/lib/python3.7/site-packages/sklearn/naive_bayes.py:485: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n",
      "/home/aalcala/anaconda3/lib/python3.7/site-packages/sklearn/naive_bayes.py:485: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n"
     ]
    }
   ],
   "source": [
    "# SKLearn Validacion Cruzada\n",
    "# Hacemos el encode de los datos\n",
    "encAtributos = preprocessing.OneHotEncoder(categorical_features=dataset.nominalAtributos[:-1],sparse=False)\n",
    "X = encAtributos.fit_transform(dataset.datos[:,:-1])\n",
    "Y = dataset.datos[:,-1]\n",
    "\n",
    "# Partimos los datos en Train y Test\n",
    "x_train, x_test, y_train,y_test = train_test_split(X,Y,test_size=0.2)\n",
    "\n",
    "# Aplicamos Naive Bayes sin Laplace\n",
    "clf = MultinomialNB(alpha = 0,fit_prior=True, class_prior=None)\n",
    "predicciones=clf.fit(x_train,y_train).predict(x_test)\n",
    "\n",
    "# Print de los resultados\n",
    "print(\"Predicciones utilizando validacion simple sin correcion de Laplace:\", predicciones)\n",
    "print(\"Clases reales de la particion de text\", y_test)\n",
    "\n",
    "# Aplicamos Naive Bayes con Laplace\n",
    "clf = MultinomialNB(alpha = 0,fit_prior=True, class_prior=None)\n",
    "predicciones=clf.fit(x_train,y_train).predict(x_test)\n",
    "\n",
    "# Print de los resultados\n",
    "print(\"Predicciones utilizando validacion simple con correcion de Laplace:\", predicciones)\n",
    "print(\"Clases reales de la particion de text\", y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
