{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/python3\n",
    "# -*- coding: utf-8 -*-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importamos Librerias\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from sortedcontainers import SortedDict\n",
    "import sklearn.naive_bayes as nb\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold, cross_val_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Datos:\n",
    "\n",
    "  TiposDeAtributos=('Continuo','Nominal')\n",
    "\n",
    "  # TODO: procesar el fichero para asignar correctamente las variables tipoAtributos, nombreAtributos, nominalAtributos, datos y diccionarios\n",
    "  # NOTA: No confundir TiposDeAtributos con tipoAtributos\n",
    "  def __init__(self, nombreFichero):\n",
    "\n",
    "      with open(nombreFichero, \"r\") as f:\n",
    "        # Guardamos el numero de datos que contiene el DataSet y esta en la primera linea\n",
    "        self.numDatos = int(f.readline())\n",
    "\n",
    "        # Guardamos el nombre de los atributos\n",
    "        self.nombreAtributos = f.readline().strip('\\n').split(',')\n",
    "        #print(self.nombreAtributos)\n",
    "\n",
    "        # Leemos el tipo de los atributos de las variables y eliminamos el ultimo que es un salto de linea\n",
    "        self.tipoAtributos = f.readline().strip('\\n').split(',')\n",
    "        #print(self.tipoAtributos)\n",
    "\n",
    "        # Comprobamos que todos los atributos sean Continuos o Nominales\n",
    "        if any(atr not in Datos.TiposDeAtributos for atr in self.tipoAtributos):\n",
    "            raise ValueError(\"Tipo de atributo erroneo\")\n",
    "\n",
    "        # Segun el atributo, asignamos True o False.\n",
    "        self.nominalAtributos = []\n",
    "\n",
    "        # Guardamos en la lista nominalAtributos en la posicion de cada uno si es o no Nominal\n",
    "        for tipo in self.tipoAtributos:\n",
    "            if tipo == self.TiposDeAtributos[0]:\n",
    "                self.nominalAtributos.append(False)\n",
    "            else:\n",
    "                self.nominalAtributos.append(True)\n",
    "        #print(self.nominalAtributos)\n",
    "\n",
    "        # Guardamos los datos del fichero y los formateamos, de tal forma que cada linea es una lista\n",
    "        datos = f.readlines()\n",
    "        datosFormat = []\n",
    "        for lista in datos:\n",
    "            datosFormat.append(lista.strip('\\n').split(','))\n",
    "\n",
    "        # print(set(sorted(datosFormat[0])))\n",
    "        listaDatosAtributos = []\n",
    "        for i in range(len(self.tipoAtributos)):\n",
    "            listaDatosAtributos.append([])\n",
    "\n",
    "        # Hacemos la traspuesta de los datos que guardamos para que cada lista de atributo guarde todos los datos\n",
    "        # de cada atributo.\n",
    "        for lista in datosFormat:\n",
    "            i = 0\n",
    "            for item in lista:\n",
    "                listaDatosAtributos[i].append(item)\n",
    "                i += 1\n",
    "\n",
    "        # Ordenamos y hacemos un set para eliminar repetidos.\n",
    "        i = 0\n",
    "        for item in listaDatosAtributos:\n",
    "            listaDatosAtributos[i] = sorted(set(item))\n",
    "            i += 1\n",
    "\n",
    "\n",
    "        # Creacion de lista diccionarios, en caso de que el atributo sea Continuo, el diccionario estara vacio\n",
    "        self.listaDicts = []\n",
    "        for i in range(len(self.tipoAtributos)):\n",
    "            self.listaDicts.append({})\n",
    "\n",
    "        # Creamos el diccionario tal y como se describe en las diapositivas, por orden y asignando valores numericos crecientes\n",
    "        i = 0\n",
    "        for atributo in listaDatosAtributos:\n",
    "            k = 0\n",
    "            if self.tipoAtributos[i] == \"Nominal\":\n",
    "                for dato in atributo:\n",
    "                    self.listaDicts[i][dato] = k\n",
    "                    k += 1\n",
    "            i += 1\n",
    "\n",
    "        # Creacion de la matriz de datos utilizando el diccionario para mapear los valores\n",
    "        # En primer lugar, creamos una matriz vacia de tamaña numero de atributos.\n",
    "        self.datos = np.empty((int(self.numDatos),int(len(self.tipoAtributos))))\n",
    "        i = 0\n",
    "        j = 0\n",
    "\n",
    "        # Metemos los datos en la matriz, mapeando con los diccionarios en el caso de que sean Nominales, y si son continuos normal.\n",
    "        for i in range(int(self.numDatos)):\n",
    "            for j in range(len(self.tipoAtributos)):\n",
    "                if self.tipoAtributos[j] == 'Nominal':\n",
    "                    self.datos[i][j] = self.listaDicts[j].get(str(datosFormat[i][j]))\n",
    "                else:\n",
    "                    self.datos[i][j] = datosFormat[i][j]\n",
    "        \n",
    "        print(self.nombreAtributos)\n",
    "        print(self.listaDicts)\n",
    "        f.close()\n",
    "\n",
    "\n",
    "  # TODO: implementar en la practica 1\n",
    "  def extraeDatos(self, idx):\n",
    "    return self.datos[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Age', 'Spectacle', 'Astigmatic', 'Tear', 'Class']\n",
      "[{'1': 0, '2': 1, '3': 2}, {'1': 0, '2': 1}, {'1': 0, '2': 1}, {'1': 0, '2': 1}, {'1': 0, '2': 1, '3': 2}]\n",
      "[[0. 0. 0. 0. 2.]\n",
      " [0. 0. 0. 1. 1.]\n",
      " [0. 0. 1. 0. 2.]\n",
      " [0. 0. 1. 1. 0.]\n",
      " [0. 1. 0. 0. 2.]\n",
      " [0. 1. 0. 1. 1.]\n",
      " [0. 1. 1. 0. 2.]\n",
      " [0. 1. 1. 1. 0.]\n",
      " [1. 0. 0. 0. 2.]\n",
      " [1. 0. 0. 1. 1.]\n",
      " [1. 0. 1. 0. 2.]\n",
      " [1. 0. 1. 1. 0.]\n",
      " [1. 1. 0. 0. 2.]\n",
      " [1. 1. 0. 1. 1.]\n",
      " [1. 1. 1. 0. 2.]\n",
      " [1. 1. 1. 1. 2.]\n",
      " [2. 0. 0. 0. 2.]\n",
      " [2. 0. 0. 1. 2.]\n",
      " [2. 0. 1. 0. 2.]\n",
      " [2. 0. 1. 1. 0.]\n",
      " [2. 1. 0. 0. 2.]\n",
      " [2. 1. 0. 1. 1.]\n",
      " [2. 1. 1. 0. 2.]\n",
      " [2. 1. 1. 1. 2.]]\n"
     ]
    }
   ],
   "source": [
    "dataset = Datos('lenses.data')\n",
    "\n",
    "print(dataset.datos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABCMeta,abstractmethod\n",
    "\n",
    "\n",
    "class Particion():\n",
    "\n",
    "  # Esta clase mantiene la lista de �ndices de Train y Test para cada partici�n del conjunto de particiones\n",
    "    def __init__(self,train=[],test=[]):\n",
    "        self.indicesTrain=train\n",
    "        self.indicesTest=test\n",
    "    \n",
    "    def __str__(self):\n",
    "        return \"Train: {}\\nTest:  {}\".format(str(self.indicesTrain),str(self.indicesTest)) \n",
    "\n",
    "#####################################################################################################\n",
    "\n",
    "class EstrategiaParticionado:\n",
    "\n",
    "    # Clase abstracta\n",
    "    __metaclass__ = ABCMeta\n",
    "\n",
    "    # Lista de las particiones\n",
    "    def __init__(self, nombre=\"\"):\n",
    "        self.nombreEstrategia = nombre\n",
    "        self.numeroParticiones = 0\n",
    "        self.particiones=[]\n",
    "\n",
    "    # Atributos: deben rellenarse adecuadamente para cada estrategia concreta: nombreEstrategia, numeroParticiones, listaParticiones. Se pasan en el constructor\n",
    "\n",
    "    @abstractmethod\n",
    "    # TODO: esta funcion deben ser implementadas en cada estrategia concreta\n",
    "    def creaParticiones(self,datos,seed=None):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValidacionSimple(EstrategiaParticionado):\n",
    "    \n",
    "    def __init__(self, porcentaje):\n",
    "        self.porcentaje = porcentaje\n",
    "        super().__init__(\"Validacion simple\")\n",
    "        \n",
    "  # Crea particiones segun el metodo tradicional de division de los datos segun el porcentaje deseado.\n",
    "  # Devuelve una lista de particiones (clase Particion)\n",
    "  # TODO: implementar\n",
    "    def creaParticiones(self,datos,seed=None):\n",
    "        np.random.seed(seed)\n",
    "\n",
    "        self.numeroParticiones = 1\n",
    "    \n",
    "        # Generamos una lista con todos los números de datos aleatorios   \n",
    "        indicesAleatorios = np.random.permutation(int(datos.numDatos))\n",
    "        \n",
    "        # Creamos la particion, en funcion del porcentaje especificado\n",
    "        self.particiones = [Particion(indicesAleatorios[:int(datos.numDatos*self.porcentaje)],\n",
    "                                      indicesAleatorios[int(datos.numDatos*self.porcentaje):])]\n",
    "        \n",
    "        return self.particiones\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [18 15 10 17 11  0 13  2 19  4 20 21 23  6  9  3  1 12]\n",
      "Test:  [ 7 16 22  8  5 14]\n"
     ]
    }
   ],
   "source": [
    "validacion_simple = ValidacionSimple(0.75)\n",
    "validacion_simple.creaParticiones(dataset)\n",
    "print(validacion_simple.particiones[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValidacionCruzada(EstrategiaParticionado):\n",
    "\n",
    "  # Crea particiones segun el metodo de validacion cruzada.\n",
    "  # El conjunto de entrenamiento se crea con las nfolds-1 particiones y el de test con la particion restante\n",
    "  # Esta funcion devuelve una lista de particiones (clase Particion)\n",
    "  # TODO: implementar\n",
    "    \n",
    "    def __init__(self, k):\n",
    "        self.k = k\n",
    "        super().__init__(\"Validacion cruzada\")\n",
    "\n",
    "    def creaParticiones(self,datos,seed=None):\n",
    "        np.random.seed(seed)\n",
    "        \n",
    "        self.numeroParticiones = self.k\n",
    "        \n",
    "        # Generamos una lista con todos los números de datos aleatorios   \n",
    "        indicesAleatorios = np.random.permutation(int(datos.numDatos))\n",
    "        \n",
    "        # Hallamos el tamaño de cada bloque\n",
    "        tamBloque = int(datos.numDatos/self.k)\n",
    "        \n",
    "        print(indicesAleatorios)\n",
    "        \n",
    "        datosSobran = datos.numDatos - (tamBloque*self.k)\n",
    "        count = 0\n",
    "        for i in range(self.k):\n",
    "            \n",
    "            train = np.delete(indicesAleatorios, range(i*tamBloque,(i+1)*tamBloque))\n",
    "            test =  indicesAleatorios[i*tamBloque:(i+1)*tamBloque]\n",
    "            \n",
    "            # Caso en el que la cuenta es justa\n",
    "            if datosSobran == 0:\n",
    "                self.particiones.append(Particion(train, test))\n",
    "                \n",
    "            # Contemplamos el caso de que la division para sacar el numero de subconjuntos no fuese entera\n",
    "            if datosSobran > 0:\n",
    "                count += 1\n",
    "                particionTest = np.append(test, train[(datos.numDatos - tamBloque)- i - 1])\n",
    "                particionTrain = np.delete(train, (datos.numDatos - tamBloque)- i - 1)\n",
    "                datosSobran -= 1\n",
    "                self.particiones.append(Particion(particionTrain, particionTest))\n",
    "                \n",
    "            \n",
    "           \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14 22 23  4 16  3  2  1 10  0 20  7 19 21  9 18 15  6  5 12 17 11 13  8]\n",
      "Train: [16  3  2  1 10  0 20  7 19 21  9 18 15  6  5 12 17 11 13]\n",
      "Test:  [14 22 23  4  8]\n",
      "Train: [14 22 23  4 10  0 20  7 19 21  9 18 15  6  5 12 17 11  8]\n",
      "Test:  [16  3  2  1 13]\n",
      "Train: [14 22 23  4 16  3  2  1 19 21  9 18 15  6  5 12 17 13  8]\n",
      "Test:  [10  0 20  7 11]\n",
      "Train: [14 22 23  4 16  3  2  1 10  0 20  7 15  6  5 12 11 13  8]\n",
      "Test:  [19 21  9 18 17]\n",
      "Train: [14 22 23  4 16  3  2  1 10  0 20  7 19 21  9 18 17 11 13  8]\n",
      "Test:  [15  6  5 12]\n"
     ]
    }
   ],
   "source": [
    "v_cruzada = ValidacionCruzada(5)\n",
    "v_cruzada.creaParticiones(dataset)\n",
    "\n",
    "for particion in v_cruzada.particiones:\n",
    "    print(particion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Clasificador:\n",
    "  \n",
    "    # Clase abstracta\n",
    "    __metaclass__ = ABCMeta\n",
    "\n",
    "    # Metodos abstractos que se implementan en casa clasificador concreto\n",
    "    @abstractmethod\n",
    "    # TODO: esta funcion debe ser implementada en cada clasificador concreto\n",
    "    # datosTrain: matriz numpy con los datos de entrenamiento\n",
    "    # atributosDiscretos: array bool con la indicatriz de los atributos nominales\n",
    "    # diccionario: array de diccionarios de la estructura Datos utilizados para la codificacion de variables discretas\n",
    "    def entrenamiento(self,datos,datosTrain,atributosDiscretos,diccionario):\n",
    "        pass\n",
    "\n",
    "\n",
    "    @abstractmethod\n",
    "    # TODO: esta funcion debe ser implementada en cada clasificador concreto\n",
    "    # devuelve un numpy array con las predicciones\n",
    "    def clasifica(self,datosTest,atributosDiscretos,diccionario):\n",
    "        pass\n",
    "\n",
    "\n",
    "    # Obtiene el numero de aciertos y errores para calcular la tasa de fallo\n",
    "    # TODO: implementar\n",
    "    def error(self,datos,pred):\n",
    "    # Aqui se compara la prediccion (pred) con las clases reales y se calcula el error    \n",
    "        i = 0\n",
    "        numDatos = datos.shape()[0]\n",
    "        error = 0\n",
    "        for i in range(numDatos):\n",
    "            if datos[i] != pred[i]:\n",
    "                error += 1\n",
    "        err = (error)/(numDatos+0.0)\n",
    "        return err\n",
    "\n",
    "\n",
    "    # Realiza una clasificacion utilizando una estrategia de particionado determinada\n",
    "    # TODO: implementar esta funcion\n",
    "    def validacion(self,particionado,dataset,clasificador,seed=None):\n",
    "\n",
    "    # Creamos las particiones siguiendo la estrategia llamando a particionado.creaParticiones\n",
    "    # - Para validacion cruzada: en el bucle hasta nv entrenamos el clasificador con la particion de train i\n",
    "    # y obtenemos el error en la particion de test i\n",
    "    # - Para validacion simple (hold-out): entrenamos el clasificador con la particion de train\n",
    "    # y obtenemos el error en la particion test. Otra opci�n es repetir la validaci�n simple un n�mero especificado de veces, obteniendo en cada una un error. Finalmente se calcular�a la media.\n",
    "        particionado.creaParticiones(dataset, seed)\n",
    "    # Comprobamos si es por validación cruzada o simple, por la longitud de la lista de particiones\n",
    "    # Validación Simple\n",
    "        if len(particionado.particiones) == 1:\n",
    "            clasificador.entrenamiento(dataset.extraeDatos(particionado.particiones[0].indicesTrain), dataset.nominalAtributos, dataset.listaDicts)\n",
    "            pred = clasificador.clasifica(dataset.extraeDatos(particionado.particiones[0].indicesTest), dataset.nominalAtributos, dataset.listaDicts)\n",
    "            ret = self.error(dataset.extraeDatos(particionado.particiones[0].indicesTest), pred)\n",
    "            if ret > 0:\n",
    "                return ret\n",
    "            else:\n",
    "                return 0\n",
    "    # Validación Cruzada        \n",
    "        else:\n",
    "            for particion in particionado.particiones:\n",
    "                clasificador.entrenmiento(dataset.extraeDatos(particion.indicesTrain), dataset.nominalAtributos, dataset.listaDicts)\n",
    "                pred = clasificador.clasifica(dataset.extraeDatos(particion.indicesTest), dataset.nominalAtributos, dataset.listaDicts)\n",
    "                ret = self.error(dataset.extraeDatos(particion.indicesTest), pred)\n",
    "                errores = np.append(errores,ret)\n",
    "                #Devolucion de la media de los errores\n",
    "            return errores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClasificadorNaiveBayes(Clasificador):\n",
    "    \n",
    "    def entrenamiento(self,dataset,datosTrain):\n",
    "     \n",
    "        # Cargamos todos los datos de la clase del dataset desde la matriz de datos\n",
    "        caca = 1\n",
    "        self.numClases = dataset.datos[:,-1]\n",
    "    \n",
    "        # Contamos las apariciones de cada uno para luego calcular la probabilidad a priori de cada clase\n",
    "        counter = Counter(self.numClases)\n",
    "        \n",
    "        # Calculamos la probabilidad de la clase y lo metemos en un diccionario ordenado segun el numero \n",
    "        # correspondiente a cada clase asignado en el diccionario\n",
    "        self.dictPrioris={}\n",
    "        \n",
    "        for k in counter:\n",
    "            k = int(k)\n",
    "            counter[k] = counter[k]/len(self.numClases)\n",
    "            self.dictPrioris[k] = counter[k]\n",
    "        \n",
    "        self.dictPrioris = SortedDict(self.dictPrioris)\n",
    "        \n",
    "        # Calcular tablas de probabilidades del entrenamiento. Tenemos que calcular por cada atributo una cuenta\n",
    "        # de las apariciones en cada clase\n",
    "        print(self.dictPrioris)\n",
    "        print(dataset.nombreAtributos)\n",
    "        print(dataset.datos[:,1])\n",
    "        matriz = np.empty((len(dataset.listaDicts[0]),len(dataset.listaDicts[-1])))\n",
    "        \n",
    "\n",
    "\n",
    "        \n",
    "    #def priori_discreto():\n",
    "        \n",
    "        \n",
    "    def clasifica(self,datosTest,atributosDiscretos,diccionario):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0. 2.]\n",
      " [0. 0. 0. 1. 1.]\n",
      " [0. 0. 1. 0. 2.]\n",
      " [0. 0. 1. 1. 0.]\n",
      " [0. 1. 0. 0. 2.]\n",
      " [0. 1. 0. 1. 1.]\n",
      " [0. 1. 1. 0. 2.]\n",
      " [0. 1. 1. 1. 0.]\n",
      " [1. 0. 0. 0. 2.]\n",
      " [1. 0. 0. 1. 1.]\n",
      " [1. 0. 1. 0. 2.]\n",
      " [1. 0. 1. 1. 0.]\n",
      " [1. 1. 0. 0. 2.]\n",
      " [1. 1. 0. 1. 1.]\n",
      " [1. 1. 1. 0. 2.]\n",
      " [1. 1. 1. 1. 2.]\n",
      " [2. 0. 0. 0. 2.]\n",
      " [2. 0. 0. 1. 2.]\n",
      " [2. 0. 1. 0. 2.]\n",
      " [2. 0. 1. 1. 0.]\n",
      " [2. 1. 0. 0. 2.]\n",
      " [2. 1. 0. 1. 1.]\n",
      " [2. 1. 1. 0. 2.]\n",
      " [2. 1. 1. 1. 2.]]\n",
      "SortedDict({0: 0.16666666666666666, 1: 0.20833333333333334, 2: 0.625})\n",
      "['Age', 'Spectacle', 'Astigmatic', 'Tear', 'Class']\n",
      "[0. 0. 0. 0. 1. 1. 1. 1. 0. 0. 0. 0. 1. 1. 1. 1. 0. 0. 0. 0. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "clasificador = ClasificadorNaiveBayes()\n",
    "print(dataset.datos)\n",
    "\n",
    "clasificador.entrenamiento(dataset, validacion_simple.particiones[0].indicesTrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validaciones utilizando SKLearn\n",
    "def validacion_simple(dataset, porcentaje):\n",
    "    \n",
    "   \n",
    "    X = dataset.datos[:,:-1]\n",
    "    y = dataset.datos[:,-1]\n",
    "    \n",
    "    return train_test_split(X,y,train_size = porcentaje, test_size = 1 - porcentaje, shuffle=True)\n",
    "    \n",
    "def validacion_cruzada(dataset, k):\n",
    "    X = dataset.datos[:, :-1]\n",
    "    Y = dataset.datos[:, -1]\n",
    "    \n",
    "    kf = KFold(n_splits=k, shuffle = True)\n",
    "    \n",
    "    particiones = []\n",
    "    \n",
    "    for train, test in kf.split(X):\n",
    "        particiones.append(Particion(train,test))\n",
    "    \n",
    "    return particiones\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VALIDACION SIMPLE:\n",
      "Train: [[2. 0. 0. 1.]\n",
      " [2. 1. 1. 1.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 1. 1. 1.]\n",
      " [1. 0. 0. 1.]\n",
      " [0. 0. 1. 0.]\n",
      " [2. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 1. 0.]\n",
      " [0. 0. 1. 1.]\n",
      " [1. 1. 1. 0.]\n",
      " [1. 1. 0. 0.]\n",
      " [1. 1. 1. 1.]\n",
      " [2. 0. 1. 1.]\n",
      " [1. 0. 1. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [2. 1. 1. 0.]\n",
      " [2. 0. 1. 0.]]\n",
      "Test:  [[1. 1. 0. 1.]\n",
      " [2. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 1. 0. 1.]\n",
      " [2. 1. 0. 1.]\n",
      " [1. 0. 1. 0.]]\n",
      "VALIDACION CRUZADA:\n",
      "Train: [ 0  1  2  4  5  6  7 11 12 13 14 15 16 17 18 19 20 21 23]\n",
      "Test:  [ 3  8  9 10 22]\n",
      "Train: [ 0  1  2  3  4  5  6  7  8  9 10 12 14 15 17 20 21 22 23]\n",
      "Test:  [11 13 16 18 19]\n",
      "Train: [ 1  3  4  5  8  9 10 11 12 13 14 15 16 17 18 19 20 22 23]\n",
      "Test:  [ 0  2  6  7 21]\n",
      "Train: [ 0  2  3  4  5  6  7  8  9 10 11 13 16 17 18 19 20 21 22]\n",
      "Test:  [ 1 12 14 15 23]\n",
      "Train: [ 0  1  2  3  6  7  8  9 10 11 12 13 14 15 16 18 19 21 22 23]\n",
      "Test:  [ 4  5 17 20]\n"
     ]
    }
   ],
   "source": [
    "# Prueba validacion simple\n",
    "\n",
    "XTrain, XTest, YTrain, YTest = validacion_simple(dataset, 0.75)\n",
    "print(\"VALIDACION SIMPLE:\")\n",
    "print(\"Train:\" ,XTrain)\n",
    "print(\"Test: \" ,XTest)\n",
    "\n",
    "particiones = validacion_cruzada(dataset,5)\n",
    "print(\"VALIDACION CRUZADA:\")\n",
    "for particion in particiones:\n",
    "    print(particion)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2. 2. 2. 2. 2. 2.]\n",
      "[1. 2. 2. 1. 1. 2.]\n"
     ]
    }
   ],
   "source": [
    "# Clasificador Naive Bayes SKLearn MULTINOMIAL\n",
    "clf = nb.MultinomialNB(alpha=1.0, fit_prior=True, class_prior=None)\n",
    "\n",
    "# Entrenamos el modelo\n",
    "clf.fit(XTrain, YTrain)\n",
    "\n",
    "# Clasifica el test\n",
    "valoresPredichos = clf.predict(XTest)\n",
    "print(valoresPredichos)\n",
    "print(YTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2. 2. 2. 2. 2. 2.]\n",
      "[1. 2. 2. 1. 1. 2.]\n"
     ]
    }
   ],
   "source": [
    "# Clasificador Naive Bayes GAUSIANA\n",
    "clasificadorGaussian = nb.GaussianNB()\n",
    "clasificadorGaussian.fit(XTrain, YTrain)\n",
    "valoresPredichos = clasificadorGaussian.predict(XTest)\n",
    "print(valoresPredichos)\n",
    "print(YTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
