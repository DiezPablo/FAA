{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/python3\n",
    "# -*- coding: utf-8 -*-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Importamos Librerias\n",
    "import numpy as np\n",
    "import math\n",
    "from collections import Counter\n",
    "from sortedcontainers import SortedDict\n",
    "import sklearn.naive_bayes as nb\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold, cross_val_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class Datos:\n",
    "\n",
    "  TiposDeAtributos=('Continuo','Nominal')\n",
    "\n",
    "  # TODO: procesar el fichero para asignar correctamente las variables tipoAtributos, nombreAtributos, nominalAtributos, datos y diccionarios\n",
    "  # NOTA: No confundir TiposDeAtributos con tipoAtributos\n",
    "  def __init__(self, nombreFichero):\n",
    "\n",
    "      with open(nombreFichero, \"r\") as f:\n",
    "        # Guardamos el numero de datos que contiene el DataSet y esta en la primera linea\n",
    "        self.numDatos = int(f.readline())\n",
    "\n",
    "        # Guardamos el nombre de los atributos\n",
    "        self.nombreAtributos = f.readline().strip('\\n').split(',')\n",
    "        #print(self.nombreAtributos)\n",
    "\n",
    "        # Leemos el tipo de los atributos de las variables y eliminamos el ultimo que es un salto de linea\n",
    "        self.tipoAtributos = f.readline().strip('\\n').split(',')\n",
    "        #print(self.tipoAtributos)\n",
    "\n",
    "        # Comprobamos que todos los atributos sean Continuos o Nominales\n",
    "        if any(atr not in Datos.TiposDeAtributos for atr in self.tipoAtributos):\n",
    "            raise ValueError(\"Tipo de atributo erroneo\")\n",
    "\n",
    "        # Segun el atributo, asignamos True o False.\n",
    "        self.nominalAtributos = []\n",
    "\n",
    "        # Guardamos en la lista nominalAtributos en la posicion de cada uno si es o no Nominal\n",
    "        for tipo in self.tipoAtributos:\n",
    "            if tipo == self.TiposDeAtributos[0]:\n",
    "                self.nominalAtributos.append(False)\n",
    "            else:\n",
    "                self.nominalAtributos.append(True)\n",
    "        #print(self.nominalAtributos)\n",
    "\n",
    "        # Guardamos los datos del fichero y los formateamos, de tal forma que cada linea es una lista\n",
    "        datos = f.readlines()\n",
    "        datosFormat = []\n",
    "        for lista in datos:\n",
    "            datosFormat.append(lista.strip('\\n').split(','))\n",
    "\n",
    "        # print(set(sorted(datosFormat[0])))\n",
    "        listaDatosAtributos = []\n",
    "        for i in range(len(self.tipoAtributos)):\n",
    "            listaDatosAtributos.append([])\n",
    "\n",
    "        # Hacemos la traspuesta de los datos que guardamos para que cada lista de atributo guarde todos los datos\n",
    "        # de cada atributo.\n",
    "        for lista in datosFormat:\n",
    "            i = 0\n",
    "            for item in lista:\n",
    "                listaDatosAtributos[i].append(item)\n",
    "                i += 1\n",
    "\n",
    "        # Ordenamos y hacemos un set para eliminar repetidos.\n",
    "        i = 0\n",
    "        for item in listaDatosAtributos:\n",
    "            listaDatosAtributos[i] = sorted(set(item))\n",
    "            i += 1\n",
    "\n",
    "\n",
    "        # Creacion de lista diccionarios, en caso de que el atributo sea Continuo, el diccionario estara vacio\n",
    "        self.listaDicts = []\n",
    "        for i in range(len(self.tipoAtributos)):\n",
    "            self.listaDicts.append({})\n",
    "\n",
    "        # Creamos el diccionario tal y como se describe en las diapositivas, por orden y asignando valores numericos crecientes\n",
    "        i = 0\n",
    "        for atributo in listaDatosAtributos:\n",
    "            k = 0\n",
    "            if self.tipoAtributos[i] == \"Nominal\":\n",
    "                for dato in atributo:\n",
    "                    self.listaDicts[i][dato] = k\n",
    "                    k += 1\n",
    "            i += 1\n",
    "\n",
    "        # Creacion de la matriz de datos utilizando el diccionario para mapear los valores\n",
    "        # En primer lugar, creamos una matriz vacia de tamaña numero de atributos.\n",
    "        self.datos = np.empty((int(self.numDatos),int(len(self.tipoAtributos))))\n",
    "        i = 0\n",
    "        j = 0\n",
    "\n",
    "        # Metemos los datos en la matriz, mapeando con los diccionarios en el caso de que sean Nominales, y si son continuos normal.\n",
    "        for i in range(int(self.numDatos)):\n",
    "            for j in range(len(self.tipoAtributos)):\n",
    "                if self.tipoAtributos[j] == 'Nominal':\n",
    "                    self.datos[i][j] = self.listaDicts[j].get(str(datosFormat[i][j]))\n",
    "                else:\n",
    "                    self.datos[i][j] = datosFormat[i][j]\n",
    "        \n",
    "        print(self.nombreAtributos)\n",
    "        print(self.listaDicts)\n",
    "        f.close()\n",
    "\n",
    "\n",
    "  # TODO: implementar en la practica 1\n",
    "  def extraeDatos(self, idx):\n",
    "    return self.datos[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Age', 'Spectacle', 'Astigmatic', 'Tear', 'Class']\n",
      "[{'1': 0, '2': 1, '3': 2}, {'1': 0, '2': 1}, {'1': 0, '2': 1}, {'1': 0, '2': 1}, {'1': 0, '2': 1, '3': 2}]\n",
      "[[0. 0. 0. 0. 2.]\n",
      " [0. 0. 0. 1. 1.]\n",
      " [0. 0. 1. 0. 2.]\n",
      " [0. 0. 1. 1. 0.]\n",
      " [0. 1. 0. 0. 2.]\n",
      " [0. 1. 0. 1. 1.]\n",
      " [0. 1. 1. 0. 2.]\n",
      " [0. 1. 1. 1. 0.]\n",
      " [1. 0. 0. 0. 2.]\n",
      " [1. 0. 0. 1. 1.]\n",
      " [1. 0. 1. 0. 2.]\n",
      " [1. 0. 1. 1. 0.]\n",
      " [1. 1. 0. 0. 2.]\n",
      " [1. 1. 0. 1. 1.]\n",
      " [1. 1. 1. 0. 2.]\n",
      " [1. 1. 1. 1. 2.]\n",
      " [2. 0. 0. 0. 2.]\n",
      " [2. 0. 0. 1. 2.]\n",
      " [2. 0. 1. 0. 2.]\n",
      " [2. 0. 1. 1. 0.]\n",
      " [2. 1. 0. 0. 2.]\n",
      " [2. 1. 0. 1. 1.]\n",
      " [2. 1. 1. 0. 2.]\n",
      " [2. 1. 1. 1. 2.]]\n"
     ]
    }
   ],
   "source": [
    "dataset = Datos('lenses.data')\n",
    "\n",
    "print(dataset.datos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from abc import ABCMeta,abstractmethod\n",
    "\n",
    "\n",
    "class Particion():\n",
    "\n",
    "  # Esta clase mantiene la lista de �ndices de Train y Test para cada partici�n del conjunto de particiones\n",
    "    def __init__(self,train=[],test=[]):\n",
    "        self.indicesTrain=train\n",
    "        self.indicesTest=test\n",
    "    \n",
    "    def __str__(self):\n",
    "        return \"Train: {}\\nTest:  {}\".format(str(self.indicesTrain),str(self.indicesTest)) \n",
    "\n",
    "#####################################################################################################\n",
    "\n",
    "class EstrategiaParticionado:\n",
    "\n",
    "    # Clase abstracta\n",
    "    __metaclass__ = ABCMeta\n",
    "\n",
    "    # Lista de las particiones\n",
    "    def __init__(self, nombre=\"\"):\n",
    "        self.nombreEstrategia = nombre\n",
    "        self.numeroParticiones = 0\n",
    "        self.particiones=[]\n",
    "\n",
    "    # Atributos: deben rellenarse adecuadamente para cada estrategia concreta: nombreEstrategia, numeroParticiones, listaParticiones. Se pasan en el constructor\n",
    "\n",
    "    @abstractmethod\n",
    "    # TODO: esta funcion deben ser implementadas en cada estrategia concreta\n",
    "    def creaParticiones(self,datos,seed=None):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class ValidacionSimple(EstrategiaParticionado):\n",
    "    \n",
    "    def __init__(self, porcentaje):\n",
    "        self.porcentaje = porcentaje\n",
    "        super().__init__(\"Validacion simple\")\n",
    "        \n",
    "  # Crea particiones segun el metodo tradicional de division de los datos segun el porcentaje deseado.\n",
    "  # Devuelve una lista de particiones (clase Particion)\n",
    "  # TODO: implementar\n",
    "    def creaParticiones(self,datos,seed=None):\n",
    "        np.random.seed(seed)\n",
    "\n",
    "        self.numeroParticiones = 1\n",
    "    \n",
    "        # Generamos una lista con todos los números de datos aleatorios   \n",
    "        indicesAleatorios = np.random.permutation(int(datos.numDatos))\n",
    "        \n",
    "        # Creamos la particion, en funcion del porcentaje especificado\n",
    "        self.particiones = [Particion(indicesAleatorios[:int(datos.numDatos*self.porcentaje)],\n",
    "                                      indicesAleatorios[int(datos.numDatos*self.porcentaje):])]\n",
    "        \n",
    "        return self.particiones\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [10 23  2  6 20 18 11 16  1 15 12  4 13 17  5 19  7  3]\n",
      "Test:  [21  0  9 22 14  8]\n"
     ]
    }
   ],
   "source": [
    "validacion_simple = ValidacionSimple(0.75)\n",
    "validacion_simple.creaParticiones(dataset)\n",
    "print(validacion_simple.particiones[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class ValidacionCruzada(EstrategiaParticionado):\n",
    "\n",
    "  # Crea particiones segun el metodo de validacion cruzada.\n",
    "  # El conjunto de entrenamiento se crea con las nfolds-1 particiones y el de test con la particion restante\n",
    "  # Esta funcion devuelve una lista de particiones (clase Particion)\n",
    "  # TODO: implementar\n",
    "    \n",
    "    def __init__(self, k):\n",
    "        self.k = k\n",
    "        super().__init__(\"Validacion cruzada\")\n",
    "\n",
    "    def creaParticiones(self,datos,seed=None):\n",
    "        np.random.seed(seed)\n",
    "        \n",
    "        self.numeroParticiones = self.k\n",
    "        \n",
    "        # Generamos una lista con todos los números de datos aleatorios   \n",
    "        indicesAleatorios = np.random.permutation(int(datos.numDatos))\n",
    "        \n",
    "        # Hallamos el tamaño de cada bloque\n",
    "        tamBloque = int(datos.numDatos/self.k)\n",
    "        \n",
    "        \n",
    "        datosSobran = datos.numDatos - (tamBloque*self.k)\n",
    "        count = 0\n",
    "        for i in range(self.k):\n",
    "            \n",
    "            train = np.delete(indicesAleatorios, range(i*tamBloque,(i+1)*tamBloque))\n",
    "            test =  indicesAleatorios[i*tamBloque:(i+1)*tamBloque]\n",
    "            \n",
    "            # Caso en el que la cuenta es justa\n",
    "            if datosSobran == 0:\n",
    "                self.particiones.append(Particion(train, test))\n",
    "                \n",
    "            # Contemplamos el caso de que la division para sacar el numero de subconjuntos no fuese entera\n",
    "            if datosSobran > 0:\n",
    "                count += 1\n",
    "                particionTest = np.append(test, train[(datos.numDatos - tamBloque)- i - 1])\n",
    "                particionTrain = np.delete(train, (datos.numDatos - tamBloque)- i - 1)\n",
    "                datosSobran -= 1\n",
    "                self.particiones.append(Particion(particionTrain, particionTest))\n",
    "                \n",
    "            \n",
    "           \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [ 1 13 22 10  0 20 21 17 16 11 14 23  2  4  3 19  7  9]\n",
      "Test:  [ 8 15 18 12  6  5]\n",
      "Train: [ 8 15 18 12  6  5 21 17 16 11 14 23  2  4  3 19  7  9]\n",
      "Test:  [ 1 13 22 10  0 20]\n",
      "Train: [ 8 15 18 12  6  5  1 13 22 10  0 20  2  4  3 19  7  9]\n",
      "Test:  [21 17 16 11 14 23]\n",
      "Train: [ 8 15 18 12  6  5  1 13 22 10  0 20 21 17 16 11 14 23]\n",
      "Test:  [ 2  4  3 19  7  9]\n"
     ]
    }
   ],
   "source": [
    "v_cruzada = ValidacionCruzada(4)\n",
    "v_cruzada.creaParticiones(dataset)\n",
    "\n",
    "for particion in v_cruzada.particiones:\n",
    "    print(particion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class Clasificador:\n",
    "  \n",
    "    # Clase abstracta\n",
    "    __metaclass__ = ABCMeta\n",
    "\n",
    "    # Metodos abstractos que se implementan en casa clasificador concreto\n",
    "    @abstractmethod\n",
    "    # TODO: esta funcion debe ser implementada en cada clasificador concreto\n",
    "    # datosTrain: matriz numpy con los datos de entrenamiento\n",
    "    # atributosDiscretos: array bool con la indicatriz de los atributos nominales\n",
    "    # diccionario: array de diccionarios de la estructura Datos utilizados para la codificacion de variables discretas\n",
    "    def entrenamiento(self,datos,datosTrain,atributosDiscretos,diccionario):\n",
    "        pass\n",
    "\n",
    "\n",
    "    @abstractmethod\n",
    "    # TODO: esta funcion debe ser implementada en cada clasificador concreto\n",
    "    # devuelve un numpy array con las predicciones\n",
    "    def clasifica(self,datosTest,atributosDiscretos,diccionario):\n",
    "        pass\n",
    "\n",
    "\n",
    "    # Obtiene el numero de aciertos y errores para calcular la tasa de fallo\n",
    "    # TODO: implementar\n",
    "    def error(self,datos,pred):\n",
    "    # Aqui se compara la prediccion (pred) con las clases reales y se calcula el error    \n",
    "        i = 0\n",
    "        real = datos[:,-1]\n",
    "        error = 0\n",
    "        for i in range(len(real)):\n",
    "            if real[i] != pred[i]:\n",
    "                error += 1\n",
    "        err = (error)/(len(real)+0.0)\n",
    "        return err\n",
    "\n",
    "\n",
    "    # Realiza una clasificacion utilizando una estrategia de particionado determinada\n",
    "    # TODO: implementar esta funcion\n",
    "    def validacion(self,particionado,dataset,clasificador,seed=None):\n",
    "\n",
    "    # Creamos las particiones siguiendo la estrategia llamando a particionado.creaParticiones\n",
    "    # - Para validacion cruzada: en el bucle hasta nv entrenamos el clasificador con la particion de train i\n",
    "    # y obtenemos el error en la particion de test i\n",
    "    # - Para validacion simple (hold-out): entrenamos el clasificador con la particion de train\n",
    "    # y obtenemos el error en la particion test. Otra opci�n es repetir la validaci�n simple un n�mero especificado de veces, obteniendo en cada una un error. Finalmente se calcular�a la media.\n",
    "        errores = 0\n",
    "        #particionado.creaParticiones(dataset, seed)\n",
    "    # Comprobamos si es por validación cruzada o simple, por la longitud de la lista de particiones\n",
    "    # Validación Simple\n",
    "        if len(particionado.particiones) == 1:\n",
    "            clasificador.entrenamiento(dataset, particionado.particiones[0].indicesTrain)\n",
    "            pred = clasificador.clasifica(dataset,particionado.particiones[0].indicesTest)\n",
    "            ret = self.error(dataset.extraeDatos(particionado.particiones[0].indicesTest), pred)\n",
    "            if ret > 0:\n",
    "                return ret\n",
    "            else:\n",
    "                return 0\n",
    "            \n",
    "    # Validación Cruzada        \n",
    "        else:\n",
    "            for particion in particionado.particiones:\n",
    "                clasificador.entrenamiento(dataset, particion.indicesTrain)\n",
    "                pred = clasificador.clasifica(dataset,particion.indicesTest)\n",
    "                ret = self.error(dataset.extraeDatos(particion.indicesTest), pred)\n",
    "                errores += ret\n",
    "            error = errores/len(particionado.particiones)\n",
    "            #Devolucion de la media de los errores\n",
    "            return error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class ClasificadorNaiveBayes(Clasificador):\n",
    "    \n",
    "    def __init__(self, laplace):\n",
    "        self.laplace = laplace\n",
    "        \n",
    "    \n",
    "    def entrenamiento(self,dataset,datosTrain):\n",
    "     \n",
    "        # Cargamos todos los datos de la clase del dataset desde la matriz de datos\n",
    "        clasesTrain = dataset.extraeDatos(datosTrain)\n",
    "        self.numClases = clasesTrain[:,-1]\n",
    "        # Contamos las apariciones de cada uno para luego calcular la probabilidad a priori de cada clase\n",
    "        counter = Counter(self.numClases)\n",
    "        # Calculamos la probabilidad de la clase y lo metemos en un diccionario ordenado segun el numero \n",
    "        # correspondiente a cada clase asignado en el diccionario\n",
    "        self.dictPrioris={}\n",
    "        for k in counter:\n",
    "            k = int(k)\n",
    "            counter[k] = counter[k]/len(self.numClases)\n",
    "            self.dictPrioris[k] = counter[k]\n",
    "            \n",
    "        # Aqui ordenamos el diccionario para que esten en el mismo orden de como extraemos los datos del dataset\n",
    "        self.dictPrioris = SortedDict(self.dictPrioris)\n",
    "        \n",
    "        # Calcular tablas de probabilidades del entrenamiento. Tenemos que calcular por cada atributo una cuenta\n",
    "        # de las apariciones en cada clase\n",
    "        # Creamos una lista de matrices, donde vamos almacenar todos los datos que hemos obtenido en los datos de Test\n",
    "        self.posteriori = np.zeros(len(dataset.nombreAtributos)-1,dtype=object)\n",
    "        \n",
    "        # Recorremos todos los datos de la matriz sin llegar a la clase\n",
    "        for i in range(len(dataset.nombreAtributos) - 1):\n",
    "            \n",
    "            # Si el dato que obtenemos es Nominal haremos el recuento de todas las veces que sale la P(D|H)\n",
    "            if dataset.nominalAtributos[i] == True:\n",
    "                \n",
    "                # Creamos una matriz de tamaño X: Número de Atributos menos la clase Y: Número de clases\n",
    "                post = np.zeros((len(dataset.listaDicts[i]),len(dataset.listaDicts[-1])))\n",
    "               \n",
    "                # Aqui contamos todos las datos que queremos del datos Train para construir la matriz de entrenamiento\n",
    "                for c in range(len(dataset.listaDicts[-1])):\n",
    "                    datosEnt = dataset.extraeDatos(datosTrain)\n",
    "                    dat = datosEnt[:,i]\n",
    "                    repes = Counter(dat[datosEnt[:,-1] == c])\n",
    "                    for r in repes:\n",
    "                        post[int(r),c] = repes[r]\n",
    "                    if self.laplace == True:\n",
    "                        self.posteriori[i] = post + 1\n",
    "                    else:\n",
    "                        self.posteriori[i] = post\n",
    "            \n",
    "            # Si el dato es Continuo obtendremos la media y la desviación tipica de la clase\n",
    "            else:\n",
    "                \n",
    "                # Creamos una matriz de X: Los datos de Media y Desivación típica Y: Número de clases\n",
    "                post = np.zeros((2,len(dataset.listaDicts[-1])))\n",
    "               \n",
    "                # Aqui obtenemos la media y desviación tipica de cada clase, despues de tener los datos de entrenamiento\n",
    "                for c in range(len(dataset.listaDicts[-1])):\n",
    "                    datosEnt = dataset.extraeDatos(datosTrain)\n",
    "                    dat = datosEnt[:,i]\n",
    "                    datos = dat[datosEnt[:,-1] == c]\n",
    "                    post[0][c] = np.mean(datos)\n",
    "                    post[1][c] = np.std(datos)\n",
    "                self.posteriori[i] = post\n",
    "        \n",
    "        #Calculamos los valores de los posteriori de todos las tablas anteriores\n",
    "        for i in range(len(dataset.listaDicts) - 1):\n",
    "            if dataset.nominalAtributos[i] == True:\n",
    "                self.posteriori[i] /= sum(self.posteriori[i])\n",
    "        \n",
    "        print(self.posteriori)\n",
    "        \n",
    "    def clasifica(self,dataset,datosTest):\n",
    "        j = 0\n",
    "        aux = 1\n",
    "        aux2 = 1\n",
    "        self.prediccion = []\n",
    "        datTest = dataset.extraeDatos(datosTest)\n",
    "        #Ahora vamos a estudiar la probabilidad de la clase con los datos obtenidos en el entrenamiento\n",
    "        #Recorremos todos las datos de la matriz de los datos Test\n",
    "        \n",
    "        for dato in datTest:\n",
    "            mapa = []\n",
    "            listaVerosimilitudes = []\n",
    "            #Aqui obtenemos los prioris de cada clase para poder obtener la probabilidad de cada una\n",
    "            for clase in range(len(self.dictPrioris)):\n",
    "            #Aqui obtenemos cada valor posteriori de nuestro entrenamiento de los datos, es decir, P(D|H)\n",
    "                for atributo in range(len(self.posteriori)):\n",
    "                    if dataset.nominalAtributos[atributo] == True:\n",
    "                        aux = self.posteriori[atributo][int(dato[atributo])][clase]\n",
    "                        listaVerosimilitudes.append(aux)\n",
    "                    #Aqui obtenemos la siguiente probabilidad P(D|H)*P(H)\n",
    "                        #aux2 = self.dictPrioris.get(clase)*aux\n",
    "                        #aux = 1\n",
    "                \n",
    "                    #Lo añadimos a una lista para obtener la probabilidad de las diferentes clases\n",
    "                        #mapa.append(aux2)\n",
    "            \n",
    "            #Aqui obtenemos la probabilidad de los atibutos continuos\n",
    "                    else:\n",
    "                        # Hacemos la formula de la distribucion normal\n",
    "                        exp1 = 1/(self.posteriori[atributo][1][clase]*math.sqrt(2*math.pi))\n",
    "                        exp2 = np.power((dato[atributo]-self.posteriori[atributo][0][clase]) ,2)\n",
    "                        exp3 = np.power(self.posteriori[atributo][1][clase],2)\n",
    "                        exp4 = exp2/exp3   \n",
    "                        exp4 = math.exp((-1/2)* exp3)\n",
    "                        aux = exp1 * exp4\n",
    "                        listaVerosimilitudes.append(aux)\n",
    "                    #aux2 = aux * self.dictPrioris.get(clase)\n",
    "                    for verosimilitud in listaVerosimilitudes:\n",
    "                        aux2 *= verosimilitud\n",
    "                    aux2 *= self.dictPrioris.get(clase)\n",
    "                    mapa.append(aux2)\n",
    "            \n",
    "            #Aqui obtenemos la predicción de mayor probabilidad y la guardamos en nuestra lista de predicciones\n",
    "            self.prediccion.append(np.argmax(mapa))\n",
    "        \n",
    "        #Devolvemos la lista con la predicción de nuestro clasifica   \n",
    "        return self.prediccion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Age', 'Spectacle', 'Astigmatic', 'Tear', 'Class']\n",
      "[{'1': 0, '2': 1, '3': 2}, {'1': 0, '2': 1}, {'1': 0, '2': 1}, {'1': 0, '2': 1}, {'1': 0, '2': 1, '3': 2}]\n",
      "VALIDACION SIMPLE Train: [ 2 14 18 20  0  5 16 23  9 13 19  8 21 17  4  7 12 22]\n",
      "Test:  [11  1  6  3 10 15]\n",
      "VALIDACION CRUZADA\n",
      "Train: [11 12  6 23  1 15  9 19  3  2 21 16  0 17 20 22  8 18]\n",
      "Test:  [ 5 13 10  4  7 14]\n",
      "Train: [ 5 13 10  4  7 14  9 19  3  2 21 16  0 17 20 22  8 18]\n",
      "Test:  [11 12  6 23  1 15]\n",
      "Train: [ 5 13 10  4  7 14 11 12  6 23  1 15  0 17 20 22  8 18]\n",
      "Test:  [ 9 19  3  2 21 16]\n",
      "Train: [ 5 13 10  4  7 14 11 12  6 23  1 15  9 19  3  2 21 16]\n",
      "Test:  [ 0 17 20 22  8 18]\n",
      "[array([[0.4       , 0.28571429, 0.26666667],\n",
      "       [0.2       , 0.42857143, 0.26666667],\n",
      "       [0.4       , 0.28571429, 0.46666667]])\n",
      " array([[0.5       , 0.33333333, 0.5       ],\n",
      "       [0.5       , 0.66666667, 0.5       ]])\n",
      " array([[0.25      , 0.83333333, 0.57142857],\n",
      "       [0.75      , 0.16666667, 0.42857143]])\n",
      " array([[0.25      , 0.16666667, 0.78571429],\n",
      "       [0.75      , 0.83333333, 0.21428571]])]\n",
      "[array([[0.33333333, 0.33333333, 0.26666667],\n",
      "       [0.33333333, 0.33333333, 0.26666667],\n",
      "       [0.33333333, 0.33333333, 0.46666667]])\n",
      " array([[0.8, 0.6, 0.5],\n",
      "       [0.2, 0.4, 0.5]])\n",
      " array([[0.2, 0.8, 0.5],\n",
      "       [0.8, 0.2, 0.5]])\n",
      " array([[0.2       , 0.2       , 0.71428571],\n",
      "       [0.8       , 0.8       , 0.28571429]])]\n",
      "[array([[0.5       , 0.28571429, 0.28571429],\n",
      "       [0.16666667, 0.42857143, 0.28571429],\n",
      "       [0.33333333, 0.28571429, 0.42857143]])\n",
      " array([[0.6       , 0.33333333, 0.61538462],\n",
      "       [0.4       , 0.66666667, 0.38461538]])\n",
      " array([[0.2       , 0.83333333, 0.53846154],\n",
      "       [0.8       , 0.16666667, 0.46153846]])\n",
      " array([[0.2       , 0.16666667, 0.84615385],\n",
      "       [0.8       , 0.83333333, 0.15384615]])]\n",
      "[array([[0.4       , 0.5       , 0.25      ],\n",
      "       [0.4       , 0.33333333, 0.375     ],\n",
      "       [0.2       , 0.16666667, 0.375     ]])\n",
      " array([[0.5, 0.4, 0.4],\n",
      "       [0.5, 0.6, 0.6]])\n",
      " array([[0.25      , 0.8       , 0.46666667],\n",
      "       [0.75      , 0.2       , 0.53333333]])\n",
      " array([[0.25      , 0.2       , 0.73333333],\n",
      "       [0.75      , 0.8       , 0.26666667]])]\n",
      "[array([[0.42857143, 0.375     , 0.33333333],\n",
      "       [0.28571429, 0.375     , 0.41666667],\n",
      "       [0.28571429, 0.25      , 0.25      ]])\n",
      " array([[0.66666667, 0.42857143, 0.36363636],\n",
      "       [0.33333333, 0.57142857, 0.63636364]])\n",
      " array([[0.16666667, 0.85714286, 0.36363636],\n",
      "       [0.83333333, 0.14285714, 0.63636364]])\n",
      " array([[0.16666667, 0.14285714, 0.72727273],\n",
      "       [0.83333333, 0.85714286, 0.27272727]])]\n",
      "El error del Clasificador NaiveBayes  para Validacion Simple es: 0.6666666666666666\n",
      "El error del Clasificador NaiveBayes para Validacion Cruzada es: 0.8333333333333334\n"
     ]
    }
   ],
   "source": [
    "dataset = Datos('lenses.data')\n",
    "\n",
    "validacion_simple = ValidacionSimple(0.75)\n",
    "validacion_simple.creaParticiones(dataset)\n",
    "v_cruzada = ValidacionCruzada(4)\n",
    "v_cruzada.creaParticiones(dataset)\n",
    "\n",
    "print(\"VALIDACION SIMPLE\",validacion_simple.particiones[0])\n",
    "\n",
    "print(\"VALIDACION CRUZADA\")\n",
    "for particion in v_cruzada.particiones:\n",
    "    print(particion)\n",
    "\n",
    "\n",
    "clasificador = ClasificadorNaiveBayes(True)\n",
    "#clasificador.entrenamiento(dataset, validacion_simple.particiones[0].indicesTrain)\n",
    "#clasificador.clasifica(dataset, validacion_simple.particiones[0].indicesTest)\n",
    "error = clasificador.validacion(validacion_simple,dataset,clasificador)\n",
    "error1 = clasificador.validacion(v_cruzada,dataset,clasificador)\n",
    "print(\"El error del Clasificador NaiveBayes  para Validacion Simple es: \"+str(error))\n",
    "print(\"El error del Clasificador NaiveBayes para Validacion Cruzada es: \"+str(error1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['TLeftSq', 'TMidSq', 'TRightSq', 'MLeftSq', 'MMidSq', 'MRightSq', 'BLeftSq', 'BMidSq', 'BRightSq', 'Class']\n",
      "[{'b': 0, 'o': 1, 'x': 2}, {'b': 0, 'o': 1, 'x': 2}, {'b': 0, 'o': 1, 'x': 2}, {'b': 0, 'o': 1, 'x': 2}, {'b': 0, 'o': 1, 'x': 2}, {'b': 0, 'o': 1, 'x': 2}, {'b': 0, 'o': 1, 'x': 2}, {'b': 0, 'o': 1, 'x': 2}, {'b': 0, 'o': 1, 'x': 2}, {'negative': 0, 'positive': 1}]\n",
      "[[2. 2. 2. ... 1. 1. 1.]\n",
      " [2. 2. 2. ... 2. 1. 1.]\n",
      " [2. 2. 2. ... 1. 2. 1.]\n",
      " ...\n",
      " [1. 2. 1. ... 1. 2. 0.]\n",
      " [1. 2. 1. ... 1. 2. 0.]\n",
      " [1. 1. 2. ... 2. 2. 0.]]\n"
     ]
    }
   ],
   "source": [
    "dataset = Datos('tic-tac-toe.data')\n",
    "print(dataset.datos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VALIDACION SIMPLE Train: [783 490 950 735 855 114 360 741 286 555 179 296 109 193 612 519 800 229\n",
      " 849 771 891 533 649 327 790 207 657 576 408 161   2 453 271 278 734  96\n",
      " 943 136 556 169 395 619 727 369 819 331 303 941 374 905 754 291 840 319\n",
      "  33 643 957 514 219 564 701  99 144 546  63 908 377 353 164 486 656 324\n",
      " 895 744 842 104 538 308 607 347 380 216  72 390 893  91 357 696 591 530\n",
      "  79 138 631 238 798 243 833 854 641 378 304 223  94 251  15 312 700 737\n",
      " 416  31 860  80 276 823 863 338 156 418 780 911 392 715 723 261 586 232\n",
      " 439 579 677 588 826 709 717 875 901 874 181 242 567 830 410 436 236 603\n",
      " 926 496 367 589 165 124 337  17  69 195  46 764 542 153 821 774 134 608\n",
      " 801 836 180 906 454 518 267 280 651 379 802 192 786 565 814 756 864 472\n",
      " 655 503 550 174 932 406  13 707 433 215 295 497  74 585 539 610 147 102\n",
      " 817 168  41 113 545 264 345 523 729 832 103 279 277 812 805 557 804 307\n",
      " 535 465 351 776 653  90 116 675 464 505 362 865 639 697 676  93 450 435\n",
      " 182 809 558 910 660 140 473  11  40 325 779 125 647 167  28 385 806  25\n",
      " 937 770   9 751 438 945 100 803 417 721 461   1 446 218 759 622 487 807\n",
      " 769 702  21 695 201 162 186 112 531 534 746 728 732 428 668 206 596 838\n",
      " 329 614 292 253 440 644 320 654 400 632 158 128 311  16 422 187 851 334\n",
      " 917  18 789 326 877 894 365 194 332  64 784 536 506  20 752 270  49 358\n",
      " 507 274 148 866 284 227 513 629  10 110 887  71 130 625 516 314 857 810\n",
      " 693 665 254 188 415 742 263 413 300 689 121 811 663 424 587 455 837 118\n",
      "  78 858 318 584 527 432 115 661 172 923 885 882 582 711  57 611 678 317\n",
      " 847 955 768 199  84 620 719 383  88 621 425  47 445 690  95 479 618 600\n",
      " 645 509 184 398 763 521 841 590  59 177 477 373 548 419 474 831 949 429\n",
      " 672  36 792 683 480 934 452 795 671 914 907  22 204 820 919 933 659 131\n",
      " 745 190 773   8 547 245 740 281 171 946 173 132 481 344 879 722 581  81\n",
      " 884 664 935 107 630 601 583 559 268 352 259 203 166 262 869 580 504 145\n",
      "  27 241 767 904 456 747 120 694 606 524   3 844 778 298 265 290  55  30\n",
      " 881 699 191 794 604 679 411 902 391 944 868 467 939 510 593 688 569 552\n",
      " 867 515 626 213 212 637 822 839 748 233 202 294 568 825 482 755 155 952\n",
      " 948 680 526 685 222 799 355 684 205 813 460 412 210 750 617 782 725 108\n",
      " 476 541 896 499 293 704 666 597 152 240 736 931  50 749 495 682 421 302\n",
      " 217 349 211 873 198 670 368 903 414   6 484 918 340 731 404 248 500 890\n",
      " 301 409  86 388  89 954 350 636 781 615  37 197 396  24 106 936 852 501\n",
      "  77 237 912 170 517 381 397 339 667 348 613  52 720 346 146 648 209 703\n",
      " 578 275 382 892 602 554 816 525 599 791 469 256 658 898 880 111  34 123\n",
      " 940 942 384 122 119 228  35 266  32  66 669 528 916 928   0 285  54  70\n",
      " 927 289 309 399 566  51 366 815 283 760 257 544 540 305 915  58 127 370\n",
      " 834 797 570 431   4 230 862 859 561 492 137 710 956 297 726 787 343 258\n",
      " 686 386 364 483 117 818 628 640 143 828  23 250 478 157 758 883 475 574\n",
      " 951 246 342 872 185 777 371 235   5 341 458 394 543 150 272  12]\n",
      "Test:  [288 126 470 635 638 226 502 563 336 463 785 762 909 753 824 234 224 691\n",
      " 577 650 176  14 947 924  61 462  62 609 149 494 407 426  97 642 151 848\n",
      " 449 772 724 616 853 356 401 633 299 447 322 766 553 208 214 508 287  98\n",
      " 200 361 282 845 133 403  29 706 310 889 220 457 761 827  76 560 485 692\n",
      " 269 575 876 738 244 718 471  42 627 624 856 922 489 101  38 442 730 154\n",
      " 870  65 843 273  85  44 387 646 733 183  87 673 708 141 572 835 861 788\n",
      " 189 698 938 249 594  19 888 163 549 491   7 376 459 231 139 335  56 899\n",
      " 846  26 466 225 511 448 796 739 330 333 634 930 522 712 405 354 363 605\n",
      " 427 705 929 592 716 221 451 850  43  45 437 420 135 743 532 323 775 498\n",
      " 595 808 537 444 681 623 328 921  53 315 252 662 239 897  68 598 562 900\n",
      "  60 493 142 793 520  75 687 713 871 765 105 878 175  83 247 573  48 178\n",
      "  67 159 129 255 316 372  82  73 306 829  92 359 160 389 443 488 468 196\n",
      " 714 260 423 313 529 512 434  39 913 953 652 886 441 375 920 430 402 551\n",
      " 321 925 571 393 757 674]\n",
      "VALIDACION CRUZADA\n",
      "Train: [147 420 361  20 702 365 317 716 708 339 662 652 942 766 635 256 231  86\n",
      "  98 279 200 532  61 703 812 505 433 722 745 736  33 833 404 508 730 554\n",
      " 206 670 620 771 801 135 252 639 410 794 910 709 164 260 957 358 440 447\n",
      " 673 892 553 764 407 792 431 487 518 645  50 896 449 869 108 262 924 825\n",
      "  97 788 588 287 306 818 340 370 750 807 527 497 837 129 280 866 543 383\n",
      " 586 284 391 380 329 174 781 249 632 634 530 844 663 895 952 686 167 861\n",
      " 817  77 126 403 245 259 198 432 354 606 205 599 134 881  91 889 357 762\n",
      " 415 748 457 669 876 185 136 525 392 318 533 376 539 679 401 246 882 850\n",
      " 319 393 191 159 885 142 915 883 572 898 760 255 515 425 331   1 301 257\n",
      " 513 540 105 944 683 887  42 229 194 146 653 155 411 746 406 751 305 117\n",
      " 538 894 149 920 773 907 737 150 689 519 517 371 228 754 930 486 544 753\n",
      " 215 767 552 103 360 467 373 842 580 496 902  46 102 590 491 163 698  32\n",
      " 237 697 315 499 922 855 272 196 537 688 803 796  15 494 741 768 274 520\n",
      " 498 398 456 731 250 871  87 643 523 918 562 641  80 880 798 927 900 582\n",
      " 678 210 955 375 345 211 123 890 658 227 454  94  81 278 408  49 320 548\n",
      " 835 492 785 529 672  60 659 647 462 221 558 330 516 765 926 704  58 867\n",
      " 603 309 170 422 469 522 682 839 865 180 830  65 592 916 637 619  68 389\n",
      " 729 946 680 112 514  29 727 169 831 655 868 854 489 203 846 444  17 197\n",
      " 941 879  64 502 261 923  40 630 834 266 316 235 739 589 341 445  34 524\n",
      " 624  12  52  88 302 343 849 724 156 222 743 336 119 591 369 954 559 929\n",
      " 312 173 468 717 560 674 845 625 276 705 550  79 909 853 218 780 304  28\n",
      " 819 692 289 113 332  51 417  56 193 904 382 128 201 840 847 793 604 615\n",
      " 490 242 953 813 356 295 479 534  23 452 125 300 346 897 501 212 790 810\n",
      " 396 400 642  43 395 352 307 949 814 226 323 500 570 475 791 614 917 526\n",
      " 610 774 473 429 956 556 258 364 160 775  21 701 597 759 511 378   2 943\n",
      " 480 622 786 240 152 344 623 233 118 567  82 732 512   6  59 877 596 141\n",
      " 824 598 782  83 752 779 563 461 578  13 687 153 789 384 947 656 481 903\n",
      " 273   4 684 649 654  70 435 477 756 275 761 611  55 557 843 800 795  90\n",
      " 747 148 878  96 372 269 864 875  92  84 939 140 374 294 347  89 706 130\n",
      " 151 607 458 291 224 665 434 267 832 940 860 451 385 555 509 270 935 414\n",
      " 551 439 671  39 430 333 124 324 594 321 579  72 528 693 183 423 681 377\n",
      " 584 162 805 107 109 326 950 138 585 476 593 465 636 823 696 806 783 777\n",
      " 427 308  99  30 234 460 734 888 690 145 353 608 755 718 248 351 733  26\n",
      " 723 428 852 379 618 241 613 466 925 251 776 797 362 576 171 485 816 595\n",
      " 884 612  76 114 219 521 446 799 472 131 204  38 933 504 667 561 188 127\n",
      " 858 101 292 886 628 740 303 601 841 873 106 327 182 359 848 399 483 386\n",
      " 244  31 908 325 784 836 474  69 931 617 758 418 564 726 363 436 314 820\n",
      " 905 405 644 568 536 616  85 699 175 826 390 455 891 264 402  74 161 448\n",
      " 934  93 566 862 945 236  24 311 176 809 488 154 685 285 243 388 328 310\n",
      " 675 787 802 928 621 166 263 574  57 725  11 214 660 770 181 691]\n",
      "Test:  [ 78  73  63 811 804 277 412 666  47 421 168 633 857 661 120 648 664  41\n",
      " 184 827 350 132 133 442 268 631 143 397 874  95 283 822 338 506 100 605\n",
      " 207 545 282 575 600  62  16 721 482  22 367 409 914 932 829 139  19 216\n",
      " 651 919 186   3 115 413 921 495 735 179 394 355 893 677   9 657 225  27\n",
      " 337 913 290 322 602 177 581 137 772 223 763 463 728 334 453   8 195 313\n",
      "  54 700 286 189 172 254 202 178 232 738 872 573 901 571  71 110 936 165\n",
      " 911 157 426  35 710 531 906 342  67 450 650 859 122 464 744 437 281 247\n",
      " 714 493   7  25 507 335 757 416 443 381 838 187  44 912 646 808 587 298\n",
      "  45 253 715 778 293 769 199 711 815 742 299 478 190 419 288 297 368 712\n",
      " 484 441 749 609 546 863  53 271 239 471 547 899 470 828 719 937 158 948\n",
      " 209 856 366 296 870 424 583 349 265 217 627 144 510 626   0 707 629  37\n",
      " 535 104 951 695   5  18  66  75 503 121  14 213  48 541 116 720 230 577\n",
      " 713 111 569 387 459 192 821 438 565 640 238  36 676  10 348 938 220 208\n",
      " 851 542 694 549 668 638]\n",
      "Train: [ 78  73  63 811 804 277 412 666  47 421 168 633 857 661 120 648 664  41\n",
      " 184 827 350 132 133 442 268 631 143 397 874  95 283 822 338 506 100 605\n",
      " 207 545 282 575 600  62  16 721 482  22 367 409 914 932 829 139  19 216\n",
      " 651 919 186   3 115 413 921 495 735 179 394 355 893 677   9 657 225  27\n",
      " 337 913 290 322 602 177 581 137 772 223 763 463 728 334 453   8 195 313\n",
      "  54 700 286 189 172 254 202 178 232 738 872 573 901 571  71 110 936 165\n",
      " 911 157 426  35 710 531 906 342  67 450 650 859 122 464 744 437 281 247\n",
      " 714 493   7  25 507 335 757 416 443 381 838 187  44 912 646 808 587 298\n",
      "  45 253 715 778 293 769 199 711 815 742 299 478 190 419 288 297 368 712\n",
      " 484 441 749 609 546 863  53 271 239 471 547 899 470 828 719 937 158 948\n",
      " 209 856 366 296 870 424 583 349 265 217 627 144 510 626   0 707 629  37\n",
      " 535 104 951 695   5  18  66  75 503 121  14 213  48 541 116 720 230 577\n",
      " 713 111 569 387 459 192 821 438 565 640 238  36 676  10 348 938 220 208\n",
      " 851 542 694 549 668 871  87 643 523 918 562 641  80 880 798 927 900 582\n",
      " 678 210 955 375 345 211 123 890 658 227 454  94  81 278 408  49 320 548\n",
      " 835 492 785 529 672  60 659 647 462 221 558 330 516 765 926 704  58 867\n",
      " 603 309 170 422 469 522 682 839 865 180 830  65 592 916 637 619  68 389\n",
      " 729 946 680 112 514  29 727 169 831 655 868 854 489 203 846 444  17 197\n",
      " 941 879  64 502 261 923  40 630 834 266 316 235 739 589 341 445  34 524\n",
      " 624  12  52  88 302 343 849 724 156 222 743 336 119 591 369 954 559 929\n",
      " 312 173 468 717 560 674 845 625 276 705 550  79 909 853 218 780 304  28\n",
      " 819 692 289 113 332  51 417  56 193 904 382 128 201 840 847 793 604 615\n",
      " 490 242 953 813 356 295 479 534  23 452 125 300 346 897 501 212 790 810\n",
      " 396 400 642  43 395 352 307 949 814 226 323 500 570 475 791 614 917 526\n",
      " 610 774 473 429 956 556 258 364 160 775  21 701 597 759 511 378   2 943\n",
      " 480 622 786 240 152 344 623 233 118 567  82 732 512   6  59 877 596 141\n",
      " 824 598 782  83 752 779 563 461 578  13 687 153 789 384 947 656 481 903\n",
      " 273   4 684 649 654  70 435 477 756 275 761 611  55 557 843 800 795  90\n",
      " 747 148 878  96 372 269 864 875  92  84 939 140 374 294 347  89 706 130\n",
      " 151 607 458 291 224 665 434 267 832 940 860 451 385 555 509 270 935 414\n",
      " 551 439 671  39 430 333 124 324 594 321 579  72 528 693 183 423 681 377\n",
      " 584 162 805 107 109 326 950 138 585 476 593 465 636 823 696 806 783 777\n",
      " 427 308  99  30 234 460 734 888 690 145 353 608 755 718 248 351 733  26\n",
      " 723 428 852 379 618 241 613 466 925 251 776 797 362 576 171 485 816 595\n",
      " 884 612  76 114 219 521 446 799 472 131 204  38 933 504 667 561 188 127\n",
      " 858 101 292 886 628 740 303 601 841 873 106 327 182 359 848 399 483 386\n",
      " 244  31 908 325 784 836 474  69 931 617 758 418 564 726 363 436 314 820\n",
      " 905 405 644 568 536 616  85 699 175 826 390 455 891 264 402  74 161 448\n",
      " 934  93 566 862 945 236  24 311 176 809 488 154 685 285 243 388 328 310\n",
      " 675 787 802 928 621 166 263 574  57 725  11 214 660 770 181 638]\n",
      "Test:  [147 420 361  20 702 365 317 716 708 339 662 652 942 766 635 256 231  86\n",
      "  98 279 200 532  61 703 812 505 433 722 745 736  33 833 404 508 730 554\n",
      " 206 670 620 771 801 135 252 639 410 794 910 709 164 260 957 358 440 447\n",
      " 673 892 553 764 407 792 431 487 518 645  50 896 449 869 108 262 924 825\n",
      "  97 788 588 287 306 818 340 370 750 807 527 497 837 129 280 866 543 383\n",
      " 586 284 391 380 329 174 781 249 632 634 530 844 663 895 952 686 167 861\n",
      " 817  77 126 403 245 259 198 432 354 606 205 599 134 881  91 889 357 762\n",
      " 415 748 457 669 876 185 136 525 392 318 533 376 539 679 401 246 882 850\n",
      " 319 393 191 159 885 142 915 883 572 898 760 255 515 425 331   1 301 257\n",
      " 513 540 105 944 683 887  42 229 194 146 653 155 411 746 406 751 305 117\n",
      " 538 894 149 920 773 907 737 150 689 519 517 371 228 754 930 486 544 753\n",
      " 215 767 552 103 360 467 373 842 580 496 902  46 102 590 491 163 698  32\n",
      " 237 697 315 499 922 855 272 196 537 688 803 796  15 494 741 768 274 520\n",
      " 498 398 456 731 250 691]\n",
      "Train: [ 78  73  63 811 804 277 412 666  47 421 168 633 857 661 120 648 664  41\n",
      " 184 827 350 132 133 442 268 631 143 397 874  95 283 822 338 506 100 605\n",
      " 207 545 282 575 600  62  16 721 482  22 367 409 914 932 829 139  19 216\n",
      " 651 919 186   3 115 413 921 495 735 179 394 355 893 677   9 657 225  27\n",
      " 337 913 290 322 602 177 581 137 772 223 763 463 728 334 453   8 195 313\n",
      "  54 700 286 189 172 254 202 178 232 738 872 573 901 571  71 110 936 165\n",
      " 911 157 426  35 710 531 906 342  67 450 650 859 122 464 744 437 281 247\n",
      " 714 493   7  25 507 335 757 416 443 381 838 187  44 912 646 808 587 298\n",
      "  45 253 715 778 293 769 199 711 815 742 299 478 190 419 288 297 368 712\n",
      " 484 441 749 609 546 863  53 271 239 471 547 899 470 828 719 937 158 948\n",
      " 209 856 366 296 870 424 583 349 265 217 627 144 510 626   0 707 629  37\n",
      " 535 104 951 695   5  18  66  75 503 121  14 213  48 541 116 720 230 577\n",
      " 713 111 569 387 459 192 821 438 565 640 238  36 676  10 348 938 220 208\n",
      " 851 542 694 549 668 147 420 361  20 702 365 317 716 708 339 662 652 942\n",
      " 766 635 256 231  86  98 279 200 532  61 703 812 505 433 722 745 736  33\n",
      " 833 404 508 730 554 206 670 620 771 801 135 252 639 410 794 910 709 164\n",
      " 260 957 358 440 447 673 892 553 764 407 792 431 487 518 645  50 896 449\n",
      " 869 108 262 924 825  97 788 588 287 306 818 340 370 750 807 527 497 837\n",
      " 129 280 866 543 383 586 284 391 380 329 174 781 249 632 634 530 844 663\n",
      " 895 952 686 167 861 817  77 126 403 245 259 198 432 354 606 205 599 134\n",
      " 881  91 889 357 762 415 748 457 669 876 185 136 525 392 318 533 376 539\n",
      " 679 401 246 882 850 319 393 191 159 885 142 915 883 572 898 760 255 515\n",
      " 425 331   1 301 257 513 540 105 944 683 887  42 229 194 146 653 155 411\n",
      " 746 406 751 305 117 538 894 149 920 773 907 737 150 689 519 517 371 228\n",
      " 754 930 486 544 753 215 767 552 103 360 467 373 842 580 496 902  46 102\n",
      " 590 491 163 698  32 237 697 315 499 922 855 272 196 537 688 803 796  15\n",
      " 494 741 768 274 520 498 398 456 731 250 687 153 789 384 947 656 481 903\n",
      " 273   4 684 649 654  70 435 477 756 275 761 611  55 557 843 800 795  90\n",
      " 747 148 878  96 372 269 864 875  92  84 939 140 374 294 347  89 706 130\n",
      " 151 607 458 291 224 665 434 267 832 940 860 451 385 555 509 270 935 414\n",
      " 551 439 671  39 430 333 124 324 594 321 579  72 528 693 183 423 681 377\n",
      " 584 162 805 107 109 326 950 138 585 476 593 465 636 823 696 806 783 777\n",
      " 427 308  99  30 234 460 734 888 690 145 353 608 755 718 248 351 733  26\n",
      " 723 428 852 379 618 241 613 466 925 251 776 797 362 576 171 485 816 595\n",
      " 884 612  76 114 219 521 446 799 472 131 204  38 933 504 667 561 188 127\n",
      " 858 101 292 886 628 740 303 601 841 873 106 327 182 359 848 399 483 386\n",
      " 244  31 908 325 784 836 474  69 931 617 758 418 564 726 363 436 314 820\n",
      " 905 405 644 568 536 616  85 699 175 826 390 455 891 264 402  74 161 448\n",
      " 934  93 566 862 945 236  24 311 176 809 488 154 685 285 243 388 328 310\n",
      " 675 787 802 928 621 166 263 574  57 725  11 214 660 770 181 691 638]\n",
      "Test:  [871  87 643 523 918 562 641  80 880 798 927 900 582 678 210 955 375 345\n",
      " 211 123 890 658 227 454  94  81 278 408  49 320 548 835 492 785 529 672\n",
      "  60 659 647 462 221 558 330 516 765 926 704  58 867 603 309 170 422 469\n",
      " 522 682 839 865 180 830  65 592 916 637 619  68 389 729 946 680 112 514\n",
      "  29 727 169 831 655 868 854 489 203 846 444  17 197 941 879  64 502 261\n",
      " 923  40 630 834 266 316 235 739 589 341 445  34 524 624  12  52  88 302\n",
      " 343 849 724 156 222 743 336 119 591 369 954 559 929 312 173 468 717 560\n",
      " 674 845 625 276 705 550  79 909 853 218 780 304  28 819 692 289 113 332\n",
      "  51 417  56 193 904 382 128 201 840 847 793 604 615 490 242 953 813 356\n",
      " 295 479 534  23 452 125 300 346 897 501 212 790 810 396 400 642  43 395\n",
      " 352 307 949 814 226 323 500 570 475 791 614 917 526 610 774 473 429 956\n",
      " 556 258 364 160 775  21 701 597 759 511 378   2 943 480 622 786 240 152\n",
      " 344 623 233 118 567  82 732 512   6  59 877 596 141 824 598 782  83 752\n",
      " 779 563 461 578  13]\n",
      "Train: [ 78  73  63 811 804 277 412 666  47 421 168 633 857 661 120 648 664  41\n",
      " 184 827 350 132 133 442 268 631 143 397 874  95 283 822 338 506 100 605\n",
      " 207 545 282 575 600  62  16 721 482  22 367 409 914 932 829 139  19 216\n",
      " 651 919 186   3 115 413 921 495 735 179 394 355 893 677   9 657 225  27\n",
      " 337 913 290 322 602 177 581 137 772 223 763 463 728 334 453   8 195 313\n",
      "  54 700 286 189 172 254 202 178 232 738 872 573 901 571  71 110 936 165\n",
      " 911 157 426  35 710 531 906 342  67 450 650 859 122 464 744 437 281 247\n",
      " 714 493   7  25 507 335 757 416 443 381 838 187  44 912 646 808 587 298\n",
      "  45 253 715 778 293 769 199 711 815 742 299 478 190 419 288 297 368 712\n",
      " 484 441 749 609 546 863  53 271 239 471 547 899 470 828 719 937 158 948\n",
      " 209 856 366 296 870 424 583 349 265 217 627 144 510 626   0 707 629  37\n",
      " 535 104 951 695   5  18  66  75 503 121  14 213  48 541 116 720 230 577\n",
      " 713 111 569 387 459 192 821 438 565 640 238  36 676  10 348 938 220 208\n",
      " 851 542 694 549 668 147 420 361  20 702 365 317 716 708 339 662 652 942\n",
      " 766 635 256 231  86  98 279 200 532  61 703 812 505 433 722 745 736  33\n",
      " 833 404 508 730 554 206 670 620 771 801 135 252 639 410 794 910 709 164\n",
      " 260 957 358 440 447 673 892 553 764 407 792 431 487 518 645  50 896 449\n",
      " 869 108 262 924 825  97 788 588 287 306 818 340 370 750 807 527 497 837\n",
      " 129 280 866 543 383 586 284 391 380 329 174 781 249 632 634 530 844 663\n",
      " 895 952 686 167 861 817  77 126 403 245 259 198 432 354 606 205 599 134\n",
      " 881  91 889 357 762 415 748 457 669 876 185 136 525 392 318 533 376 539\n",
      " 679 401 246 882 850 319 393 191 159 885 142 915 883 572 898 760 255 515\n",
      " 425 331   1 301 257 513 540 105 944 683 887  42 229 194 146 653 155 411\n",
      " 746 406 751 305 117 538 894 149 920 773 907 737 150 689 519 517 371 228\n",
      " 754 930 486 544 753 215 767 552 103 360 467 373 842 580 496 902  46 102\n",
      " 590 491 163 698  32 237 697 315 499 922 855 272 196 537 688 803 796  15\n",
      " 494 741 768 274 520 498 398 456 731 250 871  87 643 523 918 562 641  80\n",
      " 880 798 927 900 582 678 210 955 375 345 211 123 890 658 227 454  94  81\n",
      " 278 408  49 320 548 835 492 785 529 672  60 659 647 462 221 558 330 516\n",
      " 765 926 704  58 867 603 309 170 422 469 522 682 839 865 180 830  65 592\n",
      " 916 637 619  68 389 729 946 680 112 514  29 727 169 831 655 868 854 489\n",
      " 203 846 444  17 197 941 879  64 502 261 923  40 630 834 266 316 235 739\n",
      " 589 341 445  34 524 624  12  52  88 302 343 849 724 156 222 743 336 119\n",
      " 591 369 954 559 929 312 173 468 717 560 674 845 625 276 705 550  79 909\n",
      " 853 218 780 304  28 819 692 289 113 332  51 417  56 193 904 382 128 201\n",
      " 840 847 793 604 615 490 242 953 813 356 295 479 534  23 452 125 300 346\n",
      " 897 501 212 790 810 396 400 642  43 395 352 307 949 814 226 323 500 570\n",
      " 475 791 614 917 526 610 774 473 429 956 556 258 364 160 775  21 701 597\n",
      " 759 511 378   2 943 480 622 786 240 152 344 623 233 118 567  82 732 512\n",
      "   6  59 877 596 141 824 598 782  83 752 779 563 461 578  13 691 638]\n",
      "Test:  [687 153 789 384 947 656 481 903 273   4 684 649 654  70 435 477 756 275\n",
      " 761 611  55 557 843 800 795  90 747 148 878  96 372 269 864 875  92  84\n",
      " 939 140 374 294 347  89 706 130 151 607 458 291 224 665 434 267 832 940\n",
      " 860 451 385 555 509 270 935 414 551 439 671  39 430 333 124 324 594 321\n",
      " 579  72 528 693 183 423 681 377 584 162 805 107 109 326 950 138 585 476\n",
      " 593 465 636 823 696 806 783 777 427 308  99  30 234 460 734 888 690 145\n",
      " 353 608 755 718 248 351 733  26 723 428 852 379 618 241 613 466 925 251\n",
      " 776 797 362 576 171 485 816 595 884 612  76 114 219 521 446 799 472 131\n",
      " 204  38 933 504 667 561 188 127 858 101 292 886 628 740 303 601 841 873\n",
      " 106 327 182 359 848 399 483 386 244  31 908 325 784 836 474  69 931 617\n",
      " 758 418 564 726 363 436 314 820 905 405 644 568 536 616  85 699 175 826\n",
      " 390 455 891 264 402  74 161 448 934  93 566 862 945 236  24 311 176 809\n",
      " 488 154 685 285 243 388 328 310 675 787 802 928 621 166 263 574  57 725\n",
      "  11 214 660 770 181]\n"
     ]
    }
   ],
   "source": [
    "validacion_simple = ValidacionSimple(0.75)\n",
    "validacion_simple.creaParticiones(dataset)\n",
    "v_cruzada = ValidacionCruzada(4)\n",
    "v_cruzada.creaParticiones(dataset)\n",
    "\n",
    "print(\"VALIDACION SIMPLE\",validacion_simple.particiones[0])\n",
    "\n",
    "print(\"VALIDACION CRUZADA\")\n",
    "for particion in v_cruzada.particiones:\n",
    "    print(particion)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[0.18677043, 0.22912206],\n",
      "       [0.45136187, 0.29336188],\n",
      "       [0.3618677 , 0.47751606]])\n",
      " array([[0.21400778, 0.26766595],\n",
      "       [0.31128405, 0.38543897],\n",
      "       [0.47470817, 0.34689507]])\n",
      " array([[0.19066148, 0.25695931],\n",
      "       [0.45136187, 0.29122056],\n",
      "       [0.35797665, 0.45182013]])\n",
      " array([[0.24124514, 0.25481799],\n",
      "       [0.30350195, 0.35974304],\n",
      "       [0.45525292, 0.38543897]])\n",
      " array([[0.13229572, 0.17344754],\n",
      "       [0.57198444, 0.21627409],\n",
      "       [0.29571984, 0.61027837]])\n",
      " array([[0.24124514, 0.26124197],\n",
      "       [0.30350195, 0.37901499],\n",
      "       [0.45525292, 0.35974304]])\n",
      " array([[0.19455253, 0.21627409],\n",
      "       [0.41245136, 0.32334047],\n",
      "       [0.39299611, 0.46038544]])\n",
      " array([[0.23346304, 0.27837259],\n",
      "       [0.30350195, 0.36616702],\n",
      "       [0.46303502, 0.35546039]])\n",
      " array([[0.19844358, 0.23126338],\n",
      "       [0.44747082, 0.30406852],\n",
      "       [0.3540856 , 0.46466809]])]\n",
      "[array([[0.19305019, 0.24731183],\n",
      "       [0.46332046, 0.31397849],\n",
      "       [0.34362934, 0.43870968]])\n",
      " array([[0.20849421, 0.28602151],\n",
      "       [0.32046332, 0.35483871],\n",
      "       [0.47104247, 0.35913978]])\n",
      " array([[0.2007722 , 0.21935484],\n",
      "       [0.42857143, 0.31827957],\n",
      "       [0.37065637, 0.46236559]])\n",
      " array([[0.25482625, 0.27311828],\n",
      "       [0.2972973 , 0.36774194],\n",
      "       [0.44787645, 0.35913978]])\n",
      " array([[0.15444015, 0.16774194],\n",
      "       [0.57142857, 0.23655914],\n",
      "       [0.27413127, 0.59569892]])\n",
      " array([[0.24324324, 0.25591398],\n",
      "       [0.28957529, 0.38709677],\n",
      "       [0.46718147, 0.35698925]])\n",
      " array([[0.17374517, 0.22795699],\n",
      "       [0.41698842, 0.29032258],\n",
      "       [0.40926641, 0.48172043]])\n",
      " array([[0.23552124, 0.28172043],\n",
      "       [0.32046332, 0.34193548],\n",
      "       [0.44401544, 0.37634409]])\n",
      " array([[0.16988417, 0.22365591],\n",
      "       [0.44787645, 0.30107527],\n",
      "       [0.38223938, 0.47526882]])]\n",
      "[array([[0.18442623, 0.21875   ],\n",
      "       [0.43852459, 0.3       ],\n",
      "       [0.37704918, 0.48125   ]])\n",
      " array([[0.26229508, 0.28541667],\n",
      "       [0.31147541, 0.35      ],\n",
      "       [0.42622951, 0.36458333]])\n",
      " array([[0.19262295, 0.22916667],\n",
      "       [0.47131148, 0.30416667],\n",
      "       [0.33606557, 0.46666667]])\n",
      " array([[0.22540984, 0.27708333],\n",
      "       [0.28688525, 0.36458333],\n",
      "       [0.48770492, 0.35833333]])\n",
      " array([[0.15163934, 0.18125   ],\n",
      "       [0.55327869, 0.23541667],\n",
      "       [0.29508197, 0.58333333]])\n",
      " array([[0.22131148, 0.28541667],\n",
      "       [0.32377049, 0.36041667],\n",
      "       [0.45491803, 0.35416667]])\n",
      " array([[0.20491803, 0.23333333],\n",
      "       [0.43442623, 0.31458333],\n",
      "       [0.36065574, 0.45208333]])\n",
      " array([[0.23360656, 0.26041667],\n",
      "       [0.27459016, 0.37708333],\n",
      "       [0.49180328, 0.3625    ]])\n",
      " array([[0.16803279, 0.23125   ],\n",
      "       [0.45901639, 0.29583333],\n",
      "       [0.37295082, 0.47291667]])]\n",
      "[array([[0.19444444, 0.20930233],\n",
      "       [0.42063492, 0.30655391],\n",
      "       [0.38492063, 0.48414376]])\n",
      " array([[0.25      , 0.2769556 ],\n",
      "       [0.28174603, 0.38266385],\n",
      "       [0.46825397, 0.34038055]])\n",
      " array([[0.19444444, 0.22832981],\n",
      "       [0.40873016, 0.30443975],\n",
      "       [0.3968254 , 0.46723044]])\n",
      " array([[0.23809524, 0.27272727],\n",
      "       [0.32539683, 0.3615222 ],\n",
      "       [0.43650794, 0.36575053]])\n",
      " array([[0.12698413, 0.19238901],\n",
      "       [0.58730159, 0.22198732],\n",
      "       [0.28571429, 0.58562368]])\n",
      " array([[0.24206349, 0.29175476],\n",
      "       [0.31746032, 0.34460888],\n",
      "       [0.44047619, 0.36363636]])\n",
      " array([[0.19444444, 0.23678647],\n",
      "       [0.4484127 , 0.30232558],\n",
      "       [0.35714286, 0.46088795]])\n",
      " array([[0.23809524, 0.28541226],\n",
      "       [0.30952381, 0.36363636],\n",
      "       [0.45238095, 0.35095137]])\n",
      " array([[0.22222222, 0.22832981],\n",
      "       [0.43253968, 0.30443975],\n",
      "       [0.3452381 , 0.46723044]])]\n",
      "[array([[0.19367589, 0.23516949],\n",
      "       [0.43083004, 0.28813559],\n",
      "       [0.37549407, 0.47669492]])\n",
      " array([[0.22529644, 0.25211864],\n",
      "       [0.30434783, 0.375     ],\n",
      "       [0.47035573, 0.37288136]])\n",
      " array([[0.17786561, 0.23305085],\n",
      "       [0.44664032, 0.28177966],\n",
      "       [0.37549407, 0.48516949]])\n",
      " array([[0.22529644, 0.27754237],\n",
      "       [0.3083004 , 0.36864407],\n",
      "       [0.46640316, 0.35381356]])\n",
      " array([[0.1541502 , 0.1779661 ],\n",
      "       [0.58893281, 0.25423729],\n",
      "       [0.256917  , 0.56779661]])\n",
      " array([[0.23715415, 0.26694915],\n",
      "       [0.28853755, 0.37076271],\n",
      "       [0.4743083 , 0.36228814]])\n",
      " array([[0.19367589, 0.21186441],\n",
      "       [0.45454545, 0.30084746],\n",
      "       [0.35177866, 0.48728814]])\n",
      " array([[0.23715415, 0.27330508],\n",
      "       [0.31225296, 0.37923729],\n",
      "       [0.45059289, 0.34745763]])\n",
      " array([[0.2055336 , 0.22669492],\n",
      "       [0.41501976, 0.30720339],\n",
      "       [0.37944664, 0.46610169]])]\n",
      "El error del Clasificador NaiveBayes  para Validacion Simple es: 0.675\n",
      "El error del Clasificador NaiveBayes para Validacion Cruzada es: 0.6534475244072525\n"
     ]
    }
   ],
   "source": [
    "clasificador = ClasificadorNaiveBayes(True)\n",
    "#clasificador.entrenamiento(dataset, validacion_simple.particiones[0].indicesTrain)\n",
    "#clasificador.clasifica(dataset, validacion_simple.particiones[0].indicesTest)\n",
    "error = clasificador.validacion(validacion_simple,dataset,clasificador)\n",
    "error1 = clasificador.validacion(v_cruzada,dataset,clasificador)\n",
    "print(\"El error del Clasificador NaiveBayes  para Validacion Simple es: \"+str(error))\n",
    "print(\"El error del Clasificador NaiveBayes para Validacion Cruzada es: \"+str(error1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A1', 'A2', 'A3', 'A4', 'A5', 'A6', 'A7', 'A8', 'A9', 'A10', 'A11', 'A12', 'A13', 'A14', 'A15', 'A16', 'A17', 'A18', 'A19', 'A20', 'Class']\n",
      "[{'A11': 0, 'A12': 1, 'A13': 2, 'A14': 3}, {}, {'A30': 0, 'A31': 1, 'A32': 2, 'A33': 3, 'A34': 4}, {'A40': 0, 'A41': 1, 'A410': 2, 'A42': 3, 'A43': 4, 'A44': 5, 'A45': 6, 'A46': 7, 'A48': 8, 'A49': 9}, {}, {'A61': 0, 'A62': 1, 'A63': 2, 'A64': 3, 'A65': 4}, {'A71': 0, 'A72': 1, 'A73': 2, 'A74': 3, 'A75': 4}, {}, {'A91': 0, 'A92': 1, 'A93': 2, 'A94': 3}, {'A101': 0, 'A102': 1, 'A103': 2}, {}, {'A121': 0, 'A122': 1, 'A123': 2, 'A124': 3}, {}, {'A141': 0, 'A142': 1, 'A143': 2}, {'A151': 0, 'A152': 1, 'A153': 2}, {}, {'A171': 0, 'A172': 1, 'A173': 2, 'A174': 3}, {}, {'A191': 0, 'A192': 1}, {'A201': 0, 'A202': 1}, {'1': 0, '2': 1}]\n",
      "[[ 0.  6.  4. ...  1.  0.  0.]\n",
      " [ 1. 48.  2. ...  0.  0.  1.]\n",
      " [ 3. 12.  4. ...  0.  0.  0.]\n",
      " ...\n",
      " [ 3. 12.  2. ...  0.  0.  0.]\n",
      " [ 0. 45.  2. ...  1.  0.  1.]\n",
      " [ 1. 45.  4. ...  0.  0.  0.]]\n",
      "[True, False, True, True, False, True, True, False, True, True, False, True, False, True, True, False, True, False, True, True, True]\n"
     ]
    }
   ],
   "source": [
    "dataset = Datos('german.data')\n",
    "print(dataset.datos)\n",
    "print(dataset.nominalAtributos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VALIDACION SIMPLE Train: [ 10 658 191 526 580 275 115 883 313 917 657 901 956 147 167  94  59 927\n",
      " 626 994 300 183 716  69   6 798  80 815 918   7 232 975 863 397 969 527\n",
      "  27 260 203 700 490 573 635 385 367 129 229 306  16 641 957 494 495 981\n",
      " 244 923  82 955 190 760 805 324 225 731 705 179 396 356 139 492 180 600\n",
      " 800 920 717 898 320 733 607 553 256 997 157 382 652 277 144 655 727 565\n",
      " 473 257 908 867 561  65 888 948 255 807 538 638 295 298 154 519 146 413\n",
      " 905 667 811 511 673 804 741 695 732 629 419 281 343 149 390 634 389  25\n",
      " 756 265 884 982   8 259  84 323 207 424 719 162 690  53 909 269 465 577\n",
      " 590 108  73 394 215 168 656 886 639 833 770 400 684   9 570 120 329 744\n",
      " 556 808 375   4 430 458 572  97 462 788 118 537 161 876 686 192  88 661\n",
      " 204 819 582 848 604 765 464 959 976 757  40 483 498 562 979 958 568 407\n",
      " 502 858 640 581 794 476 872 383 718  95 401 294 485 373 970 500 859 615\n",
      " 405 467 775 282  51 211 826 218 348 967 188 643  15 431 738 292 377 993\n",
      " 450 688 772 230 664 482 242 784 423 334 880  20 606 707 541 809 868 342\n",
      " 985 933 799 150 322 319 189 977 617 429 318 694 743 488 446 729 223 648\n",
      " 802 328  47 931 289 153 107 603  45 650 844 428 437 480 152 311  70 197\n",
      " 764 720 928 231 889 646 602 368 706 391 327 709  18  32 266 477 703 274\n",
      " 663 882 395 142 877  52 755 330  72 585 737 767 402 166 972 219 187 687\n",
      " 398 247 551 685 376 250 530 850 759 185 620 370 484 236 127 762 340 110\n",
      " 790  28 113 134  81 691 133 186 116 138 486 228 845  67 974 723 333 601\n",
      " 598  24 543 360 563  91 283 224  39 308 284 771 253  49 831 849  98 444\n",
      " 332 248 801 387 288 415 557 533 184 549 575 474 528 951 677 515 427 453\n",
      " 879 676 177 178 726 278 779 357 457 929   5 103  13 589 991 531 781 668\n",
      " 813 964 124 354 234 814 961 926 754  77 978 126 701 692 258 145 121  83\n",
      " 361 148 674 359  86 936 459  11 735 410 834  42 670 900 268 346 221 912\n",
      " 251  41 721 214 174 965 558 392 613  43 151 426 156 347 906 734 904 628\n",
      " 698 386 915 653 366 954 312  76 353 566 111  14 750 992 682 175 576 297\n",
      " 636  92 708  60 384 990 362 106  37 469 254 778 380 816 194 689 647 499\n",
      "  89 119 296 125 820 587 249 273 505   0 715 267 669 364 237 517 947 940\n",
      " 418 331 728 205 449 696 579 105 749 852 285 350 752 506 950 608  30 583\n",
      " 164 420 463 942 456 644 509 873 622 637 132 571 594 378 512 159 793  90\n",
      " 210 973 422 627 140  22 907 623 379 170 596 591 851 554 433 953 835 730\n",
      " 448 946 916 466 899 949 503 335 823 797 304 803 302 665 412 393 337 891\n",
      " 349 983 270 287 610 999 461 789 599 445 316 586 481 659 532 216 827 968\n",
      " 271 233 291 564 548 321 345 240 276 666 998 678 199 326 725 193 552 372\n",
      "  57 455 109  50 874 680 675 632 521 614 787 169 746 662 892 766 699 712\n",
      "  31 497 421 539  46 198 944 155 782 534 471  79 416 470 173  17 597 510\n",
      " 786 631 280 795  54 621 452 478 352 618 871 213 567 740 310 887 325 546\n",
      "  96 697 130 220 792 963 710 261 114 711 922 869 785 239 235 847 520  26\n",
      " 745 544 309  12 817 838 317 937 441 611 856 135 438 860 238 409 864 654\n",
      " 435  75 748 895 136 425 704 960  63 897 496 902 523 447 921 493 217 545\n",
      " 569   3 507 758 825 896 518 516 683 171 540 338]\n",
      "Test:  [862 264 112 101 609 104  55 165 989 945 832 774   2 305 355 938 952 504\n",
      " 371 336  48 913 514 914 671  62 651 358 535 227 137 439 841 971 314 924\n",
      "  23 810 842 854 724 172 751 625 890 307 406  35 672 742  33 363  71 843\n",
      " 369 763 853 846 624 202 605 508 245 374 822 301 131 919 592 910 206 996\n",
      " 578 679 181 513 828 252 128 984  61 870 736 241 866 451 550 616 776 881\n",
      " 939 158  34 542 122 885 290 442 209 861 824 839 339 612 930 830 200 649\n",
      " 196 100 574 303 262 226 460 783 123 893 925 935 777 806 630  29 176 878\n",
      " 584 768 522 286 747  38 837 208 315 432 660 753 102 440 865 417 160 525\n",
      " 903 487 995 547 408 934 855 555 491 468 222  44 693 195 472  68 454 479\n",
      "  74 987 475 536 911 812 381 279 773 341 943 414 714  64 293 857  78 595\n",
      " 619 713 791  85 524 263 143 501  66 365 141  19 988 739 962 351 399 489\n",
      " 182 821 941 840 201 436 894  99 645 246  36 836 411 299  58 642 769 722\n",
      " 344 560 403 529 986  87 117 681  93 875 559   1 702 780 818 593 272 932\n",
      " 443 796  56 761 163 388 243 404 434 588  21 966 829 980 212 633]\n",
      "VALIDACION CRUZADA\n",
      "Train: [292 141  68 405 545 428 738 208 317 820 859 867 911 521 426 848 403 220\n",
      " 653 102 116 510 239 652 410 166 828 390 786 205 589 268 483  35 964 304\n",
      " 242 400  71  16 753 114   9 632 214 749  42 856 504 120  46 981 862 573\n",
      " 841 834  18 350 769 349 689 210 824 594 651 391 947 813 735 270 221 795\n",
      " 784 630 245   6 771 121 283 305 119 778 682 628 799 639 212 679 191 966\n",
      "  73 626 960 549 537  98  64 987 603 235 291 797 696 909 138 943  38 192\n",
      " 486 306 831 567 946 857 663 162 971 590 115 937 480 506 513 729 871  39\n",
      " 699 110 523 104  89 892 439  26 149 294 722 175 818 806 673 700 451 285\n",
      "  31 262 181 701 868 686 176 708 962 219 455  15 171 992 343 636 419 623\n",
      " 133 282 444 401 890 247 541 514 490 768 718 216 492 601 465 789 618 424\n",
      " 599 412 482 259 604 534 472 177 163 197 850 386 904 629 280 344 355 945\n",
      " 423 516  11 578 973  49 796 713 399 887 193 759 592 106 385 112 126 929\n",
      " 308 602  57 173 411  53 548 906 776  44 182 903 897  36 676 108 518 371\n",
      " 942 238 345 717  32 323 556 767 588 570 849 129 243 194 999 454 190 378\n",
      " 460 928 500 198 580 785 443   3 275 103 996 348 384 746 146 710 620 478\n",
      " 584 659 902  51 666 336 711 288 298 819   4 775 281 113 898 938 347 681\n",
      " 555 407 866 986 327 246 381 137 770 825 312 870 737 707 152 912 227  59\n",
      " 823 479 543 209 416   8 464 254 408 325 544 572 241 930 995  72 124 153\n",
      " 154  77 914 297 984 222 854 730 379 449 827 655 891 201 634 882   1 642\n",
      " 250  61 311 814 855  45 456 800 356 683 574  79 319 901 815 657 840 923\n",
      " 376 951 431 316 754 351 743 691 880 261 130 100 394 136 613 723 469 597\n",
      " 647 525 843 358 331  28 920 750 476 641 200 279 307 919 493 979 961 690\n",
      " 650  96 453 109 519 586 535 505 131 529 900  41  30 320 105 547 842 877\n",
      " 257 265 734 889 392 354 926 803 791 369 286 779 688  91 787  70 165 315\n",
      " 670 927 654 225 924 757 199 917 213 517 615 720 501 622 458 377 677  66\n",
      " 624 448 450 607 564 274 372 393 457 975 157 762 520 289 907 893 353 980\n",
      " 585 332 233 277 328 921 715 557 726 643 692 617 822 264 605 853 118 649\n",
      " 258 318 562  63 591 540 640 610 741 763 817 527 801 186 470 669 435 446\n",
      " 716 844  94 625 172 985 542 263  37 865 955 434 484 495 406 346 733 989\n",
      " 884 760 807 963 581 983 207 780 954 206 974 340 539 301 366 883 359 507\n",
      " 437 879 273 244 860  29 998 596 864 861 179 826 745 821 949  69 809 481\n",
      " 685 299 565 671   0 524 251 362 680 375 296  20 342 648 702 934 127  50\n",
      " 851 533 240 329 832 633 462 140 452 684 267 714 582 931  47 766 941 896\n",
      "  86 107  62 727 425 414  12 169 550  82 272 918 433 756 337 530 271 494\n",
      " 925 635 310 330  52 758 374 751 672 313 773 725 338 528 739 869 957 619\n",
      "  87 365 706 546 474 551 473 595 664 158 627 660 538  93 461 326 781 736\n",
      " 783 284 174 396 728 724 940 167   5 740 976  80 417  83 196 644 128 155\n",
      " 563 950 852 142 978 255 847 352 499  74 134 774  65  81 988 442 170 463\n",
      " 712 429 511 145 471 339  40   7 913 302 837 798 309 802  75 662 873 230\n",
      " 491 383 278 915 616  55 380 593 752 164 836 203 872 755 916 579 792 357\n",
      " 638 509 693  99 360 958 135 894 397 881 224 156 421 195 475 577  23  95\n",
      " 833 402 953 933 878 150 694 948  92  25 382  17]\n",
      "Test:  [143 477 965 395 515 440 764 829 611 554 303 645 695 790 777 811 571 314\n",
      " 231 808 333  90 932 560 765 502 180 788 334 503 885 772  19 409 705 721\n",
      " 888 838  21 656 944 608 389 719 569 552 413 982 159 178  24 993 321 447\n",
      " 497 260 253 531 252 876 997  54 899  78 249 373 228 910 830 485   2 977\n",
      "  34 697  67 388 204 994 188 703 522 536  43 812  10 598 468 874 441 665\n",
      "  76 970 968 959 132 631 147 324 558 269 122 600 404 793  13 747 256 398\n",
      " 744  84 936 575 566 496 559 187 123 661 875 782 223 674 361 422 368 276\n",
      "  22 237 731 794 863 488 436 445 886 226 202 646 609 459 335  33 467 438\n",
      " 300 761  97 498 935 420 576 427 553 218 217 184  48 234 668  88 185 805\n",
      " 583 637 614 139 561 621 704 732 606 952 922 835 151 804 290 160 709 972\n",
      " 969 101 148  58 512 367 846 658 295 858 839 895 489 287 341 387 742 161\n",
      " 168 487 183 236 229  85 845 418 215 675 363 532 939 816 678 526 211 956\n",
      " 568 415 667 364 908  56 125 248 266 967 293 905 508  14 748 232 687 432\n",
      " 810 144 117 466 322 991  27  60 612 587 111 990 698 189 430 370]\n",
      "Train: [143 477 965 395 515 440 764 829 611 554 303 645 695 790 777 811 571 314\n",
      " 231 808 333  90 932 560 765 502 180 788 334 503 885 772  19 409 705 721\n",
      " 888 838  21 656 944 608 389 719 569 552 413 982 159 178  24 993 321 447\n",
      " 497 260 253 531 252 876 997  54 899  78 249 373 228 910 830 485   2 977\n",
      "  34 697  67 388 204 994 188 703 522 536  43 812  10 598 468 874 441 665\n",
      "  76 970 968 959 132 631 147 324 558 269 122 600 404 793  13 747 256 398\n",
      " 744  84 936 575 566 496 559 187 123 661 875 782 223 674 361 422 368 276\n",
      "  22 237 731 794 863 488 436 445 886 226 202 646 609 459 335  33 467 438\n",
      " 300 761  97 498 935 420 576 427 553 218 217 184  48 234 668  88 185 805\n",
      " 583 637 614 139 561 621 704 732 606 952 922 835 151 804 290 160 709 972\n",
      " 969 101 148  58 512 367 846 658 295 858 839 895 489 287 341 387 742 161\n",
      " 168 487 183 236 229  85 845 418 215 675 363 532 939 816 678 526 211 956\n",
      " 568 415 667 364 908  56 125 248 266 967 293 905 508  14 748 232 687 432\n",
      " 810 144 117 466 322 991  27  60 612 587 111 990 698 189 430 370 190 378\n",
      " 460 928 500 198 580 785 443   3 275 103 996 348 384 746 146 710 620 478\n",
      " 584 659 902  51 666 336 711 288 298 819   4 775 281 113 898 938 347 681\n",
      " 555 407 866 986 327 246 381 137 770 825 312 870 737 707 152 912 227  59\n",
      " 823 479 543 209 416   8 464 254 408 325 544 572 241 930 995  72 124 153\n",
      " 154  77 914 297 984 222 854 730 379 449 827 655 891 201 634 882   1 642\n",
      " 250  61 311 814 855  45 456 800 356 683 574  79 319 901 815 657 840 923\n",
      " 376 951 431 316 754 351 743 691 880 261 130 100 394 136 613 723 469 597\n",
      " 647 525 843 358 331  28 920 750 476 641 200 279 307 919 493 979 961 690\n",
      " 650  96 453 109 519 586 535 505 131 529 900  41  30 320 105 547 842 877\n",
      " 257 265 734 889 392 354 926 803 791 369 286 779 688  91 787  70 165 315\n",
      " 670 927 654 225 924 757 199 917 213 517 615 720 501 622 458 377 677  66\n",
      " 624 448 450 607 564 274 372 393 457 975 157 762 520 289 907 893 353 980\n",
      " 585 332 233 277 328 921 715 557 726 643 692 617 822 264 605 853 118 649\n",
      " 258 318 562  63 591 540 640 610 741 763 817 527 801 186 470 669 435 446\n",
      " 716 844  94 625 172 985 542 263  37 865 955 434 484 495 406 346 733 989\n",
      " 884 760 807 963 581 983 207 780 954 206 974 340 539 301 366 883 359 507\n",
      " 437 879 273 244 860  29 998 596 864 861 179 826 745 821 949  69 809 481\n",
      " 685 299 565 671   0 524 251 362 680 375 296  20 342 648 702 934 127  50\n",
      " 851 533 240 329 832 633 462 140 452 684 267 714 582 931  47 766 941 896\n",
      "  86 107  62 727 425 414  12 169 550  82 272 918 433 756 337 530 271 494\n",
      " 925 635 310 330  52 758 374 751 672 313 773 725 338 528 739 869 957 619\n",
      "  87 365 706 546 474 551 473 595 664 158 627 660 538  93 461 326 781 736\n",
      " 783 284 174 396 728 724 940 167   5 740 976  80 417  83 196 644 128 155\n",
      " 563 950 852 142 978 255 847 352 499  74 134 774  65  81 988 442 170 463\n",
      " 712 429 511 145 471 339  40   7 913 302 837 798 309 802  75 662 873 230\n",
      " 491 383 278 915 616  55 380 593 752 164 836 203 872 755 916 579 792 357\n",
      " 638 509 693  99 360 958 135 894 397 881 224 156 421 195 475 577  23  95\n",
      " 833 402 953 933 878 150 694 948  92  25 382  17]\n",
      "Test:  [292 141  68 405 545 428 738 208 317 820 859 867 911 521 426 848 403 220\n",
      " 653 102 116 510 239 652 410 166 828 390 786 205 589 268 483  35 964 304\n",
      " 242 400  71  16 753 114   9 632 214 749  42 856 504 120  46 981 862 573\n",
      " 841 834  18 350 769 349 689 210 824 594 651 391 947 813 735 270 221 795\n",
      " 784 630 245   6 771 121 283 305 119 778 682 628 799 639 212 679 191 966\n",
      "  73 626 960 549 537  98  64 987 603 235 291 797 696 909 138 943  38 192\n",
      " 486 306 831 567 946 857 663 162 971 590 115 937 480 506 513 729 871  39\n",
      " 699 110 523 104  89 892 439  26 149 294 722 175 818 806 673 700 451 285\n",
      "  31 262 181 701 868 686 176 708 962 219 455  15 171 992 343 636 419 623\n",
      " 133 282 444 401 890 247 541 514 490 768 718 216 492 601 465 789 618 424\n",
      " 599 412 482 259 604 534 472 177 163 197 850 386 904 629 280 344 355 945\n",
      " 423 516  11 578 973  49 796 713 399 887 193 759 592 106 385 112 126 929\n",
      " 308 602  57 173 411  53 548 906 776  44 182 903 897  36 676 108 518 371\n",
      " 942 238 345 717  32 323 556 767 588 570 849 129 243 194 999 454]\n",
      "Train: [143 477 965 395 515 440 764 829 611 554 303 645 695 790 777 811 571 314\n",
      " 231 808 333  90 932 560 765 502 180 788 334 503 885 772  19 409 705 721\n",
      " 888 838  21 656 944 608 389 719 569 552 413 982 159 178  24 993 321 447\n",
      " 497 260 253 531 252 876 997  54 899  78 249 373 228 910 830 485   2 977\n",
      "  34 697  67 388 204 994 188 703 522 536  43 812  10 598 468 874 441 665\n",
      "  76 970 968 959 132 631 147 324 558 269 122 600 404 793  13 747 256 398\n",
      " 744  84 936 575 566 496 559 187 123 661 875 782 223 674 361 422 368 276\n",
      "  22 237 731 794 863 488 436 445 886 226 202 646 609 459 335  33 467 438\n",
      " 300 761  97 498 935 420 576 427 553 218 217 184  48 234 668  88 185 805\n",
      " 583 637 614 139 561 621 704 732 606 952 922 835 151 804 290 160 709 972\n",
      " 969 101 148  58 512 367 846 658 295 858 839 895 489 287 341 387 742 161\n",
      " 168 487 183 236 229  85 845 418 215 675 363 532 939 816 678 526 211 956\n",
      " 568 415 667 364 908  56 125 248 266 967 293 905 508  14 748 232 687 432\n",
      " 810 144 117 466 322 991  27  60 612 587 111 990 698 189 430 370 292 141\n",
      "  68 405 545 428 738 208 317 820 859 867 911 521 426 848 403 220 653 102\n",
      " 116 510 239 652 410 166 828 390 786 205 589 268 483  35 964 304 242 400\n",
      "  71  16 753 114   9 632 214 749  42 856 504 120  46 981 862 573 841 834\n",
      "  18 350 769 349 689 210 824 594 651 391 947 813 735 270 221 795 784 630\n",
      " 245   6 771 121 283 305 119 778 682 628 799 639 212 679 191 966  73 626\n",
      " 960 549 537  98  64 987 603 235 291 797 696 909 138 943  38 192 486 306\n",
      " 831 567 946 857 663 162 971 590 115 937 480 506 513 729 871  39 699 110\n",
      " 523 104  89 892 439  26 149 294 722 175 818 806 673 700 451 285  31 262\n",
      " 181 701 868 686 176 708 962 219 455  15 171 992 343 636 419 623 133 282\n",
      " 444 401 890 247 541 514 490 768 718 216 492 601 465 789 618 424 599 412\n",
      " 482 259 604 534 472 177 163 197 850 386 904 629 280 344 355 945 423 516\n",
      "  11 578 973  49 796 713 399 887 193 759 592 106 385 112 126 929 308 602\n",
      "  57 173 411  53 548 906 776  44 182 903 897  36 676 108 518 371 942 238\n",
      " 345 717  32 323 556 767 588 570 849 129 243 194 999 454 470 669 435 446\n",
      " 716 844  94 625 172 985 542 263  37 865 955 434 484 495 406 346 733 989\n",
      " 884 760 807 963 581 983 207 780 954 206 974 340 539 301 366 883 359 507\n",
      " 437 879 273 244 860  29 998 596 864 861 179 826 745 821 949  69 809 481\n",
      " 685 299 565 671   0 524 251 362 680 375 296  20 342 648 702 934 127  50\n",
      " 851 533 240 329 832 633 462 140 452 684 267 714 582 931  47 766 941 896\n",
      "  86 107  62 727 425 414  12 169 550  82 272 918 433 756 337 530 271 494\n",
      " 925 635 310 330  52 758 374 751 672 313 773 725 338 528 739 869 957 619\n",
      "  87 365 706 546 474 551 473 595 664 158 627 660 538  93 461 326 781 736\n",
      " 783 284 174 396 728 724 940 167   5 740 976  80 417  83 196 644 128 155\n",
      " 563 950 852 142 978 255 847 352 499  74 134 774  65  81 988 442 170 463\n",
      " 712 429 511 145 471 339  40   7 913 302 837 798 309 802  75 662 873 230\n",
      " 491 383 278 915 616  55 380 593 752 164 836 203 872 755 916 579 792 357\n",
      " 638 509 693  99 360 958 135 894 397 881 224 156 421 195 475 577  23  95\n",
      " 833 402 953 933 878 150 694 948  92  25 382  17]\n",
      "Test:  [190 378 460 928 500 198 580 785 443   3 275 103 996 348 384 746 146 710\n",
      " 620 478 584 659 902  51 666 336 711 288 298 819   4 775 281 113 898 938\n",
      " 347 681 555 407 866 986 327 246 381 137 770 825 312 870 737 707 152 912\n",
      " 227  59 823 479 543 209 416   8 464 254 408 325 544 572 241 930 995  72\n",
      " 124 153 154  77 914 297 984 222 854 730 379 449 827 655 891 201 634 882\n",
      "   1 642 250  61 311 814 855  45 456 800 356 683 574  79 319 901 815 657\n",
      " 840 923 376 951 431 316 754 351 743 691 880 261 130 100 394 136 613 723\n",
      " 469 597 647 525 843 358 331  28 920 750 476 641 200 279 307 919 493 979\n",
      " 961 690 650  96 453 109 519 586 535 505 131 529 900  41  30 320 105 547\n",
      " 842 877 257 265 734 889 392 354 926 803 791 369 286 779 688  91 787  70\n",
      " 165 315 670 927 654 225 924 757 199 917 213 517 615 720 501 622 458 377\n",
      " 677  66 624 448 450 607 564 274 372 393 457 975 157 762 520 289 907 893\n",
      " 353 980 585 332 233 277 328 921 715 557 726 643 692 617 822 264 605 853\n",
      " 118 649 258 318 562  63 591 540 640 610 741 763 817 527 801 186]\n",
      "Train: [143 477 965 395 515 440 764 829 611 554 303 645 695 790 777 811 571 314\n",
      " 231 808 333  90 932 560 765 502 180 788 334 503 885 772  19 409 705 721\n",
      " 888 838  21 656 944 608 389 719 569 552 413 982 159 178  24 993 321 447\n",
      " 497 260 253 531 252 876 997  54 899  78 249 373 228 910 830 485   2 977\n",
      "  34 697  67 388 204 994 188 703 522 536  43 812  10 598 468 874 441 665\n",
      "  76 970 968 959 132 631 147 324 558 269 122 600 404 793  13 747 256 398\n",
      " 744  84 936 575 566 496 559 187 123 661 875 782 223 674 361 422 368 276\n",
      "  22 237 731 794 863 488 436 445 886 226 202 646 609 459 335  33 467 438\n",
      " 300 761  97 498 935 420 576 427 553 218 217 184  48 234 668  88 185 805\n",
      " 583 637 614 139 561 621 704 732 606 952 922 835 151 804 290 160 709 972\n",
      " 969 101 148  58 512 367 846 658 295 858 839 895 489 287 341 387 742 161\n",
      " 168 487 183 236 229  85 845 418 215 675 363 532 939 816 678 526 211 956\n",
      " 568 415 667 364 908  56 125 248 266 967 293 905 508  14 748 232 687 432\n",
      " 810 144 117 466 322 991  27  60 612 587 111 990 698 189 430 370 292 141\n",
      "  68 405 545 428 738 208 317 820 859 867 911 521 426 848 403 220 653 102\n",
      " 116 510 239 652 410 166 828 390 786 205 589 268 483  35 964 304 242 400\n",
      "  71  16 753 114   9 632 214 749  42 856 504 120  46 981 862 573 841 834\n",
      "  18 350 769 349 689 210 824 594 651 391 947 813 735 270 221 795 784 630\n",
      " 245   6 771 121 283 305 119 778 682 628 799 639 212 679 191 966  73 626\n",
      " 960 549 537  98  64 987 603 235 291 797 696 909 138 943  38 192 486 306\n",
      " 831 567 946 857 663 162 971 590 115 937 480 506 513 729 871  39 699 110\n",
      " 523 104  89 892 439  26 149 294 722 175 818 806 673 700 451 285  31 262\n",
      " 181 701 868 686 176 708 962 219 455  15 171 992 343 636 419 623 133 282\n",
      " 444 401 890 247 541 514 490 768 718 216 492 601 465 789 618 424 599 412\n",
      " 482 259 604 534 472 177 163 197 850 386 904 629 280 344 355 945 423 516\n",
      "  11 578 973  49 796 713 399 887 193 759 592 106 385 112 126 929 308 602\n",
      "  57 173 411  53 548 906 776  44 182 903 897  36 676 108 518 371 942 238\n",
      " 345 717  32 323 556 767 588 570 849 129 243 194 999 454 190 378 460 928\n",
      " 500 198 580 785 443   3 275 103 996 348 384 746 146 710 620 478 584 659\n",
      " 902  51 666 336 711 288 298 819   4 775 281 113 898 938 347 681 555 407\n",
      " 866 986 327 246 381 137 770 825 312 870 737 707 152 912 227  59 823 479\n",
      " 543 209 416   8 464 254 408 325 544 572 241 930 995  72 124 153 154  77\n",
      " 914 297 984 222 854 730 379 449 827 655 891 201 634 882   1 642 250  61\n",
      " 311 814 855  45 456 800 356 683 574  79 319 901 815 657 840 923 376 951\n",
      " 431 316 754 351 743 691 880 261 130 100 394 136 613 723 469 597 647 525\n",
      " 843 358 331  28 920 750 476 641 200 279 307 919 493 979 961 690 650  96\n",
      " 453 109 519 586 535 505 131 529 900  41  30 320 105 547 842 877 257 265\n",
      " 734 889 392 354 926 803 791 369 286 779 688  91 787  70 165 315 670 927\n",
      " 654 225 924 757 199 917 213 517 615 720 501 622 458 377 677  66 624 448\n",
      " 450 607 564 274 372 393 457 975 157 762 520 289 907 893 353 980 585 332\n",
      " 233 277 328 921 715 557 726 643 692 617 822 264 605 853 118 649 258 318\n",
      " 562  63 591 540 640 610 741 763 817 527 801 186]\n",
      "Test:  [470 669 435 446 716 844  94 625 172 985 542 263  37 865 955 434 484 495\n",
      " 406 346 733 989 884 760 807 963 581 983 207 780 954 206 974 340 539 301\n",
      " 366 883 359 507 437 879 273 244 860  29 998 596 864 861 179 826 745 821\n",
      " 949  69 809 481 685 299 565 671   0 524 251 362 680 375 296  20 342 648\n",
      " 702 934 127  50 851 533 240 329 832 633 462 140 452 684 267 714 582 931\n",
      "  47 766 941 896  86 107  62 727 425 414  12 169 550  82 272 918 433 756\n",
      " 337 530 271 494 925 635 310 330  52 758 374 751 672 313 773 725 338 528\n",
      " 739 869 957 619  87 365 706 546 474 551 473 595 664 158 627 660 538  93\n",
      " 461 326 781 736 783 284 174 396 728 724 940 167   5 740 976  80 417  83\n",
      " 196 644 128 155 563 950 852 142 978 255 847 352 499  74 134 774  65  81\n",
      " 988 442 170 463 712 429 511 145 471 339  40   7 913 302 837 798 309 802\n",
      "  75 662 873 230 491 383 278 915 616  55 380 593 752 164 836 203 872 755\n",
      " 916 579 792 357 638 509 693  99 360 958 135 894 397 881 224 156 421 195\n",
      " 475 577  23  95 833 402 953 933 878 150 694 948  92  25 382  17]\n"
     ]
    }
   ],
   "source": [
    "validacion_simple = ValidacionSimple(0.75)\n",
    "validacion_simple.creaParticiones(dataset)\n",
    "v_cruzada = ValidacionCruzada(4)\n",
    "v_cruzada.creaParticiones(dataset)\n",
    "\n",
    "print(\"VALIDACION SIMPLE\",validacion_simple.particiones[0])\n",
    "\n",
    "print(\"VALIDACION CRUZADA\")\n",
    "for particion in v_cruzada.particiones:\n",
    "    print(particion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[0.20260223, 0.44545455],\n",
      "       [0.25092937, 0.32727273],\n",
      "       [0.06877323, 0.05909091],\n",
      "       [0.47769517, 0.16818182]])\n",
      " array([[19.24906367, 24.5       ],\n",
      "       [10.95854245, 13.27277916]])\n",
      " array([[0.02411874, 0.08144796],\n",
      "       [0.03339518, 0.09049774],\n",
      "       [0.50278293, 0.55656109],\n",
      "       [0.08348794, 0.09954751],\n",
      "       [0.35621521, 0.1719457 ]])\n",
      " array([[0.19852941, 0.30530973],\n",
      "       [0.12132353, 0.04867257],\n",
      "       [0.01102941, 0.02212389],\n",
      "       [0.17830882, 0.18141593],\n",
      "       [0.30882353, 0.22123894],\n",
      "       [0.01470588, 0.01769912],\n",
      "       [0.02389706, 0.03539823],\n",
      "       [0.04411765, 0.05309735],\n",
      "       [0.01654412, 0.00884956],\n",
      "       [0.08272059, 0.10619469]])\n",
      " array([[2949.76217228, 3803.81018519],\n",
      "       [2346.83292876, 3496.08345233]])\n",
      " array([[0.52875696, 0.70588235],\n",
      "       [0.11131725, 0.12217195],\n",
      "       [0.0742115 , 0.04072398],\n",
      "       [0.06493506, 0.02714932],\n",
      "       [0.22077922, 0.1040724 ]])\n",
      " array([[0.05751391, 0.09049774],\n",
      "       [0.14471243, 0.2081448 ],\n",
      "       [0.32282004, 0.33936652],\n",
      "       [0.19851577, 0.14479638],\n",
      "       [0.27643785, 0.21719457]])\n",
      " array([[2.89325843, 3.08796296],\n",
      "       [1.14894165, 1.09571705]])\n",
      " array([[0.03717472, 0.07727273],\n",
      "       [0.29925651, 0.36363636],\n",
      "       [0.56877323, 0.45454545],\n",
      "       [0.09479554, 0.10454545]])\n",
      " array([[0.90875233, 0.88584475],\n",
      "       [0.02793296, 0.07305936],\n",
      "       [0.06331471, 0.04109589]])\n",
      " array([[2.83707865, 2.82407407],\n",
      "       [1.10905858, 1.09568771]])\n",
      " array([[0.31412639, 0.21363636],\n",
      "       [0.24535316, 0.21818182],\n",
      "       [0.32342007, 0.36363636],\n",
      "       [0.11710037, 0.20454545]])\n",
      " array([[36.41198502, 33.35185185],\n",
      "       [11.44023415, 10.66997762]])\n",
      " array([[0.11918063, 0.21917808],\n",
      "       [0.04283054, 0.05022831],\n",
      "       [0.83798883, 0.73059361]])\n",
      " array([[0.16014898, 0.25114155],\n",
      "       [0.75791434, 0.61643836],\n",
      "       [0.08193669, 0.13242009]])\n",
      " array([[1.41947566, 1.38425926],\n",
      "       [0.57415865, 0.5896738 ]])\n",
      " array([[0.02788104, 0.03181818],\n",
      "       [0.19702602, 0.15909091],\n",
      "       [0.6394052 , 0.65      ],\n",
      "       [0.13568773, 0.15909091]])\n",
      " array([[1.1423221 , 1.17592593],\n",
      "       [0.34938019, 0.38075713]])\n",
      " array([[0.58768657, 0.63302752],\n",
      "       [0.41231343, 0.36697248]])\n",
      " array([[0.95149254, 0.98165138],\n",
      "       [0.04850746, 0.01834862]])]\n",
      "[array([[0.20231214, 0.44351464],\n",
      "       [0.238921  , 0.35146444],\n",
      "       [0.06358382, 0.05020921],\n",
      "       [0.49518304, 0.15481172]])\n",
      " array([[19.03300971, 25.43829787],\n",
      "       [10.93213299, 13.43096697]])\n",
      " array([[0.01923077, 0.09583333],\n",
      "       [0.03269231, 0.1       ],\n",
      "       [0.51730769, 0.55833333],\n",
      "       [0.08269231, 0.08333333],\n",
      "       [0.34807692, 0.1625    ]])\n",
      " array([[0.19809524, 0.27346939],\n",
      "       [0.13333333, 0.06122449],\n",
      "       [0.01142857, 0.0244898 ],\n",
      "       [0.17142857, 0.18367347],\n",
      "       [0.31428571, 0.19183673],\n",
      "       [0.0152381 , 0.02040816],\n",
      "       [0.02285714, 0.03265306],\n",
      "       [0.04190476, 0.08163265],\n",
      "       [0.01333333, 0.00816327],\n",
      "       [0.07809524, 0.12244898]])\n",
      " array([[2945.22524272, 4040.7106383 ],\n",
      "       [2387.33253827, 3599.15877425]])\n",
      " array([[0.53653846, 0.7125    ],\n",
      "       [0.09423077, 0.1125    ],\n",
      "       [0.07884615, 0.0375    ],\n",
      "       [0.06730769, 0.02083333],\n",
      "       [0.22307692, 0.11666667]])\n",
      " array([[0.05384615, 0.075     ],\n",
      "       [0.15576923, 0.22083333],\n",
      "       [0.32884615, 0.36666667],\n",
      "       [0.18269231, 0.11666667],\n",
      "       [0.27884615, 0.22083333]])\n",
      " array([[2.92815534, 3.11914894],\n",
      "       [1.12547013, 1.08110233]])\n",
      " array([[0.05009634, 0.06276151],\n",
      "       [0.29094412, 0.37656904],\n",
      "       [0.56069364, 0.46861925],\n",
      "       [0.0982659 , 0.09205021]])\n",
      " array([[0.91891892, 0.91176471],\n",
      "       [0.02509653, 0.05462185],\n",
      "       [0.05598456, 0.03361345]])\n",
      " array([[2.86407767, 2.87234043],\n",
      "       [1.11867244, 1.06425523]])\n",
      " array([[0.31213873, 0.20502092],\n",
      "       [0.22543353, 0.25104603],\n",
      "       [0.34104046, 0.31380753],\n",
      "       [0.12138728, 0.23012552]])\n",
      " array([[36.14563107, 33.93191489],\n",
      "       [11.21753808, 11.13570272]])\n",
      " array([[0.11389961, 0.17226891],\n",
      "       [0.04440154, 0.07983193],\n",
      "       [0.84169884, 0.74789916]])\n",
      " array([[0.14478764, 0.21008403],\n",
      "       [0.76640927, 0.61764706],\n",
      "       [0.08880309, 0.17226891]])\n",
      " array([[1.43495146, 1.36170213],\n",
      "       [0.60510911, 0.56964087]])\n",
      " array([[0.02119461, 0.0209205 ],\n",
      "       [0.20231214, 0.18410042],\n",
      "       [0.64354528, 0.59832636],\n",
      "       [0.13294798, 0.19665272]])\n",
      " array([[1.14563107, 1.14893617],\n",
      "       [0.35273596, 0.35602554]])\n",
      " array([[0.56866538, 0.59915612],\n",
      "       [0.43133462, 0.40084388]])\n",
      " array([[0.9516441 , 0.97890295],\n",
      "       [0.0483559 , 0.02109705]])]\n",
      "[array([[0.20377358, 0.42105263],\n",
      "       [0.24528302, 0.35087719],\n",
      "       [0.07358491, 0.05701754],\n",
      "       [0.47735849, 0.17105263]])\n",
      " array([[19.54562738, 24.31696429],\n",
      "       [11.2750275 , 13.25783049]])\n",
      " array([[0.02259887, 0.09170306],\n",
      "       [0.03766478, 0.09170306],\n",
      "       [0.50847458, 0.55895197],\n",
      "       [0.09039548, 0.10917031],\n",
      "       [0.34086629, 0.14847162]])\n",
      " array([[0.2108209 , 0.29487179],\n",
      "       [0.12686567, 0.04273504],\n",
      "       [0.0130597 , 0.02136752],\n",
      "       [0.17537313, 0.19230769],\n",
      "       [0.28731343, 0.20940171],\n",
      "       [0.00932836, 0.01282051],\n",
      "       [0.02425373, 0.03418803],\n",
      "       [0.04664179, 0.07692308],\n",
      "       [0.01492537, 0.00854701],\n",
      "       [0.09141791, 0.10683761]])\n",
      " array([[3007.00380228, 4015.48660714],\n",
      "       [2428.15539996, 3693.4318161 ]])\n",
      " array([[0.54613936, 0.70742358],\n",
      "       [0.10734463, 0.12227074],\n",
      "       [0.07721281, 0.0349345 ],\n",
      "       [0.05461394, 0.02620087],\n",
      "       [0.21468927, 0.10917031]])\n",
      " array([[0.06026365, 0.069869  ],\n",
      "       [0.1393597 , 0.24017467],\n",
      "       [0.33145009, 0.33624454],\n",
      "       [0.19774011, 0.1441048 ],\n",
      "       [0.27118644, 0.20960699]])\n",
      " array([[2.92205323, 3.01785714],\n",
      "       [1.12676071, 1.11389073]])\n",
      " array([[0.04150943, 0.07894737],\n",
      "       [0.28301887, 0.34649123],\n",
      "       [0.58490566, 0.48245614],\n",
      "       [0.09056604, 0.09210526]])\n",
      " array([[0.8979206 , 0.89867841],\n",
      "       [0.03591682, 0.06167401],\n",
      "       [0.06616257, 0.03964758]])\n",
      " array([[2.8973384 , 2.83928571],\n",
      "       [1.08608226, 1.12244608]])\n",
      " array([[0.30377358, 0.18421053],\n",
      "       [0.23396226, 0.23245614],\n",
      "       [0.33584906, 0.35087719],\n",
      "       [0.12641509, 0.23245614]])\n",
      " array([[36.54562738, 34.23660714],\n",
      "       [11.53309051, 11.63132697]])\n",
      " array([[0.12098299, 0.20264317],\n",
      "       [0.03780718, 0.04845815],\n",
      "       [0.84120983, 0.74889868]])\n",
      " array([[0.16068053, 0.25991189],\n",
      "       [0.73724008, 0.59911894],\n",
      "       [0.1020794 , 0.14096916]])\n",
      " array([[1.42205323, 1.36160714],\n",
      "       [0.57234124, 0.53330933]])\n",
      " array([[0.02641509, 0.03070175],\n",
      "       [0.20377358, 0.17105263],\n",
      "       [0.63207547, 0.64035088],\n",
      "       [0.13773585, 0.15789474]])\n",
      " array([[1.1730038 , 1.15625   ],\n",
      "       [0.37825056, 0.36309219]])\n",
      " array([[0.58143939, 0.63274336],\n",
      "       [0.41856061, 0.36725664]])\n",
      " array([[0.95833333, 0.98672566],\n",
      "       [0.04166667, 0.01327434]])]\n",
      "[array([[0.18609023, 0.45575221],\n",
      "       [0.22556391, 0.3539823 ],\n",
      "       [0.08082707, 0.04424779],\n",
      "       [0.5075188 , 0.1460177 ]])\n",
      " array([[19.03030303, 25.26126126],\n",
      "       [11.12238352, 13.17533302]])\n",
      " array([[0.02439024, 0.08370044],\n",
      "       [0.02626642, 0.09251101],\n",
      "       [0.51407129, 0.56387665],\n",
      "       [0.07879925, 0.09251101],\n",
      "       [0.3564728 , 0.16740088]])\n",
      " array([[0.21189591, 0.30603448],\n",
      "       [0.11152416, 0.06034483],\n",
      "       [0.01301115, 0.01724138],\n",
      "       [0.17286245, 0.18534483],\n",
      "       [0.30855019, 0.20689655],\n",
      "       [0.01486989, 0.01724138],\n",
      "       [0.01858736, 0.02155172],\n",
      "       [0.04089219, 0.06034483],\n",
      "       [0.01486989, 0.00862069],\n",
      "       [0.0929368 , 0.11637931]])\n",
      " array([[2938.66856061, 3815.04954955],\n",
      "       [2353.66408218, 3463.63623726]])\n",
      " array([[0.54033771, 0.7092511 ],\n",
      "       [0.10318949, 0.11894273],\n",
      "       [0.06941839, 0.03964758],\n",
      "       [0.06378987, 0.01762115],\n",
      "       [0.22326454, 0.11453744]])\n",
      " array([[0.0619137 , 0.08810573],\n",
      "       [0.15009381, 0.23788546],\n",
      "       [0.34521576, 0.31277533],\n",
      "       [0.18574109, 0.14977974],\n",
      "       [0.25703565, 0.21145374]])\n",
      " array([[2.93181818, 3.09459459],\n",
      "       [1.12777054, 1.08431386]])\n",
      " array([[0.04135338, 0.06637168],\n",
      "       [0.29511278, 0.38495575],\n",
      "       [0.56578947, 0.46902655],\n",
      "       [0.09774436, 0.07964602]])\n",
      " array([[0.9039548 , 0.89777778],\n",
      "       [0.03766478, 0.06666667],\n",
      "       [0.05838041, 0.03555556]])\n",
      " array([[2.79734848, 2.84684685],\n",
      "       [1.10894801, 1.08800336]])\n",
      " array([[0.32706767, 0.19026549],\n",
      "       [0.2406015 , 0.22566372],\n",
      "       [0.30451128, 0.38053097],\n",
      "       [0.12781955, 0.20353982]])\n",
      " array([[36.07007576, 33.21171171],\n",
      "       [11.64714876, 10.51190864]])\n",
      " array([[0.12052731, 0.2       ],\n",
      "       [0.03954802, 0.05777778],\n",
      "       [0.83992467, 0.74222222]])\n",
      " array([[0.16384181, 0.25333333],\n",
      "       [0.74387947, 0.62222222],\n",
      "       [0.09227872, 0.12444444]])\n",
      " array([[1.43560606, 1.37387387],\n",
      "       [0.6027252 , 0.56140296]])\n",
      " array([[0.02819549, 0.03097345],\n",
      "       [0.20112782, 0.19911504],\n",
      "       [0.62593985, 0.61061947],\n",
      "       [0.14473684, 0.15929204]])\n",
      " array([[1.15530303, 1.14414414],\n",
      "       [0.36219332, 0.35123583]])\n",
      " array([[0.58301887, 0.63392857],\n",
      "       [0.41698113, 0.36607143]])\n",
      " array([[0.9490566 , 0.98214286],\n",
      "       [0.0509434 , 0.01785714]])]\n",
      "[array([[0.20373832, 0.46636771],\n",
      "       [0.22803738, 0.33632287],\n",
      "       [0.06728972, 0.04932735],\n",
      "       [0.50093458, 0.14798206]])\n",
      " array([[19.2165725 , 24.38812785],\n",
      "       [10.94362322, 13.12497169]])\n",
      " array([[0.0261194 , 0.07142857],\n",
      "       [0.02985075, 0.09821429],\n",
      "       [0.51119403, 0.54017857],\n",
      "       [0.09514925, 0.09821429],\n",
      "       [0.33768657, 0.19196429]])\n",
      " array([[0.19963031, 0.27947598],\n",
      "       [0.11829945, 0.069869  ],\n",
      "       [0.00924214, 0.01746725],\n",
      "       [0.17744917, 0.19650655],\n",
      "       [0.31977819, 0.20087336],\n",
      "       [0.012939  , 0.01746725],\n",
      "       [0.02033272, 0.03056769],\n",
      "       [0.03512015, 0.07860262],\n",
      "       [0.00924214, 0.00436681],\n",
      "       [0.09796673, 0.10480349]])\n",
      " array([[3049.65725047, 3873.68493151],\n",
      "       [2426.97853184, 3340.0017002 ]])\n",
      " array([[0.56902985, 0.71875   ],\n",
      "       [0.09328358, 0.10714286],\n",
      "       [0.07649254, 0.04910714],\n",
      "       [0.05970149, 0.03125   ],\n",
      "       [0.20149254, 0.09375   ]])\n",
      " array([[0.05223881, 0.08482143],\n",
      "       [0.13992537, 0.23214286],\n",
      "       [0.33208955, 0.35714286],\n",
      "       [0.20522388, 0.11607143],\n",
      "       [0.27052239, 0.20982143]])\n",
      " array([[2.89830508, 3.15525114],\n",
      "       [1.12872816, 1.06140568]])\n",
      " array([[0.04485981, 0.07174888],\n",
      "       [0.27850467, 0.33632287],\n",
      "       [0.57570093, 0.51121076],\n",
      "       [0.10093458, 0.08071749]])\n",
      " array([[0.89513109, 0.88738739],\n",
      "       [0.03932584, 0.07207207],\n",
      "       [0.06554307, 0.04054054]])\n",
      " array([[2.81355932, 2.84018265],\n",
      "       [1.11363944, 1.09670822]])\n",
      " array([[0.32336449, 0.22421525],\n",
      "       [0.22056075, 0.23766816],\n",
      "       [0.33084112, 0.30941704],\n",
      "       [0.12523364, 0.22869955]])\n",
      " array([[36.13559322, 34.47945205],\n",
      "       [11.07721889, 11.46618167]])\n",
      " array([[0.11797753, 0.19369369],\n",
      "       [0.04494382, 0.08108108],\n",
      "       [0.83707865, 0.72522523]])\n",
      " array([[0.15730337, 0.21621622],\n",
      "       [0.75468165, 0.62612613],\n",
      "       [0.08801498, 0.15765766]])\n",
      " array([[1.40489642, 1.36986301],\n",
      "       [0.5556545 , 0.5695512 ]])\n",
      " array([[0.01682243, 0.02690583],\n",
      "       [0.21682243, 0.19730942],\n",
      "       [0.62429907, 0.60538117],\n",
      "       [0.14205607, 0.17040359]])\n",
      " array([[1.14877589, 1.16438356],\n",
      "       [0.35586743, 0.37062327]])\n",
      " array([[0.60225141, 0.62443439],\n",
      "       [0.39774859, 0.37556561]])\n",
      " array([[0.94559099, 0.98190045],\n",
      "       [0.05440901, 0.01809955]])]\n",
      "El error del Clasificador NaiveBayes  para Validacion Simple es: 0.336\n",
      "El error del Clasificador NaiveBayes para Validacion Cruzada es: 0.30000000000000004\n"
     ]
    }
   ],
   "source": [
    "clasificador = ClasificadorNaiveBayes(True)\n",
    "#clasificador.entrenamiento(dataset, validacion_simple.particiones[0].indicesTrain)\n",
    "#clasificador.clasifica(dataset, validacion_simple.particiones[0].indicesTest)\n",
    "error = clasificador.validacion(validacion_simple,dataset,clasificador)\n",
    "error1 = clasificador.validacion(v_cruzada,dataset,clasificador)\n",
    "print(\"El error del Clasificador NaiveBayes  para Validacion Simple es: \"+str(error))\n",
    "print(\"El error del Clasificador NaiveBayes para Validacion Cruzada es: \"+str(error1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicciones utilizando validacion simple sin correcion de Laplace: [1. 1. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0.\n",
      " 1. 0. 0. 1. 1. 1. 1. 0. 1. 0. 1. 0. 1. 0. 1. 1. 0. 0. 1. 0. 1. 0. 0. 0.\n",
      " 0. 0. 1. 1. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 1. 1.\n",
      " 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 1.\n",
      " 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 1. 1. 1. 0. 1. 0. 0. 1. 1. 1. 0. 1. 0. 1.\n",
      " 0. 1. 1. 0. 1. 1. 1. 0. 1. 0. 0. 0. 1. 1. 0. 0. 1. 0. 0. 0. 0. 1. 0. 1.\n",
      " 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1.\n",
      " 1. 1. 0. 1. 1. 0. 0. 0.]\n",
      "Clases reales de la particion de text [1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 1. 0. 0. 1.\n",
      " 0. 0. 1. 0. 1. 1. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 1. 1. 1. 0. 0. 1. 0.\n",
      " 0. 0. 1. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 1.\n",
      " 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 0. 1. 0.\n",
      " 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 1. 0. 1. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 1.\n",
      " 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n",
      " 0. 0. 1. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 1. 1. 0. 0. 1. 1. 0. 1.\n",
      " 1. 0. 1. 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 0. 0. 0. 0.\n",
      " 0. 0. 1. 0. 1. 0. 0. 0.]\n",
      "Predicciones utilizando validacion simple con correcion de Laplace: [1. 1. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0.\n",
      " 1. 0. 0. 1. 1. 1. 1. 0. 1. 0. 1. 0. 1. 0. 1. 1. 0. 0. 1. 0. 1. 0. 0. 0.\n",
      " 0. 0. 1. 1. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 1. 1.\n",
      " 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 1.\n",
      " 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 1. 1. 1. 0. 1. 0. 0. 1. 1. 1. 0. 1. 0. 1.\n",
      " 0. 1. 1. 0. 1. 1. 1. 0. 1. 0. 0. 0. 1. 1. 0. 0. 1. 0. 0. 0. 0. 1. 0. 1.\n",
      " 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1.\n",
      " 1. 1. 0. 1. 1. 0. 0. 0.]\n",
      "Clases reales de la particion de text [1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 1. 0. 0. 1.\n",
      " 0. 0. 1. 0. 1. 1. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 1. 1. 1. 0. 0. 1. 0.\n",
      " 0. 0. 1. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 1.\n",
      " 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 0. 1. 0.\n",
      " 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 1. 0. 1. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 1.\n",
      " 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n",
      " 0. 0. 1. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 1. 1. 0. 0. 1. 1. 0. 1.\n",
      " 1. 0. 1. 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 0. 0. 0. 0.\n",
      " 0. 0. 1. 0. 1. 0. 0. 0.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pablo/anaconda3/lib/python3.7/site-packages/sklearn/preprocessing/_encoders.py:415: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/pablo/anaconda3/lib/python3.7/site-packages/sklearn/preprocessing/_encoders.py:451: DeprecationWarning: The 'categorical_features' keyword is deprecated in version 0.20 and will be removed in 0.22. You can use the ColumnTransformer instead.\n",
      "  \"use the ColumnTransformer instead.\", DeprecationWarning)\n",
      "/home/pablo/anaconda3/lib/python3.7/site-packages/sklearn/naive_bayes.py:485: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n",
      "/home/pablo/anaconda3/lib/python3.7/site-packages/sklearn/naive_bayes.py:485: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n"
     ]
    }
   ],
   "source": [
    "#SKLearn Validacion Cruzada\n",
    "# Hacemos el encode de los datos\n",
    "encAtributos = preprocessing.OneHotEncoder(categorical_features=dataset.nominalAtributos[:-1],sparse=False)\n",
    "X = encAtributos.fit_transform(dataset.datos[:,:-1])\n",
    "Y = dataset.datos[:,-1]\n",
    "\n",
    "# Partimos los datos en Train y Test\n",
    "x_train, x_test, y_train,y_test = train_test_split(X,Y,test_size=0.2)\n",
    "\n",
    "# Aplicamos Naive Bayes sin Laplace\n",
    "clf = MultinomialNB(alpha = 0,fit_prior=True, class_prior=None)\n",
    "predicciones=clf.fit(x_train,y_train).predict(x_test)\n",
    "\n",
    "# Print de los resultados\n",
    "print(\"Predicciones utilizando validacion simple sin correcion de Laplace:\", predicciones)\n",
    "print(\"Clases reales de la particion de text\", y_test)\n",
    "\n",
    "# Aplicamos Naive Bayes con Laplace\n",
    "clf = MultinomialNB(alpha = 0,fit_prior=True, class_prior=None)\n",
    "predicciones=clf.fit(x_train,y_train).predict(x_test)\n",
    "\n",
    "# Print de los resultados\n",
    "print(\"Predicciones utilizando validacion simple con correcion de Laplace:\", predicciones)\n",
    "print(\"Clases reales de la particion de text\", y_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicciones utilizando validacion simple sin correcion de Laplace: [0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 0. 0. 1. 0. 0. 1.\n",
      " 1. 0. 0. 1. 1. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0.\n",
      " 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 1. 0. 0. 0.\n",
      " 1. 0. 0. 0. 0. 1. 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 0.\n",
      " 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 1. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 1. 1. 1. 0. 1. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0.\n",
      " 1. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0.\n",
      " 1. 0. 1. 0. 0. 0. 1. 1.]\n",
      "Clases reales de la particion de text [0. 0. 0. 1. 0. 0. 1. 1. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0.\n",
      " 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0.\n",
      " 1. 0. 0. 0. 1. 1. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0.\n",
      " 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 0.\n",
      " 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 1. 1. 0. 0. 1. 0. 0.\n",
      " 1. 0. 0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.\n",
      " 0. 0. 0. 1. 1. 0. 1. 0. 1. 0. 0. 1. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0.\n",
      " 1. 0. 1. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 1. 0. 1. 1. 1. 0. 1. 0. 0.\n",
      " 1. 0. 0. 0. 1. 0. 1. 0.]\n",
      "Predicciones utilizando validacion simple con correcion de Laplace: [0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 0. 0. 1. 0. 0. 1.\n",
      " 1. 0. 0. 1. 1. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0.\n",
      " 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 1. 0. 0. 0.\n",
      " 1. 0. 0. 0. 0. 1. 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 0.\n",
      " 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 1. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 1. 1. 1. 0. 1. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0.\n",
      " 1. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0.\n",
      " 1. 0. 1. 0. 0. 0. 1. 1.]\n",
      "Clases reales de la particion de text [0. 0. 0. 1. 0. 0. 1. 1. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0.\n",
      " 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0.\n",
      " 1. 0. 0. 0. 1. 1. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0.\n",
      " 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 0.\n",
      " 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 1. 1. 0. 0. 1. 0. 0.\n",
      " 1. 0. 0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.\n",
      " 0. 0. 0. 1. 1. 0. 1. 0. 1. 0. 0. 1. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0.\n",
      " 1. 0. 1. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 1. 0. 1. 1. 1. 0. 1. 0. 0.\n",
      " 1. 0. 0. 0. 1. 0. 1. 0.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pablo/anaconda3/lib/python3.7/site-packages/sklearn/preprocessing/_encoders.py:415: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/pablo/anaconda3/lib/python3.7/site-packages/sklearn/preprocessing/_encoders.py:451: DeprecationWarning: The 'categorical_features' keyword is deprecated in version 0.20 and will be removed in 0.22. You can use the ColumnTransformer instead.\n",
      "  \"use the ColumnTransformer instead.\", DeprecationWarning)\n",
      "/home/pablo/anaconda3/lib/python3.7/site-packages/sklearn/naive_bayes.py:485: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n",
      "/home/pablo/anaconda3/lib/python3.7/site-packages/sklearn/naive_bayes.py:485: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n"
     ]
    }
   ],
   "source": [
    "# SKLearn Validacion Cruzada\n",
    "# Hacemos el encode de los datos\n",
    "encAtributos = preprocessing.OneHotEncoder(categorical_features=dataset.nominalAtributos[:-1],sparse=False)\n",
    "X = encAtributos.fit_transform(dataset.datos[:,:-1])\n",
    "Y = dataset.datos[:,-1]\n",
    "\n",
    "# Partimos los datos en Train y Test\n",
    "x_train, x_test, y_train,y_test = train_test_split(X,Y,test_size=0.2)\n",
    "\n",
    "# Aplicamos Naive Bayes sin Laplace\n",
    "clf = MultinomialNB(alpha = 0,fit_prior=True, class_prior=None)\n",
    "predicciones=clf.fit(x_train,y_train).predict(x_test)\n",
    "\n",
    "# Print de los resultados\n",
    "print(\"Predicciones utilizando validacion simple sin correcion de Laplace:\", predicciones)\n",
    "print(\"Clases reales de la particion de text\", y_test)\n",
    "\n",
    "# Aplicamos Naive Bayes con Laplace\n",
    "clf = MultinomialNB(alpha = 0,fit_prior=True, class_prior=None)\n",
    "predicciones=clf.fit(x_train,y_train).predict(x_test)\n",
    "\n",
    "# Print de los resultados\n",
    "print(\"Predicciones utilizando validacion simple con correcion de Laplace:\", predicciones)\n",
    "print(\"Clases reales de la particion de text\", y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
