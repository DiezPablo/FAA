{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/python3\n",
    "# -*- coding: utf-8 -*-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importamos Librerias\n",
    "import numpy as np\n",
    "import math\n",
    "from collections import Counter\n",
    "from sortedcontainers import SortedDict\n",
    "import sklearn.naive_bayes as nb\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold, cross_val_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Datos:\n",
    "\n",
    "  TiposDeAtributos=('Continuo','Nominal')\n",
    "\n",
    "  # TODO: procesar el fichero para asignar correctamente las variables tipoAtributos, nombreAtributos, nominalAtributos, datos y diccionarios\n",
    "  # NOTA: No confundir TiposDeAtributos con tipoAtributos\n",
    "  def __init__(self, nombreFichero):\n",
    "\n",
    "      with open(nombreFichero, \"r\") as f:\n",
    "        # Guardamos el numero de datos que contiene el DataSet y esta en la primera linea\n",
    "        self.numDatos = int(f.readline())\n",
    "\n",
    "        # Guardamos el nombre de los atributos\n",
    "        self.nombreAtributos = f.readline().strip('\\n').split(',')\n",
    "        #print(self.nombreAtributos)\n",
    "\n",
    "        # Leemos el tipo de los atributos de las variables y eliminamos el ultimo que es un salto de linea\n",
    "        self.tipoAtributos = f.readline().strip('\\n').split(',')\n",
    "        #print(self.tipoAtributos)\n",
    "\n",
    "        # Comprobamos que todos los atributos sean Continuos o Nominales\n",
    "        if any(atr not in Datos.TiposDeAtributos for atr in self.tipoAtributos):\n",
    "            raise ValueError(\"Tipo de atributo erroneo\")\n",
    "\n",
    "        # Segun el atributo, asignamos True o False.\n",
    "        self.nominalAtributos = []\n",
    "\n",
    "        # Guardamos en la lista nominalAtributos en la posicion de cada uno si es o no Nominal\n",
    "        for tipo in self.tipoAtributos:\n",
    "            if tipo == self.TiposDeAtributos[0]:\n",
    "                self.nominalAtributos.append(False)\n",
    "            else:\n",
    "                self.nominalAtributos.append(True)\n",
    "        #print(self.nominalAtributos)\n",
    "\n",
    "        # Guardamos los datos del fichero y los formateamos, de tal forma que cada linea es una lista\n",
    "        datos = f.readlines()\n",
    "        datosFormat = []\n",
    "        for lista in datos:\n",
    "            datosFormat.append(lista.strip('\\n').split(','))\n",
    "\n",
    "        # print(set(sorted(datosFormat[0])))\n",
    "        listaDatosAtributos = []\n",
    "        for i in range(len(self.tipoAtributos)):\n",
    "            listaDatosAtributos.append([])\n",
    "\n",
    "        # Hacemos la traspuesta de los datos que guardamos para que cada lista de atributo guarde todos los datos\n",
    "        # de cada atributo.\n",
    "        for lista in datosFormat:\n",
    "            i = 0\n",
    "            for item in lista:\n",
    "                listaDatosAtributos[i].append(item)\n",
    "                i += 1\n",
    "\n",
    "        # Ordenamos y hacemos un set para eliminar repetidos.\n",
    "        i = 0\n",
    "        for item in listaDatosAtributos:\n",
    "            listaDatosAtributos[i] = sorted(set(item))\n",
    "            i += 1\n",
    "\n",
    "\n",
    "        # Creacion de lista diccionarios, en caso de que el atributo sea Continuo, el diccionario estara vacio\n",
    "        self.listaDicts = []\n",
    "        for i in range(len(self.tipoAtributos)):\n",
    "            self.listaDicts.append({})\n",
    "\n",
    "        # Creamos el diccionario tal y como se describe en las diapositivas, por orden y asignando valores numericos crecientes\n",
    "        i = 0\n",
    "        for atributo in listaDatosAtributos:\n",
    "            k = 0\n",
    "            if self.tipoAtributos[i] == \"Nominal\":\n",
    "                for dato in atributo:\n",
    "                    self.listaDicts[i][dato] = k\n",
    "                    k += 1\n",
    "            i += 1\n",
    "\n",
    "        # Creacion de la matriz de datos utilizando el diccionario para mapear los valores\n",
    "        # En primer lugar, creamos una matriz vacia de tamaña numero de atributos.\n",
    "        self.datos = np.empty((int(self.numDatos),int(len(self.tipoAtributos))))\n",
    "        i = 0\n",
    "        j = 0\n",
    "\n",
    "        # Metemos los datos en la matriz, mapeando con los diccionarios en el caso de que sean Nominales, y si son continuos normal.\n",
    "        for i in range(int(self.numDatos)):\n",
    "            for j in range(len(self.tipoAtributos)):\n",
    "                if self.tipoAtributos[j] == 'Nominal':\n",
    "                    self.datos[i][j] = self.listaDicts[j].get(str(datosFormat[i][j]))\n",
    "                else:\n",
    "                    self.datos[i][j] = datosFormat[i][j]\n",
    "        \n",
    "        print(self.nombreAtributos)\n",
    "        print(self.listaDicts)\n",
    "        f.close()\n",
    "\n",
    "\n",
    "  # TODO: implementar en la practica 1\n",
    "  def extraeDatos(self, idx):\n",
    "    return self.datos[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Age', 'Spectacle', 'Astigmatic', 'Tear', 'Class']\n",
      "[{'1': 0, '2': 1, '3': 2}, {'1': 0, '2': 1}, {'1': 0, '2': 1}, {'1': 0, '2': 1}, {'1': 0, '2': 1, '3': 2}]\n",
      "[[0. 0. 0. 0. 2.]\n",
      " [0. 0. 0. 1. 1.]\n",
      " [0. 0. 1. 0. 2.]\n",
      " [0. 0. 1. 1. 0.]\n",
      " [0. 1. 0. 0. 2.]\n",
      " [0. 1. 0. 1. 1.]\n",
      " [0. 1. 1. 0. 2.]\n",
      " [0. 1. 1. 1. 0.]\n",
      " [1. 0. 0. 0. 2.]\n",
      " [1. 0. 0. 1. 1.]\n",
      " [1. 0. 1. 0. 2.]\n",
      " [1. 0. 1. 1. 0.]\n",
      " [1. 1. 0. 0. 2.]\n",
      " [1. 1. 0. 1. 1.]\n",
      " [1. 1. 1. 0. 2.]\n",
      " [1. 1. 1. 1. 2.]\n",
      " [2. 0. 0. 0. 2.]\n",
      " [2. 0. 0. 1. 2.]\n",
      " [2. 0. 1. 0. 2.]\n",
      " [2. 0. 1. 1. 0.]\n",
      " [2. 1. 0. 0. 2.]\n",
      " [2. 1. 0. 1. 1.]\n",
      " [2. 1. 1. 0. 2.]\n",
      " [2. 1. 1. 1. 2.]]\n"
     ]
    }
   ],
   "source": [
    "dataset = Datos('lenses.data')\n",
    "\n",
    "print(dataset.datos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABCMeta,abstractmethod\n",
    "\n",
    "\n",
    "class Particion():\n",
    "\n",
    "  # Esta clase mantiene la lista de �ndices de Train y Test para cada partici�n del conjunto de particiones\n",
    "    def __init__(self,train=[],test=[]):\n",
    "        self.indicesTrain=train\n",
    "        self.indicesTest=test\n",
    "    \n",
    "    def __str__(self):\n",
    "        return \"Train: {}\\nTest:  {}\".format(str(self.indicesTrain),str(self.indicesTest)) \n",
    "\n",
    "#####################################################################################################\n",
    "\n",
    "class EstrategiaParticionado:\n",
    "\n",
    "    # Clase abstracta\n",
    "    __metaclass__ = ABCMeta\n",
    "\n",
    "    # Lista de las particiones\n",
    "    def __init__(self, nombre=\"\"):\n",
    "        self.nombreEstrategia = nombre\n",
    "        self.numeroParticiones = 0\n",
    "        self.particiones=[]\n",
    "\n",
    "    # Atributos: deben rellenarse adecuadamente para cada estrategia concreta: nombreEstrategia, numeroParticiones, listaParticiones. Se pasan en el constructor\n",
    "\n",
    "    @abstractmethod\n",
    "    # TODO: esta funcion deben ser implementadas en cada estrategia concreta\n",
    "    def creaParticiones(self,datos,seed=None):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValidacionSimple(EstrategiaParticionado):\n",
    "    \n",
    "    def __init__(self, porcentaje):\n",
    "        self.porcentaje = porcentaje\n",
    "        super().__init__(\"Validacion simple\")\n",
    "        \n",
    "  # Crea particiones segun el metodo tradicional de division de los datos segun el porcentaje deseado.\n",
    "  # Devuelve una lista de particiones (clase Particion)\n",
    "  # TODO: implementar\n",
    "    def creaParticiones(self,datos,seed=None):\n",
    "        np.random.seed(seed)\n",
    "\n",
    "        self.numeroParticiones = 1\n",
    "    \n",
    "        # Generamos una lista con todos los números de datos aleatorios   \n",
    "        indicesAleatorios = np.random.permutation(int(datos.numDatos))\n",
    "        \n",
    "        # Creamos la particion, en funcion del porcentaje especificado\n",
    "        self.particiones = [Particion(indicesAleatorios[:int(datos.numDatos*self.porcentaje)],\n",
    "                                      indicesAleatorios[int(datos.numDatos*self.porcentaje):])]\n",
    "        \n",
    "        return self.particiones\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [ 5 16  1 20 17 19  4 18 15 21  2 13 22  3 10  6 12  9]\n",
      "Test:  [ 8  7 11 14  0 23]\n"
     ]
    }
   ],
   "source": [
    "validacion_simple = ValidacionSimple(0.75)\n",
    "validacion_simple.creaParticiones(dataset)\n",
    "print(validacion_simple.particiones[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValidacionCruzada(EstrategiaParticionado):\n",
    "\n",
    "  # Crea particiones segun el metodo de validacion cruzada.\n",
    "  # El conjunto de entrenamiento se crea con las nfolds-1 particiones y el de test con la particion restante\n",
    "  # Esta funcion devuelve una lista de particiones (clase Particion)\n",
    "  # TODO: implementar\n",
    "    \n",
    "    def __init__(self, k):\n",
    "        self.k = k\n",
    "        super().__init__(\"Validacion cruzada\")\n",
    "\n",
    "    def creaParticiones(self,datos,seed=None):\n",
    "        np.random.seed(seed)\n",
    "        \n",
    "        self.numeroParticiones = self.k\n",
    "        \n",
    "        # Generamos una lista con todos los números de datos aleatorios   \n",
    "        indicesAleatorios = np.random.permutation(int(datos.numDatos))\n",
    "        \n",
    "        # Hallamos el tamaño de cada bloque\n",
    "        tamBloque = int(datos.numDatos/self.k)\n",
    "        \n",
    "        \n",
    "        datosSobran = datos.numDatos - (tamBloque*self.k)\n",
    "        count = 0\n",
    "        for i in range(self.k):\n",
    "            \n",
    "            train = np.delete(indicesAleatorios, range(i*tamBloque,(i+1)*tamBloque))\n",
    "            test =  indicesAleatorios[i*tamBloque:(i+1)*tamBloque]\n",
    "            \n",
    "            # Caso en el que la cuenta es justa\n",
    "            if datosSobran == 0:\n",
    "                self.particiones.append(Particion(train, test))\n",
    "                \n",
    "            # Contemplamos el caso de que la division para sacar el numero de subconjuntos no fuese entera\n",
    "            if datosSobran > 0:\n",
    "                count += 1\n",
    "                particionTest = np.append(test, train[(datos.numDatos - tamBloque)- i - 1])\n",
    "                particionTrain = np.delete(train, (datos.numDatos - tamBloque)- i - 1)\n",
    "                datosSobran -= 1\n",
    "                self.particiones.append(Particion(particionTrain, particionTest))\n",
    "                \n",
    "            \n",
    "           \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [21  8 12 17 14 20 19 18 11 10  7  9 13  4  5 16 23  2]\n",
      "Test:  [ 0  6  1 22 15  3]\n",
      "Train: [ 0  6  1 22 15  3 19 18 11 10  7  9 13  4  5 16 23  2]\n",
      "Test:  [21  8 12 17 14 20]\n",
      "Train: [ 0  6  1 22 15  3 21  8 12 17 14 20 13  4  5 16 23  2]\n",
      "Test:  [19 18 11 10  7  9]\n",
      "Train: [ 0  6  1 22 15  3 21  8 12 17 14 20 19 18 11 10  7  9]\n",
      "Test:  [13  4  5 16 23  2]\n"
     ]
    }
   ],
   "source": [
    "v_cruzada = ValidacionCruzada(4)\n",
    "v_cruzada.creaParticiones(dataset)\n",
    "\n",
    "for particion in v_cruzada.particiones:\n",
    "    print(particion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Clasificador:\n",
    "  \n",
    "    # Clase abstracta\n",
    "    __metaclass__ = ABCMeta\n",
    "\n",
    "    # Metodos abstractos que se implementan en casa clasificador concreto\n",
    "    @abstractmethod\n",
    "    # TODO: esta funcion debe ser implementada en cada clasificador concreto\n",
    "    # datosTrain: matriz numpy con los datos de entrenamiento\n",
    "    # atributosDiscretos: array bool con la indicatriz de los atributos nominales\n",
    "    # diccionario: array de diccionarios de la estructura Datos utilizados para la codificacion de variables discretas\n",
    "    def entrenamiento(self,datos,datosTrain,atributosDiscretos,diccionario):\n",
    "        pass\n",
    "\n",
    "\n",
    "    @abstractmethod\n",
    "    # TODO: esta funcion debe ser implementada en cada clasificador concreto\n",
    "    # devuelve un numpy array con las predicciones\n",
    "    def clasifica(self,datosTest,atributosDiscretos,diccionario):\n",
    "        pass\n",
    "\n",
    "\n",
    "    # Obtiene el numero de aciertos y errores para calcular la tasa de fallo\n",
    "    # TODO: implementar\n",
    "    def error(self,datos,pred):\n",
    "    # Aqui se compara la prediccion (pred) con las clases reales y se calcula el error    \n",
    "        i = 0\n",
    "        real = datos[:,-1]\n",
    "        error = 0\n",
    "        for i in range(len(real)):\n",
    "            if real[i] != pred[i]:\n",
    "                error += 1\n",
    "        err = (error)/(len(real)+0.0)\n",
    "        return err\n",
    "\n",
    "\n",
    "    # Realiza una clasificacion utilizando una estrategia de particionado determinada\n",
    "    # TODO: implementar esta funcion\n",
    "    def validacion(self,particionado,dataset,clasificador,seed=None):\n",
    "\n",
    "    # Creamos las particiones siguiendo la estrategia llamando a particionado.creaParticiones\n",
    "    # - Para validacion cruzada: en el bucle hasta nv entrenamos el clasificador con la particion de train i\n",
    "    # y obtenemos el error en la particion de test i\n",
    "    # - Para validacion simple (hold-out): entrenamos el clasificador con la particion de train\n",
    "    # y obtenemos el error en la particion test. Otra opci�n es repetir la validaci�n simple un n�mero especificado de veces, obteniendo en cada una un error. Finalmente se calcular�a la media.\n",
    "        errores = 0\n",
    "        #particionado.creaParticiones(dataset, seed)\n",
    "    # Comprobamos si es por validación cruzada o simple, por la longitud de la lista de particiones\n",
    "    # Validación Simple\n",
    "        if len(particionado.particiones) == 1:\n",
    "            clasificador.entrenamiento(dataset, particionado.particiones[0].indicesTrain)\n",
    "            pred = clasificador.clasifica(dataset,particionado.particiones[0].indicesTest)\n",
    "            ret = self.error(dataset.extraeDatos(particionado.particiones[0].indicesTest), pred)\n",
    "            if ret > 0:\n",
    "                return ret\n",
    "            else:\n",
    "                return 0\n",
    "            \n",
    "    # Validación Cruzada        \n",
    "        else:\n",
    "            for particion in particionado.particiones:\n",
    "                clasificador.entrenamiento(dataset, particion.indicesTrain)\n",
    "                pred = clasificador.clasifica(dataset,particion.indicesTest)\n",
    "                ret = self.error(dataset.extraeDatos(particion.indicesTest), pred)\n",
    "                errores += ret\n",
    "            error = errores/len(particionado.particiones)\n",
    "            #Devolucion de la media de los errores\n",
    "            return error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClasificadorNaiveBayes(Clasificador):\n",
    "    \n",
    "    def __init__(self, laplace):\n",
    "        self.laplace = laplace\n",
    "        \n",
    "    \n",
    "    def entrenamiento(self,dataset,datosTrain):\n",
    "     \n",
    "        # Cargamos todos los datos de la clase del dataset desde la matriz de datos\n",
    "        clasesTrain = dataset.extraeDatos(datosTrain)\n",
    "        self.numClases = clasesTrain[:,-1]\n",
    "        # Contamos las apariciones de cada uno para luego calcular la probabilidad a priori de cada clase\n",
    "        counter = Counter(self.numClases)\n",
    "        # Calculamos la probabilidad de la clase y lo metemos en un diccionario ordenado segun el numero \n",
    "        # correspondiente a cada clase asignado en el diccionario\n",
    "        self.dictPrioris={}\n",
    "        for k in counter:\n",
    "            k = int(k)\n",
    "            counter[k] = counter[k]/len(self.numClases)\n",
    "            self.dictPrioris[k] = counter[k]\n",
    "            \n",
    "        # Aqui ordenamos el diccionario para que esten en el mismo orden de como extraemos los datos del dataset\n",
    "        self.dictPrioris = SortedDict(self.dictPrioris)\n",
    "        \n",
    "        # Calcular tablas de probabilidades del entrenamiento. Tenemos que calcular por cada atributo una cuenta\n",
    "        # de las apariciones en cada clase\n",
    "        # Creamos una lista de matrices, donde vamos almacenar todos los datos que hemos obtenido en los datos de Test\n",
    "        self.posteriori = np.zeros(len(dataset.nombreAtributos)-1,dtype=object)\n",
    "        \n",
    "        # Recorremos todos los datos de la matriz sin llegar a la clase\n",
    "        for i in range(len(dataset.nombreAtributos) - 1):\n",
    "            \n",
    "            # Si el dato que obtenemos es Nominal haremos el recuento de todas las veces que sale la P(D|H)\n",
    "            if dataset.nominalAtributos[i] == True:\n",
    "                \n",
    "                # Creamos una matriz de tamaño X: Número de Atributos menos la clase Y: Número de clases\n",
    "                post = np.zeros((len(dataset.listaDicts[i]),len(dataset.listaDicts[-1])))\n",
    "               \n",
    "                # Aqui contamos todos las datos que queremos del datos Train para construir la matriz de entrenamiento\n",
    "                for c in range(len(dataset.listaDicts[-1])):\n",
    "                    datosEnt = dataset.extraeDatos(datosTrain)\n",
    "                    dat = datosEnt[:,i]\n",
    "                    repes = Counter(dat[datosEnt[:,-1] == c])\n",
    "                    for r in repes:\n",
    "                        post[int(r),c] = repes[r]\n",
    "                    if self.laplace == True:\n",
    "                        self.posteriori[i] = post +1\n",
    "                    else:\n",
    "                        self.posteriori[i] = post\n",
    "            \n",
    "            # Si el dato es Continuo obtendremos la media y la desviación tipica de la clase\n",
    "            else:\n",
    "                \n",
    "                # Creamos una matriz de X: Los datos de Media y Desivación típica Y: Número de clases\n",
    "                post = np.zeros((2,len(dataset.listaDicts[-1])))\n",
    "               \n",
    "                # Aqui obtenemos la media y desviación tipica de cada clase, despues de tener los datos de entrenamiento\n",
    "                for c in range(len(dataset.listaDicts[-1])):\n",
    "                    datosEnt = dataset.extraeDatos(datosTrain)\n",
    "                    dat = datosEnt[:,i]\n",
    "                    datos = dat[datosEnt[:,-1] == c]\n",
    "                    post[0][c] = np.mean(datos)\n",
    "                    post[1][c] = np.std(datos)\n",
    "                self.posteriori[i] = post\n",
    "            \n",
    "        #Calculamos los valores de los posteriori de todos las tablas anteriores\n",
    "        for j in range(len(dataset.nominalAtributos)-1):\n",
    "            if dataset.nominalAtributos[j] == True:\n",
    "                for i in range(len(dataset.listaDicts) - 1):\n",
    "                    self.posteriori[i] /= sum(self.posteriori[i])\n",
    "\n",
    "        \n",
    "    def clasifica(self,dataset,datosTest):\n",
    "        j = 0\n",
    "        aux = 1\n",
    "        aux2 = 1\n",
    "        self.prediccion = []\n",
    "        datTest = dataset.extraeDatos(datosTest)\n",
    "        #Ahora vamos a estudiar la probabilidad de la clase con los datos obtenidos en el entrenamiento\n",
    "        #Recorremos todos las datos de la matriz de los datos Test\n",
    "        \n",
    "        for dato in datTest:\n",
    "            mapa = []\n",
    "            listaVerosimilitudes = []\n",
    "            #Aqui obtenemos los prioris de cada clase para poder obtener la probabilidad de cada una\n",
    "            for clase in range(len(self.dictPrioris)):\n",
    "            #Aqui obtenemos cada valor posteriori de nuestro entrenamiento de los datos, es decir, P(D|H)\n",
    "                for atributo in range(len(self.posteriori)):\n",
    "                    if dataset.nominalAtributos[atributo] == True:\n",
    "                        aux = self.posteriori[atributo][int(dato[atributo])][clase]\n",
    "                        listaVerosimilitudes.append(aux)\n",
    "                    #Aqui obtenemos la siguiente probabilidad P(D|H)*P(H)\n",
    "                        #aux2 = self.dictPrioris.get(clase)*aux\n",
    "                        #aux = 1\n",
    "                \n",
    "                    #Lo añadimos a una lista para obtener la probabilidad de las diferentes clases\n",
    "                        #mapa.append(aux2)\n",
    "            \n",
    "            #Aqui obtenemos la probabilidad de los atibutos continuos\n",
    "                    else:\n",
    "                        # Hacemos la formula de la distribucion normal\n",
    "                        exp1 = 1/(self.posteriori[atributo][1][clase]*math.sqrt(2*math.pi))\n",
    "                        exp2 = np.power((dato[atributo]-self.posteriori[atributo][0][clase]) ,2)\n",
    "                        exp3 = np.power(self.posteriori[atributo][1][clase],2)\n",
    "                        exp4 = exp2/exp3   \n",
    "                        exp4 = math.exp((-1/2)* exp3)\n",
    "                        aux = exp1 * exp4\n",
    "                        listaVerosimilitudes.append(aux)\n",
    "                    #aux2 = aux * self.dictPrioris.get(clase)\n",
    "                    for verosimilitud in listaVerosimilitudes:\n",
    "                        aux2 *= verosimilitud\n",
    "                    aux2 *= self.dictPrioris.get(clase)\n",
    "                    mapa.append(aux2)\n",
    "            \n",
    "            #Aqui obtenemos la predicción de mayor probabilidad y la guardamos en nuestra lista de predicciones\n",
    "            self.prediccion.append(np.argmax(mapa))\n",
    "        \n",
    "        #Devolvemos la lista con la predicción de nuestro clasifica   \n",
    "        return self.prediccion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "only size-1 arrays can be converted to Python scalars",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-275-177411b8a88b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#clasificador.entrenamiento(dataset, validacion_simple.particiones[0].indicesTrain)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#clasificador.clasifica(dataset, validacion_simple.particiones[0].indicesTest)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0merror\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclasificador\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidacion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidacion_simple\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mclasificador\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0merror1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclasificador\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidacion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv_cruzada\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mclasificador\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"El error del Clasificador NaiveBayes  para Validacion Simple es: \"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-265-fc3769d437ab>\u001b[0m in \u001b[0;36mvalidacion\u001b[0;34m(self, particionado, dataset, clasificador, seed)\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparticionado\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparticiones\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mclasificador\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mentrenamiento\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparticionado\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparticiones\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindicesTrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m             \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclasificador\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasifica\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mparticionado\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparticiones\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindicesTest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextraeDatos\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparticionado\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparticiones\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindicesTest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-274-e7481a1cc3e1>\u001b[0m in \u001b[0;36mclasifica\u001b[0;34m(self, dataset, datosTest)\u001b[0m\n\u001b[1;32m    103\u001b[0m                         \u001b[0mexp2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdato\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mposteriori\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0matributo\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mclase\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mdato\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0matributo\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mposteriori\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0matributo\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mclase\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m                         \u001b[0mexp3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexp2\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m                         \u001b[0mexp4\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m \u001b[0mexp3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m                         \u001b[0maux\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexp1\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mexp4\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m                         \u001b[0mlistaVerosimilitudes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maux\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: only size-1 arrays can be converted to Python scalars"
     ]
    }
   ],
   "source": [
    "clasificador = ClasificadorNaiveBayes(True)\n",
    "#clasificador.entrenamiento(dataset, validacion_simple.particiones[0].indicesTrain)\n",
    "#clasificador.clasifica(dataset, validacion_simple.particiones[0].indicesTest)\n",
    "error = clasificador.validacion(validacion_simple,dataset,clasificador)\n",
    "error1 = clasificador.validacion(v_cruzada,dataset,clasificador)\n",
    "print(\"El error del Clasificador NaiveBayes  para Validacion Simple es: \"+str(error))\n",
    "print(\"El error del Clasificador NaiveBayes para Validacion Cruzada es: \"+str(error1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['TLeftSq', 'TMidSq', 'TRightSq', 'MLeftSq', 'MMidSq', 'MRightSq', 'BLeftSq', 'BMidSq', 'BRightSq', 'Class']\n",
      "[{'b': 0, 'o': 1, 'x': 2}, {'b': 0, 'o': 1, 'x': 2}, {'b': 0, 'o': 1, 'x': 2}, {'b': 0, 'o': 1, 'x': 2}, {'b': 0, 'o': 1, 'x': 2}, {'b': 0, 'o': 1, 'x': 2}, {'b': 0, 'o': 1, 'x': 2}, {'b': 0, 'o': 1, 'x': 2}, {'b': 0, 'o': 1, 'x': 2}, {'negative': 0, 'positive': 1}]\n",
      "[[2. 2. 2. ... 1. 1. 1.]\n",
      " [2. 2. 2. ... 2. 1. 1.]\n",
      " [2. 2. 2. ... 1. 2. 1.]\n",
      " ...\n",
      " [1. 2. 1. ... 1. 2. 0.]\n",
      " [1. 2. 1. ... 1. 2. 0.]\n",
      " [1. 1. 2. ... 2. 2. 0.]]\n"
     ]
    }
   ],
   "source": [
    "dataset = Datos('tic-tac-toe.data')\n",
    "print(dataset.datos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VALIDACION SIMPLE Train: [ 69 840 850 718 541 916 106 910 306  35 229 546 374 778 903 885 185  33\n",
      " 182  60 331 614 409 689 172 640 254 325 192 741 294 427 152  74 765 560\n",
      " 186 658 589 313 272 410 240 446  19 109 393 609 715 119 533 833 940 300\n",
      " 779 333 198 180  14 856 150 528 390 157 357 768  81 857 561 890 303  64\n",
      " 535 190 794 881 836 644 709 642  68 935 316 672 162 284 554 909 188 607\n",
      " 619 617 412 415 563 674 868 666 544 138 829 259 448 505 558 716 117 101\n",
      " 870 711 520 746 273 766  41 334 690 657 481 722 945 866 739 341 625 457\n",
      " 459  40 467 817 255 841  77 136 456 557 361 914 204 626 299 613 382 276\n",
      " 736 641 864 576 228 612 693 469 618 745 295 555 388 203 453 591 869 943\n",
      " 407 782 728   1 628 556 726 241 954 953 703 932 936 789 783 629 222 898\n",
      " 320 568 441  20 487 403  13 336 861 624 887  42  83 307 113 494 719 748\n",
      " 353 776 750 777 207 790 842 305 634 880 280 754 475 116 845 495  62 616\n",
      " 534 706 838 174 443 144 137 860 518 925 428 478 740 392 134 149 749 735\n",
      " 145 194 565 742 673 552 924 521 828 695 744 756 811 471  80 324 858 807\n",
      " 764 751 775 922  61 801 464  44 115 525 512 830 466 141 480 592 809 620\n",
      " 140 279 168 812 623 952 603 118 928 820 551  70   8 574  75 834 882 129\n",
      " 156  29 217 236 179  56 611   7 848 246 375 622 825 593  52 345 549  92\n",
      " 424 878 651 423 293 951 460 238 753 771 601 455 791 508 451 472 806   5\n",
      " 492 867 321 685 847 239  48 647 660 148 123 814 358 879 355 675 737 653\n",
      "   0 163 573  89 843 366 462 543 103 486 436 214  38 761 793 652 232 268\n",
      " 406 434 216 432 912 223 927 821 404 164 155 900 209 844 721 763 491 159\n",
      " 717  11 208 659 684 539 383 309 454 286 225 386 646 147 397 181  47 323\n",
      " 906  30 414 639 877  25 328 894 907 911 797 705 173 482  55 408 199 329\n",
      " 638 257 500 550 360 154 177 479 275 394 686 700 253 676 114 567 939 670\n",
      " 440 476 169  36 195 120 234 260 379 242 510 633 135 678 760 874 682  97\n",
      " 949 594 755 697 261 523 662 493 269 669 391 896 298 430  46 347 161 835\n",
      " 615 547 444  85 531 720 112 819 798 458 792 178 449 650 608 681 189  10\n",
      " 485 213 274  45 130 463 537 363 282 474 897 497 816 281 769 908 921 732\n",
      " 215 694 846 899  91 805  22  78 649 343   6  49 488 202 302 800 757 743\n",
      " 808 452 184 796 559 489 263 915 918 767 934 586 131 513 124 354 873 941\n",
      " 165 314 327 271 893 422 664  93 643  51 621 399 515 338 318 496 665  82\n",
      " 522 504 197 901 247  58  17 352 439  21 251 337  63 402 372 369 926 122\n",
      " 724 606 332 687 529 158 631  57 381 580 824  94  59 400 636  43 854 919\n",
      "  54 351 153  28 365 571  15 884 278 596 121 127 291 931 425 359 243 524\n",
      "  86 330 645  79 862 431  37 582 656  53 810 187 438 418 713 219 465 289\n",
      " 315 498 950 104 585 818 277 637   9 256 590 548 815  26 553 335 461 249\n",
      " 723  31 201  34 917 610 747 655 855   2 413 105 772 308 445 502 654 785\n",
      " 385 258 266 483 895 499 210 344  39 193 588  66 852 506 398 731 733 770\n",
      " 837 822 584 671 501 905 196 799 100 447 920 839 200 191 509 572  72 417\n",
      " 948 701  71 442  90 142 265 937 110 876 923 224 450  76 632  32]\n",
      "Test:  [712 107  96 758  16 946  73 511 933 680 395 290 708 784 143 781   3 902\n",
      " 312  84 827 167 578 326 262 133 802 350 564 231 473  95 913 470 126 151\n",
      "  24 851 183 942 696 738 419 883 319 788 635 102 759 707 727 384 220 540\n",
      " 663 532 602 730 929 517 530 581  87 507 683 503 411 734 248  50 604 875\n",
      " 545 435 587 401 362 795 600 526 577 630 519 378 930 297 514 725 477 128\n",
      " 704 944 780  65 310 823 287 516  27 340 627 396 484 773 221 583 405 566\n",
      "  98 787 371 270 938 527 288 904 832 389 267 426 205 542 125 283 111 661\n",
      " 597 211  18 437 849 688 285 264 376 679 872 235 349 364 166 346 891 955\n",
      " 468  23 252  99   4 871 569 956 218 813 301  88 668 226 752 859 892 176\n",
      " 146 429 692 804 433 416 237 826 296 420 774 160 605 762 373 957 171 863\n",
      " 562  12 599 377 245 865 698 570  67 677 889 598 667 227 886 702 490 311\n",
      " 206 538 579 230 317 595 348 170 244 803 370 699 322 786 356 575 729 831\n",
      " 233 853 212 710 380 387 888 304 367 108 132 175 339 250 947 368 691 714\n",
      " 421 292 342 536 139 648]\n",
      "VALIDACION CRUZADA\n",
      "Train: [941 780 795 109 333 639 419 279 341 691 305 444 471 888 732 447 623 658\n",
      " 299 863 165 692 254 954 423 847  11 293 553  59 800 174 302 684 861 194\n",
      " 268 887 610 216 889 226 775 562 477 796 544 713  12 366 325 914 628 777\n",
      " 536 497 457 508 714 356 651 885 645 176 510 600 449 842 438 446 337 455\n",
      " 463 638 931 397 150 334 829  25 803  83  35 956  78 284  62 892 227 217\n",
      " 913 192 792 142 566 170 593 825 676 152 637 199 921  68 615 208 603  92\n",
      " 875 517 677 534 202  97 488 412 573 345 374 395 619 877 654 550 719 411\n",
      " 650 837 166 277 125 872 102 673 933 953 561 582 221 118 818 586 519 138\n",
      " 276 155 567 647  34 106 297 904 870 667  72 525  13 704 898 843 244 344\n",
      " 126 158 405 524 484  22 533 662 347 123 110 687 101 572 450 695 709 788\n",
      "  60 219 343 661 699 678 821 580  28 945 938 107 549 838 173 809 456  43\n",
      " 434 156 393 368 750 890 239 379   3 718 797 184 901 172 422 330 348 518\n",
      "  86 362 944 599 307 290 526 190 209 883  87 918 894 424 899 633 808 806\n",
      " 591 352 132 272 256 538 380  19 753 329 136 328   0 902 611 592 859 265\n",
      " 911 339 666 505 744 577 233 741 807 445 617 375 486 264 916 830 502 730\n",
      " 160  65 862 644 791 242 127  36  27 311 458 784 428 789 823 642 151  37\n",
      " 819 786 292 120 291 625 607 841 602 258 113 773 238 111 372 421 280 765\n",
      " 548 386 357  56 827 811 115 410 560 737 575 270 474 163 373 537 768  61\n",
      " 614   4 218 275 853 620 108 503 767 336 383 213 653 182 571 679 539  26\n",
      " 300 309 855  17 493 178 369  73 460  80 551 351 630 848 105 215  58  79\n",
      " 241 596 849 824 682 346 881 418 664 948 521 869 643 427  75 454 820 153\n",
      " 392 496 770 766 448 915 288  38 755 464 327 646 912 262 854 747  69 813\n",
      " 267 900 665 558 470 585 835 689 724 391 740 228 378 453 326 873  57 746\n",
      " 435 723  74 271 624 162  21 708 626 206 568 754 498 426 283 668 482 816\n",
      " 237 919 340 756 507 943 188 314 936  84 927 349 211 893   6 377 632 164\n",
      " 476 917 495 729 878 385 306 144 674 570 832 414 895  39 882 790 802 253\n",
      " 461  49 908 605  33 846  23 303 826 866 822 574 409 506 355 925 376 169\n",
      " 432 701  16 278 930 812 909 389 604 387  40 399 251 274 245 231 928 154\n",
      "  15 799 252 831 554 635 441 304 703  10  32  18 532 542 613 396  85 629\n",
      " 398 616 787 531 957 745 390  98 706 468 606  29 833 752  53 248 321 652\n",
      " 367 698 934 763 100 608 939  63 546 622 175  30 530 670 406  66 130 844\n",
      " 317   5 232 656  96  64 234 946 742 552 705 149 294 951 431 762 697 490\n",
      " 785 711 681 371 236 793 758 648 478 860 686 547 181 133 430 301 700 952\n",
      " 771 779 663  24 774 511 141 437 920 649 836 433 581 594 436 235 858 520\n",
      "  20 212 117 891 187 257 403 320 140 515 319 828 203 459 671 171 541 685\n",
      " 402 588 200 727 137 636 778 189 483 735 565 250 937 543 214 489 147 243\n",
      " 289 273  48 834 949 103  46 782 308 557 657  77  99 407 159 195 529 776\n",
      " 318 814 451 475 940 161 134 358 867 472 874 384 260 876 851 749 177 492\n",
      "  47 466 282 210 499 201 783 569 494  44 897 717 631 926 360 157 361 481\n",
      " 798 769  93 500 675  45  54 694 759 143 733 716 266 856  82 342]\n",
      "Test:  [950 609 148 114 331 693 683 501 196 116   1 817 947 388 485 353 259 576\n",
      " 298 559 850 509 514  89 743 627 751 815 413 710 400 429 942 415 112  88\n",
      " 128   8 601 932 955 382 512 296 884 655 198 590 852 589 439 240 845 527\n",
      " 868 185 145 584 696 335 104 761 186 146 350 731 634 564 168 707   2 801\n",
      " 332 903  55  31 229 131 734 363 910 207 462 223  52 420 760 805 545 929\n",
      " 408  70 907  50 906 467 129 757 587 598 922 230 886 669 864   9 688 491\n",
      " 739 579 621 139 452 516 871 794 840 191  81 167 465 135 365 359  41 555\n",
      " 443 425 597 204 641 857 540 121 364 528  91 712 839 563 323 480 381 179\n",
      " 417 316   7 479  76  94 220 247 896  51 612 469 338 810 249 180 286 772\n",
      " 263 522 222 905 404 721 473 781  14 672 442 315 504  67  95 748  42 322\n",
      " 738 804 193  71 269 370 354 680 923 618 281 660 416 487 659 197 715 324\n",
      " 312 595 225 246 310 394 720 523 640 122 726 255 865 513 224 287 702 578\n",
      " 285 879 880 924 728 935 440 295 764 535 583 261 119 690 124  90 313 205\n",
      " 725 183 736 401 722 556]\n",
      "Train: [950 609 148 114 331 693 683 501 196 116   1 817 947 388 485 353 259 576\n",
      " 298 559 850 509 514  89 743 627 751 815 413 710 400 429 942 415 112  88\n",
      " 128   8 601 932 955 382 512 296 884 655 198 590 852 589 439 240 845 527\n",
      " 868 185 145 584 696 335 104 761 186 146 350 731 634 564 168 707   2 801\n",
      " 332 903  55  31 229 131 734 363 910 207 462 223  52 420 760 805 545 929\n",
      " 408  70 907  50 906 467 129 757 587 598 922 230 886 669 864   9 688 491\n",
      " 739 579 621 139 452 516 871 794 840 191  81 167 465 135 365 359  41 555\n",
      " 443 425 597 204 641 857 540 121 364 528  91 712 839 563 323 480 381 179\n",
      " 417 316   7 479  76  94 220 247 896  51 612 469 338 810 249 180 286 772\n",
      " 263 522 222 905 404 721 473 781  14 672 442 315 504  67  95 748  42 322\n",
      " 738 804 193  71 269 370 354 680 923 618 281 660 416 487 659 197 715 324\n",
      " 312 595 225 246 310 394 720 523 640 122 726 255 865 513 224 287 702 578\n",
      " 285 879 880 924 728 935 440 295 764 535 583 261 119 690 124  90 313 205\n",
      " 725 183 736 401 722 538 380  19 753 329 136 328   0 902 611 592 859 265\n",
      " 911 339 666 505 744 577 233 741 807 445 617 375 486 264 916 830 502 730\n",
      " 160  65 862 644 791 242 127  36  27 311 458 784 428 789 823 642 151  37\n",
      " 819 786 292 120 291 625 607 841 602 258 113 773 238 111 372 421 280 765\n",
      " 548 386 357  56 827 811 115 410 560 737 575 270 474 163 373 537 768  61\n",
      " 614   4 218 275 853 620 108 503 767 336 383 213 653 182 571 679 539  26\n",
      " 300 309 855  17 493 178 369  73 460  80 551 351 630 848 105 215  58  79\n",
      " 241 596 849 824 682 346 881 418 664 948 521 869 643 427  75 454 820 153\n",
      " 392 496 770 766 448 915 288  38 755 464 327 646 912 262 854 747  69 813\n",
      " 267 900 665 558 470 585 835 689 724 391 740 228 378 453 326 873  57 746\n",
      " 435 723  74 271 624 162  21 708 626 206 568 754 498 426 283 668 482 816\n",
      " 237 919 340 756 507 943 188 314 936  84 927 349 211 893   6 377 632 164\n",
      " 476 917 495 729 878 385 306 144 674 570 832 414 895  39 882 790 802 253\n",
      " 461  49 908 605  33 846  23 303 826 866 822 574 409 506 355 925 376 169\n",
      " 432 701  16 278 930 812 909 389 604 387  40 399 251 274 245 231 928 154\n",
      "  15 799 252 831 554 635 441 304 703  10  32  18 532 542 613 396  85 629\n",
      " 398 616 787 531 957 745 390  98 706 468 606  29 833 752  53 248 321 652\n",
      " 367 698 934 763 100 608 939  63 546 622 175  30 530 670 406  66 130 844\n",
      " 317   5 232 656  96  64 234 946 742 552 705 149 294 951 431 762 697 490\n",
      " 785 711 681 371 236 793 758 648 478 860 686 547 181 133 430 301 700 952\n",
      " 771 779 663  24 774 511 141 437 920 649 836 433 581 594 436 235 858 520\n",
      "  20 212 117 891 187 257 403 320 140 515 319 828 203 459 671 171 541 685\n",
      " 402 588 200 727 137 636 778 189 483 735 565 250 937 543 214 489 147 243\n",
      " 289 273  48 834 949 103  46 782 308 557 657  77  99 407 159 195 529 776\n",
      " 318 814 451 475 940 161 134 358 867 472 874 384 260 876 851 749 177 492\n",
      "  47 466 282 210 499 201 783 569 494  44 897 717 631 926 360 157 361 481\n",
      " 798 769  93 500 675  45  54 694 759 143 733 716 266 856  82 556]\n",
      "Test:  [941 780 795 109 333 639 419 279 341 691 305 444 471 888 732 447 623 658\n",
      " 299 863 165 692 254 954 423 847  11 293 553  59 800 174 302 684 861 194\n",
      " 268 887 610 216 889 226 775 562 477 796 544 713  12 366 325 914 628 777\n",
      " 536 497 457 508 714 356 651 885 645 176 510 600 449 842 438 446 337 455\n",
      " 463 638 931 397 150 334 829  25 803  83  35 956  78 284  62 892 227 217\n",
      " 913 192 792 142 566 170 593 825 676 152 637 199 921  68 615 208 603  92\n",
      " 875 517 677 534 202  97 488 412 573 345 374 395 619 877 654 550 719 411\n",
      " 650 837 166 277 125 872 102 673 933 953 561 582 221 118 818 586 519 138\n",
      " 276 155 567 647  34 106 297 904 870 667  72 525  13 704 898 843 244 344\n",
      " 126 158 405 524 484  22 533 662 347 123 110 687 101 572 450 695 709 788\n",
      "  60 219 343 661 699 678 821 580  28 945 938 107 549 838 173 809 456  43\n",
      " 434 156 393 368 750 890 239 379   3 718 797 184 901 172 422 330 348 518\n",
      "  86 362 944 599 307 290 526 190 209 883  87 918 894 424 899 633 808 806\n",
      " 591 352 132 272 256 342]\n",
      "Train: [950 609 148 114 331 693 683 501 196 116   1 817 947 388 485 353 259 576\n",
      " 298 559 850 509 514  89 743 627 751 815 413 710 400 429 942 415 112  88\n",
      " 128   8 601 932 955 382 512 296 884 655 198 590 852 589 439 240 845 527\n",
      " 868 185 145 584 696 335 104 761 186 146 350 731 634 564 168 707   2 801\n",
      " 332 903  55  31 229 131 734 363 910 207 462 223  52 420 760 805 545 929\n",
      " 408  70 907  50 906 467 129 757 587 598 922 230 886 669 864   9 688 491\n",
      " 739 579 621 139 452 516 871 794 840 191  81 167 465 135 365 359  41 555\n",
      " 443 425 597 204 641 857 540 121 364 528  91 712 839 563 323 480 381 179\n",
      " 417 316   7 479  76  94 220 247 896  51 612 469 338 810 249 180 286 772\n",
      " 263 522 222 905 404 721 473 781  14 672 442 315 504  67  95 748  42 322\n",
      " 738 804 193  71 269 370 354 680 923 618 281 660 416 487 659 197 715 324\n",
      " 312 595 225 246 310 394 720 523 640 122 726 255 865 513 224 287 702 578\n",
      " 285 879 880 924 728 935 440 295 764 535 583 261 119 690 124  90 313 205\n",
      " 725 183 736 401 722 941 780 795 109 333 639 419 279 341 691 305 444 471\n",
      " 888 732 447 623 658 299 863 165 692 254 954 423 847  11 293 553  59 800\n",
      " 174 302 684 861 194 268 887 610 216 889 226 775 562 477 796 544 713  12\n",
      " 366 325 914 628 777 536 497 457 508 714 356 651 885 645 176 510 600 449\n",
      " 842 438 446 337 455 463 638 931 397 150 334 829  25 803  83  35 956  78\n",
      " 284  62 892 227 217 913 192 792 142 566 170 593 825 676 152 637 199 921\n",
      "  68 615 208 603  92 875 517 677 534 202  97 488 412 573 345 374 395 619\n",
      " 877 654 550 719 411 650 837 166 277 125 872 102 673 933 953 561 582 221\n",
      " 118 818 586 519 138 276 155 567 647  34 106 297 904 870 667  72 525  13\n",
      " 704 898 843 244 344 126 158 405 524 484  22 533 662 347 123 110 687 101\n",
      " 572 450 695 709 788  60 219 343 661 699 678 821 580  28 945 938 107 549\n",
      " 838 173 809 456  43 434 156 393 368 750 890 239 379   3 718 797 184 901\n",
      " 172 422 330 348 518  86 362 944 599 307 290 526 190 209 883  87 918 894\n",
      " 424 899 633 808 806 591 352 132 272 256 822 574 409 506 355 925 376 169\n",
      " 432 701  16 278 930 812 909 389 604 387  40 399 251 274 245 231 928 154\n",
      "  15 799 252 831 554 635 441 304 703  10  32  18 532 542 613 396  85 629\n",
      " 398 616 787 531 957 745 390  98 706 468 606  29 833 752  53 248 321 652\n",
      " 367 698 934 763 100 608 939  63 546 622 175  30 530 670 406  66 130 844\n",
      " 317   5 232 656  96  64 234 946 742 552 705 149 294 951 431 762 697 490\n",
      " 785 711 681 371 236 793 758 648 478 860 686 547 181 133 430 301 700 952\n",
      " 771 779 663  24 774 511 141 437 920 649 836 433 581 594 436 235 858 520\n",
      "  20 212 117 891 187 257 403 320 140 515 319 828 203 459 671 171 541 685\n",
      " 402 588 200 727 137 636 778 189 483 735 565 250 937 543 214 489 147 243\n",
      " 289 273  48 834 949 103  46 782 308 557 657  77  99 407 159 195 529 776\n",
      " 318 814 451 475 940 161 134 358 867 472 874 384 260 876 851 749 177 492\n",
      "  47 466 282 210 499 201 783 569 494  44 897 717 631 926 360 157 361 481\n",
      " 798 769  93 500 675  45  54 694 759 143 733 716 266 856  82 342 556]\n",
      "Test:  [538 380  19 753 329 136 328   0 902 611 592 859 265 911 339 666 505 744\n",
      " 577 233 741 807 445 617 375 486 264 916 830 502 730 160  65 862 644 791\n",
      " 242 127  36  27 311 458 784 428 789 823 642 151  37 819 786 292 120 291\n",
      " 625 607 841 602 258 113 773 238 111 372 421 280 765 548 386 357  56 827\n",
      " 811 115 410 560 737 575 270 474 163 373 537 768  61 614   4 218 275 853\n",
      " 620 108 503 767 336 383 213 653 182 571 679 539  26 300 309 855  17 493\n",
      " 178 369  73 460  80 551 351 630 848 105 215  58  79 241 596 849 824 682\n",
      " 346 881 418 664 948 521 869 643 427  75 454 820 153 392 496 770 766 448\n",
      " 915 288  38 755 464 327 646 912 262 854 747  69 813 267 900 665 558 470\n",
      " 585 835 689 724 391 740 228 378 453 326 873  57 746 435 723  74 271 624\n",
      " 162  21 708 626 206 568 754 498 426 283 668 482 816 237 919 340 756 507\n",
      " 943 188 314 936  84 927 349 211 893   6 377 632 164 476 917 495 729 878\n",
      " 385 306 144 674 570 832 414 895  39 882 790 802 253 461  49 908 605  33\n",
      " 846  23 303 826 866]\n",
      "Train: [950 609 148 114 331 693 683 501 196 116   1 817 947 388 485 353 259 576\n",
      " 298 559 850 509 514  89 743 627 751 815 413 710 400 429 942 415 112  88\n",
      " 128   8 601 932 955 382 512 296 884 655 198 590 852 589 439 240 845 527\n",
      " 868 185 145 584 696 335 104 761 186 146 350 731 634 564 168 707   2 801\n",
      " 332 903  55  31 229 131 734 363 910 207 462 223  52 420 760 805 545 929\n",
      " 408  70 907  50 906 467 129 757 587 598 922 230 886 669 864   9 688 491\n",
      " 739 579 621 139 452 516 871 794 840 191  81 167 465 135 365 359  41 555\n",
      " 443 425 597 204 641 857 540 121 364 528  91 712 839 563 323 480 381 179\n",
      " 417 316   7 479  76  94 220 247 896  51 612 469 338 810 249 180 286 772\n",
      " 263 522 222 905 404 721 473 781  14 672 442 315 504  67  95 748  42 322\n",
      " 738 804 193  71 269 370 354 680 923 618 281 660 416 487 659 197 715 324\n",
      " 312 595 225 246 310 394 720 523 640 122 726 255 865 513 224 287 702 578\n",
      " 285 879 880 924 728 935 440 295 764 535 583 261 119 690 124  90 313 205\n",
      " 725 183 736 401 722 941 780 795 109 333 639 419 279 341 691 305 444 471\n",
      " 888 732 447 623 658 299 863 165 692 254 954 423 847  11 293 553  59 800\n",
      " 174 302 684 861 194 268 887 610 216 889 226 775 562 477 796 544 713  12\n",
      " 366 325 914 628 777 536 497 457 508 714 356 651 885 645 176 510 600 449\n",
      " 842 438 446 337 455 463 638 931 397 150 334 829  25 803  83  35 956  78\n",
      " 284  62 892 227 217 913 192 792 142 566 170 593 825 676 152 637 199 921\n",
      "  68 615 208 603  92 875 517 677 534 202  97 488 412 573 345 374 395 619\n",
      " 877 654 550 719 411 650 837 166 277 125 872 102 673 933 953 561 582 221\n",
      " 118 818 586 519 138 276 155 567 647  34 106 297 904 870 667  72 525  13\n",
      " 704 898 843 244 344 126 158 405 524 484  22 533 662 347 123 110 687 101\n",
      " 572 450 695 709 788  60 219 343 661 699 678 821 580  28 945 938 107 549\n",
      " 838 173 809 456  43 434 156 393 368 750 890 239 379   3 718 797 184 901\n",
      " 172 422 330 348 518  86 362 944 599 307 290 526 190 209 883  87 918 894\n",
      " 424 899 633 808 806 591 352 132 272 256 538 380  19 753 329 136 328   0\n",
      " 902 611 592 859 265 911 339 666 505 744 577 233 741 807 445 617 375 486\n",
      " 264 916 830 502 730 160  65 862 644 791 242 127  36  27 311 458 784 428\n",
      " 789 823 642 151  37 819 786 292 120 291 625 607 841 602 258 113 773 238\n",
      " 111 372 421 280 765 548 386 357  56 827 811 115 410 560 737 575 270 474\n",
      " 163 373 537 768  61 614   4 218 275 853 620 108 503 767 336 383 213 653\n",
      " 182 571 679 539  26 300 309 855  17 493 178 369  73 460  80 551 351 630\n",
      " 848 105 215  58  79 241 596 849 824 682 346 881 418 664 948 521 869 643\n",
      " 427  75 454 820 153 392 496 770 766 448 915 288  38 755 464 327 646 912\n",
      " 262 854 747  69 813 267 900 665 558 470 585 835 689 724 391 740 228 378\n",
      " 453 326 873  57 746 435 723  74 271 624 162  21 708 626 206 568 754 498\n",
      " 426 283 668 482 816 237 919 340 756 507 943 188 314 936  84 927 349 211\n",
      " 893   6 377 632 164 476 917 495 729 878 385 306 144 674 570 832 414 895\n",
      "  39 882 790 802 253 461  49 908 605  33 846  23 303 826 866 342 556]\n",
      "Test:  [822 574 409 506 355 925 376 169 432 701  16 278 930 812 909 389 604 387\n",
      "  40 399 251 274 245 231 928 154  15 799 252 831 554 635 441 304 703  10\n",
      "  32  18 532 542 613 396  85 629 398 616 787 531 957 745 390  98 706 468\n",
      " 606  29 833 752  53 248 321 652 367 698 934 763 100 608 939  63 546 622\n",
      " 175  30 530 670 406  66 130 844 317   5 232 656  96  64 234 946 742 552\n",
      " 705 149 294 951 431 762 697 490 785 711 681 371 236 793 758 648 478 860\n",
      " 686 547 181 133 430 301 700 952 771 779 663  24 774 511 141 437 920 649\n",
      " 836 433 581 594 436 235 858 520  20 212 117 891 187 257 403 320 140 515\n",
      " 319 828 203 459 671 171 541 685 402 588 200 727 137 636 778 189 483 735\n",
      " 565 250 937 543 214 489 147 243 289 273  48 834 949 103  46 782 308 557\n",
      " 657  77  99 407 159 195 529 776 318 814 451 475 940 161 134 358 867 472\n",
      " 874 384 260 876 851 749 177 492  47 466 282 210 499 201 783 569 494  44\n",
      " 897 717 631 926 360 157 361 481 798 769  93 500 675  45  54 694 759 143\n",
      " 733 716 266 856  82]\n"
     ]
    }
   ],
   "source": [
    "validacion_simple = ValidacionSimple(0.75)\n",
    "validacion_simple.creaParticiones(dataset)\n",
    "v_cruzada = ValidacionCruzada(4)\n",
    "v_cruzada.creaParticiones(dataset)\n",
    "\n",
    "print(\"VALIDACION SIMPLE\",validacion_simple.particiones[0])\n",
    "\n",
    "print(\"VALIDACION CRUZADA\")\n",
    "for particion in v_cruzada.particiones:\n",
    "    print(particion)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El error del Clasificador NaiveBayes  para Validacion Simple es: 0.6583333333333333\n",
      "El error del Clasificador NaiveBayes para Validacion Cruzada es: 0.6534257322175732\n"
     ]
    }
   ],
   "source": [
    "clasificador = ClasificadorNaiveBayes(True)\n",
    "#clasificador.entrenamiento(dataset, validacion_simple.particiones[0].indicesTrain)\n",
    "#clasificador.clasifica(dataset, validacion_simple.particiones[0].indicesTest)\n",
    "error = clasificador.validacion(validacion_simple,dataset,clasificador)\n",
    "error1 = clasificador.validacion(v_cruzada,dataset,clasificador)\n",
    "print(\"El error del Clasificador NaiveBayes  para Validacion Simple es: \"+str(error))\n",
    "print(\"El error del Clasificador NaiveBayes para Validacion Cruzada es: \"+str(error1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A1', 'A2', 'A3', 'A4', 'A5', 'A6', 'A7', 'A8', 'A9', 'A10', 'A11', 'A12', 'A13', 'A14', 'A15', 'A16', 'A17', 'A18', 'A19', 'A20', 'Class']\n",
      "[{'A11': 0, 'A12': 1, 'A13': 2, 'A14': 3}, {}, {'A30': 0, 'A31': 1, 'A32': 2, 'A33': 3, 'A34': 4}, {'A40': 0, 'A41': 1, 'A410': 2, 'A42': 3, 'A43': 4, 'A44': 5, 'A45': 6, 'A46': 7, 'A48': 8, 'A49': 9}, {}, {'A61': 0, 'A62': 1, 'A63': 2, 'A64': 3, 'A65': 4}, {'A71': 0, 'A72': 1, 'A73': 2, 'A74': 3, 'A75': 4}, {}, {'A91': 0, 'A92': 1, 'A93': 2, 'A94': 3}, {'A101': 0, 'A102': 1, 'A103': 2}, {}, {'A121': 0, 'A122': 1, 'A123': 2, 'A124': 3}, {}, {'A141': 0, 'A142': 1, 'A143': 2}, {'A151': 0, 'A152': 1, 'A153': 2}, {}, {'A171': 0, 'A172': 1, 'A173': 2, 'A174': 3}, {}, {'A191': 0, 'A192': 1}, {'A201': 0, 'A202': 1}, {'1': 0, '2': 1}]\n",
      "[[ 0.  6.  4. ...  1.  0.  0.]\n",
      " [ 1. 48.  2. ...  0.  0.  1.]\n",
      " [ 3. 12.  4. ...  0.  0.  0.]\n",
      " ...\n",
      " [ 3. 12.  2. ...  0.  0.  0.]\n",
      " [ 0. 45.  2. ...  1.  0.  1.]\n",
      " [ 1. 45.  4. ...  0.  0.  0.]]\n",
      "[True, False, True, True, False, True, True, False, True, True, False, True, False, True, True, False, True, False, True, True, True]\n"
     ]
    }
   ],
   "source": [
    "dataset = Datos('german.data')\n",
    "print(dataset.datos)\n",
    "print(dataset.nominalAtributos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VALIDACION SIMPLE Train: [286 363 976 946 215 648 356 632 936 113 248 384 106 383 709 958 952 483\n",
      "  30 209  27 739 723 104 524 289  22 969 649 807 766 737  41 774 746 537\n",
      " 831 430 697 987 624  95  23 315  16 761 550 718 525 518 922 710 139 675\n",
      " 111 736 777 488 487 154 912 876 210  10 138  13 583  86 277 713 181 872\n",
      " 120  56 473 916 466 674 461 750 653 480 192 695 174 651 378 862 848  89\n",
      "  60 825 386 569  62 532 874 763 422 257 692 571 197 190 638  25 178 547\n",
      " 307 741 457 475 541 942 219 860 711 443 837 141 539 792 527 453 933 368\n",
      "  14 147 968 431 464 371 426 246 606 870 953 633 826 281 344 556 167 612\n",
      " 793 887 749 622 913 463 852 978 997 745 585 511 879 621 691 716 896  53\n",
      " 785 365 419 157 882  48 298 451 109 489 884 251 565 411 346 983 602 495\n",
      "   5 406 748 714 944 735 352 999 468 706 222 374 514 963 108 868 904 221\n",
      " 574 490 211 255 821  49 889 254  28 469 630 364 439 935 545 670 762  47\n",
      " 280 682 437 316 666 149 742 449  75 859  81 809 854 689 820 305 819 398\n",
      " 945 900 216 857 107 961 584 782 618 493 940 628 769 986 579 223 562  20\n",
      " 720 231 619 790  90 995 372 435 917 329 905  84 690 150 268  44 459 786\n",
      " 699 117 421  51 119  59 250 452 768 984 928 698 971 393 163 789 972 245\n",
      "  67 853 114 808 728 288 705 891 877 795 176 337 552   6 397 531  66 607\n",
      " 494  73 340 897 824  85 115  82 771 408 520 921 179 667 290 719  57 278\n",
      " 403 171 203 676 358 799 204 794 631 267  17 582 613 300 263 342 593 898\n",
      " 669 627 380 361 472 770 395 351 903 544 528 143 131 939 400 529 385 620\n",
      " 733 781   0 226 405 458 693 172 843 275 605  40 764 640 517 183 194 580\n",
      "   7 636 775 977 725 270  78  97 321 369 772 123 743 568 575 810 470 626\n",
      " 925 132 923 721 993 413 133 950 701 911  99 355 146 985 373 856 188 394\n",
      " 573 822 642 994 328 811 379 432 804 899 313 173 864 817 572 800 927 187\n",
      " 752 125 867 292  68 673 310  33 166 595 134 233 855 894 910 788 476 297\n",
      " 279 128 581 554 773 685 227 592 576 915 225 753 724 389 357 778 650 610\n",
      " 681 142  52 318 327 625 479 234 841 805 540 702 220 960  39 988 635 509\n",
      " 262 343 758 615 442 465 578 596 272 678 387 829 577 533 880 803 754 703\n",
      " 448 842 604 240  65 484  32 259 918 410 237 639 161 601 418 213 542 932\n",
      " 136 382 260  38 516   8 434  29 407 100  54 830 553 521   2 496 823 103\n",
      " 847 185 235 184 957 840 390 377 973 156 871 895 162 427 186  92 101 218\n",
      "  18 441 732 780 814 561 846 655   1 266 338  83 534 558 243 228 866 683\n",
      " 212 311 704 964 975 241  69 500 608 647 991 360 646 331 291 838 308 412\n",
      " 193 152 349 979 839 813 326   3 110  61 543 791 663 334 816 815 806 548\n",
      "  37 934 717 594 536 295  98 629  88 564  35 124 332 798 570 566 121 140\n",
      " 312 207 116 980 370 677  43 482 658 924 302  12 456 137 165 303 481 417\n",
      " 189 196 906 301 919 205 285 428 687 660 309 914 965 893 609 175 722 756\n",
      " 341 347 643 474 271 530 849 512 296 425 325 224 616 696 388 195  79  63\n",
      " 467 501 299 269 256 679 232 450 611 440 930  72 505  74 873 901 959 850\n",
      " 747 729 833 656 261 404 865 645 230 668 433 637 206 598 229  11 802  87\n",
      " 967 180 158 414 617 591 247  80 191 401 503 314 160 375 283 409 947 707\n",
      " 734 844 317 892 471 535 603 551 499 444 949 589]\n",
      "Test:  [886 416 851 765 783 818 526 740  31 366 354 943 751 731 282 760 201 738\n",
      " 730 863 966 926 155 252 657  93 652 938 974 447 662 563 274 726 586 902\n",
      " 908 954 265 996 515 492 684 198 712 324 486 834 787 208  50 276 755  91\n",
      " 504 239 546  55 498 981 238 560 671 614 129  19 989 367   4 508 253 244\n",
      " 888 587 502 214 845 491 881 330 641 715 948 623 339 801 832 931 399 323\n",
      " 970  94 127  21 454 890 869 776 555 513 937 293 148 982 306  76 112 168\n",
      " 353 909 249 151 836 320 159 600 322 381 759 105 827 335 170 883 177 956\n",
      " 567 336 199 672 757 523  64 462 455 130 392  96 258 153 796 497 708 144\n",
      " 200  58 446 998 362 549  77 634 135 659 878 779  45 319 420 835 164  34\n",
      " 929 828 169 744 507 990 287 126 294 217 992 423 559 391 202 688 478 644\n",
      " 962 588 122 920 145 875 861 941  70 686 436 885 396 680  42 460 784 348\n",
      " 477 858 599 522  26 429 727 445 273 402 284 700 485 955  24  46 597 415\n",
      " 333 694 590 538 665 951 519 182   9 812 376 510 304 661 242 557 102 664\n",
      " 236 654 424 350 506 907 438 797  36 767 264  15 359 118 345  71]\n",
      "VALIDACION CRUZADA\n",
      "Train: [ 47 840 322 596 664 279 642 896 863 141 982 525 133 618 452 824 979 578\n",
      " 166 516  16 776 441 482 402 302 101 362 325 918 264 708 635 105 682 668\n",
      " 200 413 428 785 479 986  52 628 527 905 502 390  99 865 429 796 788 122\n",
      " 508 632 114 337 389  25 308 804 872 282 597 500 967 757 108 510 497 953\n",
      " 650 176 172 859 661 246  67 659 738 146 170 415 567 648 241 130 911  82\n",
      " 756 120 109 990 453 117 270 764 928 467 495 268 813 208  53 367 730  46\n",
      "  96 304 129 585 305  81 545 582 498 336 161 480 612  74 526 223 890 587\n",
      " 514 548 629 725 839  15 741 319 607  97 561 320 154 891 144 744 151 121\n",
      " 658 866  21 247 893 983 763 951 927 687 970 954  12 752 802 771  89 645\n",
      " 245 225 255 901 504 537 493 751 931   4 132 823 920 846 289 712 677 351\n",
      "  23 956 887 793 915 847 755 812 714 466 158 187 392 258  18 600 243 439\n",
      " 550 705 209 958 984 923 475 630 609 377 674 886 499 424 599 235 874 274\n",
      " 216 341  73  50 786 975  35 517 544 524 622 803 213  75 373   7 260  85\n",
      "   0 761 256 365 773  24 879 312 817 464 679  71 819 963 985 239 372 692\n",
      "  40 476 766 633 487 165 111 507 580  49 613 347 921 386 521 562 162 180\n",
      "  19 670 414 941 758 125 481 328 430 728 621 361 513 418 140 695 267 422\n",
      " 327 349 595 233   6 794 605  66 969 488 398 181 849 126 147  28 932 880\n",
      " 721 449 554 486 448 867 420 145 657 195 724 950 762 416 426 103 297 190\n",
      " 707  13 434 939 197 248  30 655 293 383  98 573 254 509  56 403 791 335\n",
      " 237 427 746 462 385 559 759 566 404 294 310  87 283 778 281 307 826  11\n",
      " 676 570 138 115 186 665 355 128 551 369 991  51 964 106 454 557 116 436\n",
      " 296  29 743  31 344 740  94 718 458 799 765 156 591  44  32 957  76 981\n",
      " 906 576 152 269 326 401 717 821 856 460 539 770 455 442 271 196 324  27\n",
      " 703 827 995 731 359 807 702 857 540 352 251   2 753  93 444 916 892 801\n",
      " 440 242 330 110 935 829 945 445   1 137 640 211 690 910  69 583 543 174\n",
      " 491 948 227 358 431 421  20 653 735 394 409 370 577 419 675 266 342 204\n",
      " 549   5 163 119 977 505 531 534 925 795 275 760  57 978 104 694 378 299\n",
      " 459 808 364 833 638 232 142 584 936 150 136 443 845 647 883 903  91 380\n",
      " 864 568 153  26 870 617 602 375 368  43 854 750 530 701 716 432 875 729\n",
      " 112 792 720 529 226 478 907 315 686 998 754 198 783 300 188 749 844 437\n",
      " 625 881 837 962 435 897 929 366 727 338 298 388 722 904 726 382 805 207\n",
      " 381 818 858 218 363 830 626 553 118 882 820 221  38 930 131 185 321 555\n",
      " 485 199 371 175 594 292   3 579 919 683 178 742 417 148 556 619 789 593\n",
      " 777 592 333 889 688 912 290 350  92  36 457 408 908 287 522 606 944 262\n",
      " 552  58 769 768 490 715 989 937 748 700 836 965 988 782 745 238 876 533\n",
      " 334 203 784 894 183 641 994 244 346 634 272 306 339  72 425 996 825 217\n",
      " 662  14 406 684 182 604 660 329 219 733 603 971 160 309 976 902 966 542\n",
      " 926 447 501 356 354 623 149 323 379 473 909 465 471 252 532 615 143 586\n",
      " 697 214 222 215 868 331 192 913  59 396  83 960 164  80 841 747 815 259\n",
      " 288 842 673 652 450 693 938 535 470 236  70 811 484 689 340 139  10 201\n",
      " 917 873 171 781  62 698 519 189 173  86 649  54 737 852 314  65 968 134\n",
      " 520 736 451 814 611 790 374 972 316 546 838 590]\n",
      "Test:  [980  37 732  77  41 317 899 780 234 184 885 680 135  64 489 774 410 564\n",
      " 518 438 563 224 179 831 997 228 992 798  63 851 423 503 631 205 477 860\n",
      " 446 571 357 397 961  90 572 772 343 949 230 646  78 706 496 393 973 286\n",
      " 313 511   9 411 506 569 828 240 202 194 332 276 637 734 360 391 843 492\n",
      " 946 461 113  88 433 295 574 614 800 878 278 127 565 395 643 231 797 191\n",
      " 947  68 855 656 512 952 376 627  39 999 168 671 400 639 528 835 123 177\n",
      " 636 654  34 523 624 558 900 861 723 895 273 877 669 767 806 387 250 869\n",
      " 678 318 353 468 713 598 167 206   8  17 345 672 263 699 787 515 412 993\n",
      " 541 610 616 987  42 832 934 853 474  33  60 779 974 696 871 210 663 810\n",
      " 775 581 704 667 816 257 311 469 405 285 348 107 862 914 220 280 102 301\n",
      " 691 193 848 620 719 384 538 710  61  95 588 644 681 933  55 212 456 834\n",
      " 265 483 253 229 407 955 959 159 709 169 601 898 850 155 809 291  45 942\n",
      " 494 739 922 560 608 940 124 651 884 924 711 822 277 157 261  84 399 589\n",
      " 249 100  79  48 685 536 666 463 943 575 284  22 472 547 303 888]\n",
      "Train: [980  37 732  77  41 317 899 780 234 184 885 680 135  64 489 774 410 564\n",
      " 518 438 563 224 179 831 997 228 992 798  63 851 423 503 631 205 477 860\n",
      " 446 571 357 397 961  90 572 772 343 949 230 646  78 706 496 393 973 286\n",
      " 313 511   9 411 506 569 828 240 202 194 332 276 637 734 360 391 843 492\n",
      " 946 461 113  88 433 295 574 614 800 878 278 127 565 395 643 231 797 191\n",
      " 947  68 855 656 512 952 376 627  39 999 168 671 400 639 528 835 123 177\n",
      " 636 654  34 523 624 558 900 861 723 895 273 877 669 767 806 387 250 869\n",
      " 678 318 353 468 713 598 167 206   8  17 345 672 263 699 787 515 412 993\n",
      " 541 610 616 987  42 832 934 853 474  33  60 779 974 696 871 210 663 810\n",
      " 775 581 704 667 816 257 311 469 405 285 348 107 862 914 220 280 102 301\n",
      " 691 193 848 620 719 384 538 710  61  95 588 644 681 933  55 212 456 834\n",
      " 265 483 253 229 407 955 959 159 709 169 601 898 850 155 809 291  45 942\n",
      " 494 739 922 560 608 940 124 651 884 924 711 822 277 157 261  84 399 589\n",
      " 249 100  79  48 685 536 666 463 943 575 284  22 472 547 303 888 372 692\n",
      "  40 476 766 633 487 165 111 507 580  49 613 347 921 386 521 562 162 180\n",
      "  19 670 414 941 758 125 481 328 430 728 621 361 513 418 140 695 267 422\n",
      " 327 349 595 233   6 794 605  66 969 488 398 181 849 126 147  28 932 880\n",
      " 721 449 554 486 448 867 420 145 657 195 724 950 762 416 426 103 297 190\n",
      " 707  13 434 939 197 248  30 655 293 383  98 573 254 509  56 403 791 335\n",
      " 237 427 746 462 385 559 759 566 404 294 310  87 283 778 281 307 826  11\n",
      " 676 570 138 115 186 665 355 128 551 369 991  51 964 106 454 557 116 436\n",
      " 296  29 743  31 344 740  94 718 458 799 765 156 591  44  32 957  76 981\n",
      " 906 576 152 269 326 401 717 821 856 460 539 770 455 442 271 196 324  27\n",
      " 703 827 995 731 359 807 702 857 540 352 251   2 753  93 444 916 892 801\n",
      " 440 242 330 110 935 829 945 445   1 137 640 211 690 910  69 583 543 174\n",
      " 491 948 227 358 431 421  20 653 735 394 409 370 577 419 675 266 342 204\n",
      " 549   5 163 119 977 505 531 534 925 795 275 760  57 978 104 694 378 299\n",
      " 459 808 364 833 638 232 142 584 936 150 136 443 845 647 883 903  91 380\n",
      " 864 568 153  26 870 617 602 375 368  43 854 750 530 701 716 432 875 729\n",
      " 112 792 720 529 226 478 907 315 686 998 754 198 783 300 188 749 844 437\n",
      " 625 881 837 962 435 897 929 366 727 338 298 388 722 904 726 382 805 207\n",
      " 381 818 858 218 363 830 626 553 118 882 820 221  38 930 131 185 321 555\n",
      " 485 199 371 175 594 292   3 579 919 683 178 742 417 148 556 619 789 593\n",
      " 777 592 333 889 688 912 290 350  92  36 457 408 908 287 522 606 944 262\n",
      " 552  58 769 768 490 715 989 937 748 700 836 965 988 782 745 238 876 533\n",
      " 334 203 784 894 183 641 994 244 346 634 272 306 339  72 425 996 825 217\n",
      " 662  14 406 684 182 604 660 329 219 733 603 971 160 309 976 902 966 542\n",
      " 926 447 501 356 354 623 149 323 379 473 909 465 471 252 532 615 143 586\n",
      " 697 214 222 215 868 331 192 913  59 396  83 960 164  80 841 747 815 259\n",
      " 288 842 673 652 450 693 938 535 470 236  70 811 484 689 340 139  10 201\n",
      " 917 873 171 781  62 698 519 189 173  86 649  54 737 852 314  65 968 134\n",
      " 520 736 451 814 611 790 374 972 316 546 838 590]\n",
      "Test:  [ 47 840 322 596 664 279 642 896 863 141 982 525 133 618 452 824 979 578\n",
      " 166 516  16 776 441 482 402 302 101 362 325 918 264 708 635 105 682 668\n",
      " 200 413 428 785 479 986  52 628 527 905 502 390  99 865 429 796 788 122\n",
      " 508 632 114 337 389  25 308 804 872 282 597 500 967 757 108 510 497 953\n",
      " 650 176 172 859 661 246  67 659 738 146 170 415 567 648 241 130 911  82\n",
      " 756 120 109 990 453 117 270 764 928 467 495 268 813 208  53 367 730  46\n",
      "  96 304 129 585 305  81 545 582 498 336 161 480 612  74 526 223 890 587\n",
      " 514 548 629 725 839  15 741 319 607  97 561 320 154 891 144 744 151 121\n",
      " 658 866  21 247 893 983 763 951 927 687 970 954  12 752 802 771  89 645\n",
      " 245 225 255 901 504 537 493 751 931   4 132 823 920 846 289 712 677 351\n",
      "  23 956 887 793 915 847 755 812 714 466 158 187 392 258  18 600 243 439\n",
      " 550 705 209 958 984 923 475 630 609 377 674 886 499 424 599 235 874 274\n",
      " 216 341  73  50 786 975  35 517 544 524 622 803 213  75 373   7 260  85\n",
      "   0 761 256 365 773  24 879 312 817 464 679  71 819 963 985 239]\n",
      "Train: [980  37 732  77  41 317 899 780 234 184 885 680 135  64 489 774 410 564\n",
      " 518 438 563 224 179 831 997 228 992 798  63 851 423 503 631 205 477 860\n",
      " 446 571 357 397 961  90 572 772 343 949 230 646  78 706 496 393 973 286\n",
      " 313 511   9 411 506 569 828 240 202 194 332 276 637 734 360 391 843 492\n",
      " 946 461 113  88 433 295 574 614 800 878 278 127 565 395 643 231 797 191\n",
      " 947  68 855 656 512 952 376 627  39 999 168 671 400 639 528 835 123 177\n",
      " 636 654  34 523 624 558 900 861 723 895 273 877 669 767 806 387 250 869\n",
      " 678 318 353 468 713 598 167 206   8  17 345 672 263 699 787 515 412 993\n",
      " 541 610 616 987  42 832 934 853 474  33  60 779 974 696 871 210 663 810\n",
      " 775 581 704 667 816 257 311 469 405 285 348 107 862 914 220 280 102 301\n",
      " 691 193 848 620 719 384 538 710  61  95 588 644 681 933  55 212 456 834\n",
      " 265 483 253 229 407 955 959 159 709 169 601 898 850 155 809 291  45 942\n",
      " 494 739 922 560 608 940 124 651 884 924 711 822 277 157 261  84 399 589\n",
      " 249 100  79  48 685 536 666 463 943 575 284  22 472 547 303 888  47 840\n",
      " 322 596 664 279 642 896 863 141 982 525 133 618 452 824 979 578 166 516\n",
      "  16 776 441 482 402 302 101 362 325 918 264 708 635 105 682 668 200 413\n",
      " 428 785 479 986  52 628 527 905 502 390  99 865 429 796 788 122 508 632\n",
      " 114 337 389  25 308 804 872 282 597 500 967 757 108 510 497 953 650 176\n",
      " 172 859 661 246  67 659 738 146 170 415 567 648 241 130 911  82 756 120\n",
      " 109 990 453 117 270 764 928 467 495 268 813 208  53 367 730  46  96 304\n",
      " 129 585 305  81 545 582 498 336 161 480 612  74 526 223 890 587 514 548\n",
      " 629 725 839  15 741 319 607  97 561 320 154 891 144 744 151 121 658 866\n",
      "  21 247 893 983 763 951 927 687 970 954  12 752 802 771  89 645 245 225\n",
      " 255 901 504 537 493 751 931   4 132 823 920 846 289 712 677 351  23 956\n",
      " 887 793 915 847 755 812 714 466 158 187 392 258  18 600 243 439 550 705\n",
      " 209 958 984 923 475 630 609 377 674 886 499 424 599 235 874 274 216 341\n",
      "  73  50 786 975  35 517 544 524 622 803 213  75 373   7 260  85   0 761\n",
      " 256 365 773  24 879 312 817 464 679  71 819 963 985 239 883 903  91 380\n",
      " 864 568 153  26 870 617 602 375 368  43 854 750 530 701 716 432 875 729\n",
      " 112 792 720 529 226 478 907 315 686 998 754 198 783 300 188 749 844 437\n",
      " 625 881 837 962 435 897 929 366 727 338 298 388 722 904 726 382 805 207\n",
      " 381 818 858 218 363 830 626 553 118 882 820 221  38 930 131 185 321 555\n",
      " 485 199 371 175 594 292   3 579 919 683 178 742 417 148 556 619 789 593\n",
      " 777 592 333 889 688 912 290 350  92  36 457 408 908 287 522 606 944 262\n",
      " 552  58 769 768 490 715 989 937 748 700 836 965 988 782 745 238 876 533\n",
      " 334 203 784 894 183 641 994 244 346 634 272 306 339  72 425 996 825 217\n",
      " 662  14 406 684 182 604 660 329 219 733 603 971 160 309 976 902 966 542\n",
      " 926 447 501 356 354 623 149 323 379 473 909 465 471 252 532 615 143 586\n",
      " 697 214 222 215 868 331 192 913  59 396  83 960 164  80 841 747 815 259\n",
      " 288 842 673 652 450 693 938 535 470 236  70 811 484 689 340 139  10 201\n",
      " 917 873 171 781  62 698 519 189 173  86 649  54 737 852 314  65 968 134\n",
      " 520 736 451 814 611 790 374 972 316 546 838 590]\n",
      "Test:  [372 692  40 476 766 633 487 165 111 507 580  49 613 347 921 386 521 562\n",
      " 162 180  19 670 414 941 758 125 481 328 430 728 621 361 513 418 140 695\n",
      " 267 422 327 349 595 233   6 794 605  66 969 488 398 181 849 126 147  28\n",
      " 932 880 721 449 554 486 448 867 420 145 657 195 724 950 762 416 426 103\n",
      " 297 190 707  13 434 939 197 248  30 655 293 383  98 573 254 509  56 403\n",
      " 791 335 237 427 746 462 385 559 759 566 404 294 310  87 283 778 281 307\n",
      " 826  11 676 570 138 115 186 665 355 128 551 369 991  51 964 106 454 557\n",
      " 116 436 296  29 743  31 344 740  94 718 458 799 765 156 591  44  32 957\n",
      "  76 981 906 576 152 269 326 401 717 821 856 460 539 770 455 442 271 196\n",
      " 324  27 703 827 995 731 359 807 702 857 540 352 251   2 753  93 444 916\n",
      " 892 801 440 242 330 110 935 829 945 445   1 137 640 211 690 910  69 583\n",
      " 543 174 491 948 227 358 431 421  20 653 735 394 409 370 577 419 675 266\n",
      " 342 204 549   5 163 119 977 505 531 534 925 795 275 760  57 978 104 694\n",
      " 378 299 459 808 364 833 638 232 142 584 936 150 136 443 845 647]\n",
      "Train: [980  37 732  77  41 317 899 780 234 184 885 680 135  64 489 774 410 564\n",
      " 518 438 563 224 179 831 997 228 992 798  63 851 423 503 631 205 477 860\n",
      " 446 571 357 397 961  90 572 772 343 949 230 646  78 706 496 393 973 286\n",
      " 313 511   9 411 506 569 828 240 202 194 332 276 637 734 360 391 843 492\n",
      " 946 461 113  88 433 295 574 614 800 878 278 127 565 395 643 231 797 191\n",
      " 947  68 855 656 512 952 376 627  39 999 168 671 400 639 528 835 123 177\n",
      " 636 654  34 523 624 558 900 861 723 895 273 877 669 767 806 387 250 869\n",
      " 678 318 353 468 713 598 167 206   8  17 345 672 263 699 787 515 412 993\n",
      " 541 610 616 987  42 832 934 853 474  33  60 779 974 696 871 210 663 810\n",
      " 775 581 704 667 816 257 311 469 405 285 348 107 862 914 220 280 102 301\n",
      " 691 193 848 620 719 384 538 710  61  95 588 644 681 933  55 212 456 834\n",
      " 265 483 253 229 407 955 959 159 709 169 601 898 850 155 809 291  45 942\n",
      " 494 739 922 560 608 940 124 651 884 924 711 822 277 157 261  84 399 589\n",
      " 249 100  79  48 685 536 666 463 943 575 284  22 472 547 303 888  47 840\n",
      " 322 596 664 279 642 896 863 141 982 525 133 618 452 824 979 578 166 516\n",
      "  16 776 441 482 402 302 101 362 325 918 264 708 635 105 682 668 200 413\n",
      " 428 785 479 986  52 628 527 905 502 390  99 865 429 796 788 122 508 632\n",
      " 114 337 389  25 308 804 872 282 597 500 967 757 108 510 497 953 650 176\n",
      " 172 859 661 246  67 659 738 146 170 415 567 648 241 130 911  82 756 120\n",
      " 109 990 453 117 270 764 928 467 495 268 813 208  53 367 730  46  96 304\n",
      " 129 585 305  81 545 582 498 336 161 480 612  74 526 223 890 587 514 548\n",
      " 629 725 839  15 741 319 607  97 561 320 154 891 144 744 151 121 658 866\n",
      "  21 247 893 983 763 951 927 687 970 954  12 752 802 771  89 645 245 225\n",
      " 255 901 504 537 493 751 931   4 132 823 920 846 289 712 677 351  23 956\n",
      " 887 793 915 847 755 812 714 466 158 187 392 258  18 600 243 439 550 705\n",
      " 209 958 984 923 475 630 609 377 674 886 499 424 599 235 874 274 216 341\n",
      "  73  50 786 975  35 517 544 524 622 803 213  75 373   7 260  85   0 761\n",
      " 256 365 773  24 879 312 817 464 679  71 819 963 985 239 372 692  40 476\n",
      " 766 633 487 165 111 507 580  49 613 347 921 386 521 562 162 180  19 670\n",
      " 414 941 758 125 481 328 430 728 621 361 513 418 140 695 267 422 327 349\n",
      " 595 233   6 794 605  66 969 488 398 181 849 126 147  28 932 880 721 449\n",
      " 554 486 448 867 420 145 657 195 724 950 762 416 426 103 297 190 707  13\n",
      " 434 939 197 248  30 655 293 383  98 573 254 509  56 403 791 335 237 427\n",
      " 746 462 385 559 759 566 404 294 310  87 283 778 281 307 826  11 676 570\n",
      " 138 115 186 665 355 128 551 369 991  51 964 106 454 557 116 436 296  29\n",
      " 743  31 344 740  94 718 458 799 765 156 591  44  32 957  76 981 906 576\n",
      " 152 269 326 401 717 821 856 460 539 770 455 442 271 196 324  27 703 827\n",
      " 995 731 359 807 702 857 540 352 251   2 753  93 444 916 892 801 440 242\n",
      " 330 110 935 829 945 445   1 137 640 211 690 910  69 583 543 174 491 948\n",
      " 227 358 431 421  20 653 735 394 409 370 577 419 675 266 342 204 549   5\n",
      " 163 119 977 505 531 534 925 795 275 760  57 978 104 694 378 299 459 808\n",
      " 364 833 638 232 142 584 936 150 136 443 845 647]\n",
      "Test:  [883 903  91 380 864 568 153  26 870 617 602 375 368  43 854 750 530 701\n",
      " 716 432 875 729 112 792 720 529 226 478 907 315 686 998 754 198 783 300\n",
      " 188 749 844 437 625 881 837 962 435 897 929 366 727 338 298 388 722 904\n",
      " 726 382 805 207 381 818 858 218 363 830 626 553 118 882 820 221  38 930\n",
      " 131 185 321 555 485 199 371 175 594 292   3 579 919 683 178 742 417 148\n",
      " 556 619 789 593 777 592 333 889 688 912 290 350  92  36 457 408 908 287\n",
      " 522 606 944 262 552  58 769 768 490 715 989 937 748 700 836 965 988 782\n",
      " 745 238 876 533 334 203 784 894 183 641 994 244 346 634 272 306 339  72\n",
      " 425 996 825 217 662  14 406 684 182 604 660 329 219 733 603 971 160 309\n",
      " 976 902 966 542 926 447 501 356 354 623 149 323 379 473 909 465 471 252\n",
      " 532 615 143 586 697 214 222 215 868 331 192 913  59 396  83 960 164  80\n",
      " 841 747 815 259 288 842 673 652 450 693 938 535 470 236  70 811 484 689\n",
      " 340 139  10 201 917 873 171 781  62 698 519 189 173  86 649  54 737 852\n",
      " 314  65 968 134 520 736 451 814 611 790 374 972 316 546 838 590]\n"
     ]
    }
   ],
   "source": [
    "validacion_simple = ValidacionSimple(0.75)\n",
    "validacion_simple.creaParticiones(dataset)\n",
    "v_cruzada = ValidacionCruzada(4)\n",
    "v_cruzada.creaParticiones(dataset)\n",
    "\n",
    "print(\"VALIDACION SIMPLE\",validacion_simple.particiones[0])\n",
    "\n",
    "print(\"VALIDACION CRUZADA\")\n",
    "for particion in v_cruzada.particiones:\n",
    "    print(particion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "only size-1 arrays can be converted to Python scalars",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-281-177411b8a88b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#clasificador.entrenamiento(dataset, validacion_simple.particiones[0].indicesTrain)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#clasificador.clasifica(dataset, validacion_simple.particiones[0].indicesTest)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0merror\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclasificador\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidacion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidacion_simple\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mclasificador\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0merror1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclasificador\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidacion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv_cruzada\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mclasificador\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"El error del Clasificador NaiveBayes  para Validacion Simple es: \"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-265-fc3769d437ab>\u001b[0m in \u001b[0;36mvalidacion\u001b[0;34m(self, particionado, dataset, clasificador, seed)\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparticionado\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparticiones\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mclasificador\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mentrenamiento\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparticionado\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparticiones\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindicesTrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m             \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclasificador\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasifica\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mparticionado\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparticiones\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindicesTest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextraeDatos\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparticionado\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparticiones\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindicesTest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-274-e7481a1cc3e1>\u001b[0m in \u001b[0;36mclasifica\u001b[0;34m(self, dataset, datosTest)\u001b[0m\n\u001b[1;32m    103\u001b[0m                         \u001b[0mexp2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdato\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mposteriori\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0matributo\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mclase\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mdato\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0matributo\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mposteriori\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0matributo\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mclase\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m                         \u001b[0mexp3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexp2\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m                         \u001b[0mexp4\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m \u001b[0mexp3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m                         \u001b[0maux\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexp1\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mexp4\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m                         \u001b[0mlistaVerosimilitudes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maux\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: only size-1 arrays can be converted to Python scalars"
     ]
    }
   ],
   "source": [
    "clasificador = ClasificadorNaiveBayes(True)\n",
    "#clasificador.entrenamiento(dataset, validacion_simple.particiones[0].indicesTrain)\n",
    "#clasificador.clasifica(dataset, validacion_simple.particiones[0].indicesTest)\n",
    "error = clasificador.validacion(validacion_simple,dataset,clasificador)\n",
    "error1 = clasificador.validacion(v_cruzada,dataset,clasificador)\n",
    "print(\"El error del Clasificador NaiveBayes  para Validacion Simple es: \"+str(error))\n",
    "print(\"El error del Clasificador NaiveBayes para Validacion Cruzada es: \"+str(error1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SKLearn Validacion Cruzada\n",
    "# Hacemos el encode de los datos\n",
    "encAtributos = preprocessing.OneHotEncoder(categorical_features=dataset.nominalAtributos[:-1],sparse=False)\n",
    "X = encAtributos.fit_transform(dataset.datos[:,:-1])\n",
    "Y = dataset.datos[:,-1]\n",
    "\n",
    "# Partimos los datos en Train y Test\n",
    "x_train, x_test, y_train,y_test = train_test_split(X,Y,test_size=0.2)\n",
    "\n",
    "# Aplicamos Naive Bayes sin Laplace\n",
    "clf = MultinomialNB(alpha = 0,fit_prior=True, class_prior=None)\n",
    "predicciones=clf.fit(x_train,y_train).predict(x_test)\n",
    "\n",
    "# Print de los resultados\n",
    "print(\"Predicciones utilizando validacion simple sin correcion de Laplace:\", predicciones)\n",
    "print(\"Clases reales de la particion de text\", y_test)\n",
    "\n",
    "# Aplicamos Naive Bayes con Laplace\n",
    "clf = MultinomialNB(alpha = 0,fit_prior=True, class_prior=None)\n",
    "predicciones=clf.fit(x_train,y_train).predict(x_test)\n",
    "\n",
    "# Print de los resultados\n",
    "print(\"Predicciones utilizando validacion simple con correcion de Laplace:\", predicciones)\n",
    "print(\"Clases reales de la particion de text\", y_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SKLearn Validacion Cruzada\n",
    "# Hacemos el encode de los datos\n",
    "encAtributos = preprocessing.OneHotEncoder(categorical_features=dataset.nominalAtributos[:-1],sparse=False)\n",
    "X = encAtributos.fit_transform(dataset.datos[:,:-1])\n",
    "Y = dataset.datos[:,-1]\n",
    "\n",
    "# Partimos los datos en Train y Test\n",
    "x_train, x_test, y_train,y_test = train_test_split(X,Y,test_size=0.2)\n",
    "\n",
    "# Aplicamos Naive Bayes sin Laplace\n",
    "clf = MultinomialNB(alpha = 0,fit_prior=True, class_prior=None)\n",
    "predicciones=clf.fit(x_train,y_train).predict(x_test)\n",
    "\n",
    "# Print de los resultados\n",
    "print(\"Predicciones utilizando validacion simple sin correcion de Laplace:\", predicciones)\n",
    "print(\"Clases reales de la particion de text\", y_test)\n",
    "\n",
    "# Aplicamos Naive Bayes con Laplace\n",
    "clf = MultinomialNB(alpha = 0,fit_prior=True, class_prior=None)\n",
    "predicciones=clf.fit(x_train,y_train).predict(x_test)\n",
    "\n",
    "# Print de los resultados\n",
    "print(\"Predicciones utilizando validacion simple con correcion de Laplace:\", predicciones)\n",
    "print(\"Clases reales de la particion de text\", y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
