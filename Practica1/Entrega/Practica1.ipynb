{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>PRÁCTICA 1 - FUNDAMENTOS DE APRENDIZAJE AUTOMÁTICO</h1>\n",
    "<h3>Realizada la práctica por:<br/>\n",
    "    <ol>\n",
    "    -Pablo Díez del Pozo<br/>\n",
    "    -Alejandro Alcalá Álvarez\n",
    "    </ol>\n",
    " </h3>\n",
    "<h3>Grupo: 1461</h3>\n",
    "<h3>Pareja: 01</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 932,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/python3\n",
    "# -*- coding: utf-8 -*-"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Importaciones necesarias para la ejecucion del código</h3>\n",
    "<p>Podemos observar todos los import necesarios que tenemos que realizar para que la ejecución de nuestro codigo funcione a la perfección, a continuación, explicaremos cada uno de los imports y para que son necesarios:</p>\n",
    "<ol>\n",
    "    <li>Random: se utiliza para hacer las secuencias de índices aleatorios para las particiones de entrenamiento y de clasificación.\n",
    "    <li>Math: se utiliza para hacer la distribución normal para los atributos que sean continuos y asi poder calcular su probabilidad.\n",
    "    <li>Numpy: Es la libreria mas utilizada en esta práctica, debido a que almacenamos los datos en una matriz numpy y guardamos las probabilidades posterioris de los atributos en un array de matrices de numpy.\n",
    "    <li>ABC: se utiliza para haces clases y métodos abstractos.\n",
    "    <li>Datos: se utiliza para importar toda la funcionalidad de nuestro modulo Datos.\n",
    "    <li>Collections: se utiliza para contabilizar las probabilidades condicionadas y para ver cuantas clases hay en el fichero\n",
    "    <li> SortedDict: se utiliza para ordenar el diccionario que creamos con las probabilidades a priori de cada clase\n",
    "    <li>Sklearn: se utiliza para hacer el tercer apartado de esta práctica, donde nos da una implementación del algoritmo de Naive-Bayes\n",
    "    <li>Pyplot: se utiliza en el último apartado de la práctica, donde nos da una implementación para pintar la curva ROC.\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 933,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "from abc import ABCMeta, abstractmethod\n",
    "from Datos import Datos\n",
    "from collections import Counter\n",
    "from sortedcontainers import SortedDict\n",
    "from sklearn.metrics import confusion_matrix, auc\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold, cross_val_predict\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from EstrategiaParticionado import Particion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Obtener los datos de los Distintos Dataset</h3>\n",
    "<p>Aqui vamos a poder observar como vamos a codificar los datos que nos dan en un fichero a una matriz Numpy para poder tratar los datos para poder entrenarlos y clasificarlos con Naive-Bayes</p>\n",
    "<p>Vamos a ver como llamando a la clase Datos y que en su constructor le ponemos la ruta del fichero se crea la matriz numpy de los datos, pero a demás de esa matriz también guardamos información necesaria para poder entrenarlos y clasificarlos correctamente. Por ejemplo, guardamos si los atributos son continuos o discretos.</p>\n",
    "<p>A continuación, vamos a mostrar una ejecución para cada uno de los conjuntos de datos que nos dan para hacer Naive-Bayes. En la celda de abajo vereis la ejecución.</p>\n",
    "<p> No se incluye el código de \"Datos.py\", ya que es el mismo que se utiliza en la P0.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 934,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============MATRIZ NUMPY DEL CONJUNTO DE DATOS LENSES=====================\n",
      "[[0. 0. 0. 0. 2.]\n",
      " [0. 0. 0. 1. 1.]\n",
      " [0. 0. 1. 0. 2.]\n",
      " [0. 0. 1. 1. 0.]\n",
      " [0. 1. 0. 0. 2.]\n",
      " [0. 1. 0. 1. 1.]\n",
      " [0. 1. 1. 0. 2.]\n",
      " [0. 1. 1. 1. 0.]\n",
      " [1. 0. 0. 0. 2.]\n",
      " [1. 0. 0. 1. 1.]\n",
      " [1. 0. 1. 0. 2.]\n",
      " [1. 0. 1. 1. 0.]\n",
      " [1. 1. 0. 0. 2.]\n",
      " [1. 1. 0. 1. 1.]\n",
      " [1. 1. 1. 0. 2.]\n",
      " [1. 1. 1. 1. 2.]\n",
      " [2. 0. 0. 0. 2.]\n",
      " [2. 0. 0. 1. 2.]\n",
      " [2. 0. 1. 0. 2.]\n",
      " [2. 0. 1. 1. 0.]\n",
      " [2. 1. 0. 0. 2.]\n",
      " [2. 1. 0. 1. 1.]\n",
      " [2. 1. 1. 0. 2.]\n",
      " [2. 1. 1. 1. 2.]]\n",
      "============================================================================\n"
     ]
    }
   ],
   "source": [
    "dataset1 = Datos('../Datasets/lenses.data')\n",
    "print(\"==============MATRIZ NUMPY DEL CONJUNTO DE DATOS LENSES=====================\")\n",
    "print(dataset.datos)\n",
    "print(\"============================================================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 935,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============MATRIZ NUMPY DEL CONJUNTO DE DATOS TIC-TAC-TOE================\n",
      "[[2. 2. 2. ... 1. 1. 1.]\n",
      " [2. 2. 2. ... 2. 1. 1.]\n",
      " [2. 2. 2. ... 1. 2. 1.]\n",
      " ...\n",
      " [1. 2. 1. ... 1. 2. 0.]\n",
      " [1. 2. 1. ... 1. 2. 0.]\n",
      " [1. 1. 2. ... 2. 2. 0.]]\n",
      "============================================================================\n"
     ]
    }
   ],
   "source": [
    "dataset2 = Datos('../Datasets/tic-tac-toe.data')\n",
    "print(\"==============MATRIZ NUMPY DEL CONJUNTO DE DATOS TIC-TAC-TOE================\")\n",
    "print(dataset2.datos)\n",
    "print(\"============================================================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 936,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============MATRIZ NUMPY DEL CONJUNTO DE DATOS GERMAN=====================\n",
      "[[ 0.  6.  4. ...  1.  0.  0.]\n",
      " [ 1. 48.  2. ...  0.  0.  1.]\n",
      " [ 3. 12.  4. ...  0.  0.  0.]\n",
      " ...\n",
      " [ 3. 12.  2. ...  0.  0.  0.]\n",
      " [ 0. 45.  2. ...  1.  0.  1.]\n",
      " [ 1. 45.  4. ...  0.  0.  0.]]\n",
      "============================================================================\n"
     ]
    }
   ],
   "source": [
    "dataset3 = Datos('../Datasets/german.data')\n",
    "print(\"==============MATRIZ NUMPY DEL CONJUNTO DE DATOS GERMAN=====================\")\n",
    "print(dataset3.datos)\n",
    "print(\"============================================================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Apartado 1: Estrategia de Particionado</h3>\n",
    "<p>En este apartado vamos a probar las dos estrategias de particionado de los datos que hemos tenido que implementar en esta práctica, las cuales son:</p>\n",
    "    <ol>\n",
    "        <p>- Validación Simple.</p>\n",
    "        <p>- Validación Cruzada.</p>\n",
    "    </ol>\n",
    "<p>Nuestra estrategia de <strong>validación simple</strong> consiste en meterle un porcentaje por el cual queremos dividir el conjunto de datos en dos subconjuntos de datos, donde uno lo vamos a utilizar para entrenar y el otro lo vamos a utilizar para hacer la predicción con nuestro clasificador. En la celda de abajo mostraremos el código necesario para poder realizar correctamente la validacion simple.</p>\n",
    "<p>Como podemos observar en el código de abajo de validación simple, lo que hacemos es que ponemos una semilla a random y decimos que el numero de particiones va a ser uno. A continuación, haremos un permutacion de numeros aleatorios entre el 0  y el número de datos que hay en el fichero. Por ultimo, lo que hacemos es que le creamos la partición que va a tener en su interior los dos subconjuntos de Train y Test. En esa permutación lo multiplicamos por el porcentaje que le hemos dado nosotros para crear los dos suboconjuntos.</p>\n",
    "<p>Debajo de esta celda vamos a comprobar en como funciona  la validación simple con diferentes porcentajes para obtener el subconjunto de datos de Train y Test, en los diferentes conjuntos de datos </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 937,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class Particion():\n",
    "\n",
    "  # Esta clase mantiene la lista de �ndices de Train y Test para cada partici�n del conjunto de particiones\n",
    "  def __init__(self, train=[], test=[]):\n",
    "    self.indicesTrain = train\n",
    "    self.indicesTest = test\n",
    "\n",
    "  def __str__(self):\n",
    "    return \"Train: {}\\nTest:  {}\".format(str(self.indicesTrain), str(self.indicesTest))\n",
    "\n",
    "class EstrategiaParticionado:\n",
    "  # Clase abstracta\n",
    "  __metaclass__ = ABCMeta\n",
    "\n",
    "  # Lista de las particiones\n",
    "  def __init__(self, nombre=\"\"):\n",
    "    self.nombreEstrategia = nombre\n",
    "    self.numeroParticiones = 0\n",
    "    self.particiones = []\n",
    "\n",
    "  # Atributos: deben rellenarse adecuadamente para cada estrategia concreta: nombreEstrategia, numeroParticiones, listaParticiones. Se pasan en el constructor\n",
    "\n",
    "  @abstractmethod\n",
    "  # TODO: esta funcion deben ser implementadas en cada estrategia concreta\n",
    "  def creaParticiones(self, datos, seed=None):\n",
    "    pass\n",
    "\n",
    "class ValidacionSimple(EstrategiaParticionado):\n",
    "\n",
    "  def __init__(self, porcentaje):\n",
    "    self.porcentaje = porcentaje\n",
    "    super().__init__(\"Validacion simple\")\n",
    "\n",
    "  # Crea particiones segun el metodo tradicional de division de los datos segun el porcentaje deseado.\n",
    "  # Devuelve una lista de particiones (clase Particion)\n",
    "  # TODO: implementar\n",
    "  def creaParticiones(self, datos, seed=None):\n",
    "    \n",
    "    np.random.seed(seed)\n",
    "    self.numeroParticiones = 1\n",
    "\n",
    "    # Generamos una lista con todos los números de datos aleatorios\n",
    "    indicesAleatorios = np.random.permutation(int(datos.numDatos))\n",
    "\n",
    "    # Creamos la particion, en funcion del porcentaje especificado\n",
    "    self.particiones = [Particion(indicesAleatorios[:int(datos.numDatos * self.porcentaje)],\n",
    "                                  indicesAleatorios[int(datos.numDatos * self.porcentaje):])]\n",
    "\n",
    "    return self.particiones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 938,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -- VALIDACIÓN SIMPLE CON LENSES DATA 70% -- \n",
      "Train: [23  9  8  0 15 18 10 21 16 19 14  3  5  7  4 22]\n",
      "Test:  [12 20  2 11 13 17  6  1]\n"
     ]
    }
   ],
   "source": [
    "print(\" -- VALIDACIÓN SIMPLE CON LENSES DATA 70% -- \")\n",
    "estrategia = ValidacionSimple(0.7)\n",
    "estrategia.creaParticiones(dataset1)\n",
    "print(estrategia.particiones[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 939,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -- VALIDACIÓN SIMPLE CON TIC-TAC-TOE DATA 80% -- \n",
      "Train: [173 142 907 512 752 956 133 565 312 808 313 529 584 381 395 815 477 685\n",
      " 178 602 116 203 299 479 912 932 919 370 235  53 389 519  11 575 526 820\n",
      "  76 205 305 334 285 244 884 295 470 923 652 593 260 623 627 483  28 913\n",
      " 933 268 698 204  27 613 761  25 842 166 356 922 499  73 650 362 524 603\n",
      " 405 682 174 355 280 228 445 156 545 347 219 282 443 749 642 102 751  67\n",
      "   4  90 331  40 484 687 446  77 592 789 462 247 941 710 909 436 534 583\n",
      " 544 643 876 269 864 390 138 949 847 439 935   6 672 766  91 918 391 134\n",
      " 848  13 691 233 457 251 407 732 329  49 563 508 944 240 704 460 472 764\n",
      "  69 791 667 170 514 293 521 191 695 629 834 438 348  93 254 676 469 806\n",
      " 754 571 498 180 504 274 574 568 151 635 888 493 755 373 825  81 350 926\n",
      " 794 618 164  87 306 826 550 874 291 231 937 799 377 346 659 435 411 803\n",
      " 807 262 220 790 179 954 535  94 624 492 928 904 382  61 152 728 863 889\n",
      " 886 647  18 408 399 202 182 322 838 678 898 787 212 590 419 924 851 196\n",
      "  83 509 715 850 500 609 683 188 264 827 481 217  19 734 587 122 487 658\n",
      " 750 952 120 655  39 402 895 733 557 387 209 425 653 157 776 649 130 573\n",
      " 140 931 713  22 549  20 369 321  64 739 793 674 415 748 308 902  58 242\n",
      " 515 936 511 777 819 852 681  65 608 899 686  60 358 288 539 468 832 836\n",
      " 694 505 319  10 542 149 490 194 378 155 393 688 914 742 229 747 570 225\n",
      " 625 158 893  41 849 616 192 543 588 756 788 410 104 266 246 372 576 862\n",
      " 578 906 383 841  48 882 137 689 569  45 256 527 184 332 441 737 185 853\n",
      " 579 177 792  38 559 778 651 161 349 657 816 942 187 660 845 673 679 414\n",
      " 413  35  36 475 921 582 136 163 270 359   2 309 828 449 654 404 877 330\n",
      " 429 538 757 767 548 844 341 796 437 637 703  85 392 172 397 661 442 380\n",
      " 132 811 770 232   1 354 227 714 801 617  75 701 771 541 630 798 491 781\n",
      " 605  50 489   9 769 868 200 900 292 736 581 873 553 294 417 315 167 320\n",
      " 705 236  79 300 943 371  47 386 763 510 497 117 317 147 917 289 857 951\n",
      "  88 572 113 633 210 818 712 141 706  37 934 881 741  89 427 665 111 910\n",
      " 118 199 365 440 453 566 364 663 176 927 537 342 528 333 303  30 416  42\n",
      " 473 597 947 375 903 639 162 452 555 901 638 467  34 905 208 148  92 357\n",
      " 190 671 797  68 890 257 883 206 168 290 252 466 207 920  23 394 374 238\n",
      " 666 768 249 139 248 433  62 518  52 641 517 800 822 226 829 708 696 360\n",
      " 197   8 843 558 476 730 523 388 494 434 946 861 129 878 817 839  63 628\n",
      " 840 459 611 745 250 773 146 621  96 645  82 871 738 869 345 805 298 802\n",
      " 809 765 930 896 831 821 721 955 361 253 406 680 128 101 824 634 284 279\n",
      " 115 181 224 327 556  32 784 669 615 376 577 218 743 887 536  98 237 458\n",
      " 486 296 925 945 324 243 344 865 823   0 531 631 447 153 403 513 363  78\n",
      " 722 716 367 263 891 272 875 277  70 444 668 719 533 255 503 211 957 131\n",
      " 422 530  24 507 718  80 740  21 814 310 516 938 311 753 112 870 351 454\n",
      " 201 522 804 664 150 223 599 620 316 600  71 276 384 729 121 560 423 143\n",
      " 525 646 450 352 337 421  33 532 709 216 606 343  16 858 644 684]\n",
      "Test:  [114 222 854 837 456 195 124 338 412 552 775 353 833 780 774 103 502 725\n",
      " 175 214 866 702 126 929 640 420 697 125  66  57 144  43   7 610 451 782\n",
      " 595 632 717 880  74 860 304 213 601 859 727 551 318 830 123  54 700 692\n",
      " 567 726 594 707 431 159 892 461 314  72 940 772 670 432 562 485 915 506\n",
      " 690 939 160 785 612 127 699 234 731 622 267 546 455 867   5 109  51  31\n",
      " 401 948 626 619 297 916 366 198 636 547 585 744 340 307 810 301 586 379\n",
      " 846  86 286  46 261 735 554 465 471 677 398 323 894 448 230 328 400 275\n",
      " 607 501 783  44  56 245 396 215 779 430 591 480 474 746 879 596 760 604\n",
      " 241 813 105 100 564 135 495 488 171 589 107 283 812 540  59 186 662 145\n",
      " 287  97 648 478 302 598 221 872  14 169 711 108 786  95 520 426 835 409\n",
      " 855 908 271  55 496 239 110 326 325 885 418 675 119 336 106 281  15 165\n",
      " 424 656 720 762 278 368   3 759 724 580 950 273 482 463  99 953 428 614\n",
      " 464 693 897  26 154 385 561 183  84  12  29 265 339 193 258 856 335  17\n",
      " 259 795 911 758 189 723]\n"
     ]
    }
   ],
   "source": [
    "print(\" -- VALIDACIÓN SIMPLE CON TIC-TAC-TOE DATA 80% -- \")\n",
    "estrategia = ValidacionSimple(0.75)\n",
    "estrategia.creaParticiones(dataset2)\n",
    "print(estrategia.particiones[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 940,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -- VALIDACIÓN SIMPLE CON GERMAN DATA 75% -- \n",
      "Train: [516 410 479  48 130 864  16 550 695  73 134 246  15 427  23 737 318 611\n",
      " 456  65 346 614 453 997 436 113 883 899 243 966 748 424 591  55 499 701\n",
      " 375 564 661 944 849 489 785 874 284 642 710 267 329 744 822 897 496 281\n",
      " 419 439  39 594 223 773 351 880 866 380 482 494 625 242 306 925 368 754\n",
      "  33 878 544 600 647 750 248 824 801 562  47 228 138 177 188 369 728 753\n",
      " 417 511 596 355 595 741 821 493  21 696 546 373 859 549 632  46 432 658\n",
      " 886 580 464 644 840  66 277 557 289  78 716 828  18 746 505 761 397 685\n",
      " 868 650 558 504 876 694 574 330 514 816 843 796  42 856 163 599 920 911\n",
      "  57 510 183 190 317 818  22 297 934 909  38 906 837 245 589 879 724 234\n",
      " 408 450  93 778 729  75 205 141 648 407 980 115 635 872 722 437 780 639\n",
      " 584 683  89 328 310 825 521 403 128   2 161 327   0 255   9 636 362 295\n",
      " 268 517 791 698 795 951 358 214 217 940 143 735 480 910 195  36 231 534\n",
      " 264 535 755 382 561 942 829 280 303  97 846 211 900 553 560 889 416 775\n",
      " 691 927 604 144 430 383 133  13  99 405 915 893 336 218 830 367 107 220\n",
      " 763 739  79 364 124 207 756 473 165 261  51 253 476 314 137 988 806 497\n",
      " 187 629 171  35 804 258 844 921 465 749 425 519 723 543 463 787 851 492\n",
      " 917 483 139 770 973 678 786 429 300 363 509 993 765 345 800 907 447 654\n",
      " 986   4 953 185 140 814  56 379 394 882 452 142 254 180  60 620 794 759\n",
      " 381 971 937  68 674 222 237  96 182 114 100 978 472 542 556 225 344 455\n",
      " 841 132 860   5 601 834 168 421 151 607 266 602 751 624 512 626 478 819\n",
      " 332 547 960 875 727 395 287 365 660 995 621 252 663 301 449 941 817 404\n",
      " 426 581 215 613 372 468 563 783 212 569 760 777 152 216 963 337 551 977\n",
      " 106  32 194 490 319 686 865 396 508 315 249 265 201 665 668 340  74 854\n",
      " 279   8 774 108 916 175 964 745 414 657 131  45 641 764 119 420 998 554\n",
      " 538 996 565 968 257 148 711 708  37  62  34 930 690 895 938  43 983 959\n",
      " 662 707  86 486 333 305 676  17 302 576 443 605 477 256 976 833 154 736\n",
      " 628 360 967 999 970  29 954 994 392 158 987 923 653 612 191 530 627 103\n",
      "  87 460 890 720  54 293 805 931 260 227 922 766 230 157  64 377 802 853\n",
      " 659 771 850 127 615 316  44 495 655 898 247  58 877 782 975 441 461 529\n",
      " 159 990 992 583 680 118 862 384 393 592 179 769 979 233 522 292 950 359\n",
      " 250 197 936 415 418  71 263 758 884 203 238 166 466 622 193 272 112 278\n",
      " 539 206 428  14 903 282 164 200 475  92 842 241 244  80 323  24 308 520\n",
      " 969 656 401  31 721 689  81 767   7 792 341 640 283  94 671 371 196 974\n",
      " 552 320 730 705 491 167 667 387 350 962 672 169 366 105 221 873 413 891\n",
      " 679 313 335 498 858 444  98 454 926 545 488  27 136  53 374 867 675 275\n",
      " 697 718 881 451 847 390 807 904 704 772 406 831   3  49 378 870 887 273\n",
      " 174 808 343 669 651 713 677 892 682  10  82 471 793 939  41 567 943 484\n",
      " 664 309 784 274 570 572 172 634 326 918 202 577 348 385 376 798 291 924\n",
      " 610  90 536  95 349 956 102 631 762 170 573 507 448 199 262 932 643 422\n",
      " 502 532 435 989  70 957 109 526 582 457 666 445 598 178 646  63 470 325\n",
      " 474  88 645 991 286 869 515 706 630 126 606 603   6 213 269 533  19 321\n",
      " 145 354 434 353 537 597 232 399 198 901 947 160]\n",
      "Test:  [885 731 402 673 855 559 259 848 540 240 985  67 423 700 981 322 162 153\n",
      " 670 500 352 820  59  40 189 811 186 155 984 440 637 725 742 528 933  28\n",
      "  12 949 914 919  77 438 965 285 121 150 311 835 442 324 411 501 684 799\n",
      "  84 836 459  85 952 391 719 902 566 743 687 649 726 712 946 149 541 709\n",
      " 524   1 575 251 433  25 129 487 609 294 815 961 356 863 334 431  20 146\n",
      " 579 838 116 347 125  83 224 738 173 331 409 715 236 226 312 827 469 122\n",
      " 618 531 176 361 209 184 693  69 912 593 779 101 857 896 619 192 776 688\n",
      " 797 204 803 982 905 342 555  72 958 752 617 945 400 714 616 568 467 523\n",
      " 757 481 147 587 812 111 578 888  26 703 790 768 181  11 548 702 276 462\n",
      " 852 503 788 288 398 845 571 296 732 518 652 229 527 972 110 832 633 590\n",
      " 339 299 235 458 717 338 638 809 485 747 506 446 928 734 239 120 357 740\n",
      " 913 117 388 871 270  50 839 208 826 935  91 608 948 823 389 861 370 623\n",
      "  61 699 588 156  52 733 789 307 894 813 412 929 908 210 586 810 104 692\n",
      " 525 585  30 781 135  76 290 955 304 386 271 298 219 513 123 681]\n"
     ]
    }
   ],
   "source": [
    "print(\" -- VALIDACIÓN SIMPLE CON GERMAN DATA 75% -- \")\n",
    "estrategia = ValidacionSimple(0.75)\n",
    "estrategia.creaParticiones(dataset3)\n",
    "print(estrategia.particiones[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Nuestra estrategia de <strong>validación cruzada</strong> consiste en dividir nuestro conjunto de datos en particiones de Train y Test como en validación simple, pero este proceso lo vamos a hacer K veces, para que todos los datos esten presenten en los dos subconjunto de datos para poder obtener una mejora a la hora de entrenar y clasificar.A continuación, mostraremos nuestra implementación de la validación cruzada; donde vamos a hacer K veces las divisiones del conunto de datos y si por algún motivo nuestra división de todos los subconjuntos no es perfecta balancearemos los datos sobrantes para poder entrenarlos y clasificarlos.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 941,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class ValidacionCruzada(EstrategiaParticionado):\n",
    "\n",
    "  # Crea particiones segun el metodo de validacion cruzada.\n",
    "  # El conjunto de entrenamiento se crea con las nfolds-1 particiones y el de test con la particion restante\n",
    "  # Esta funcion devuelve una lista de particiones (clase Particion)\n",
    "  # TODO: implementar\n",
    "\n",
    "  def __init__(self, k):\n",
    "    self.k = k\n",
    "    super().__init__(\"Validacion cruzada\")\n",
    "\n",
    "  def creaParticiones(self, datos, seed=None):\n",
    "    \n",
    "    np.random.seed(seed)\n",
    "\n",
    "    self.numeroParticiones = self.k\n",
    "\n",
    "    # Generamos una lista con todos los números de datos aleatorios\n",
    "    indicesAleatorios = np.random.permutation(int(datos.numDatos))\n",
    "\n",
    "    # Hallamos el tamaño de cada bloque\n",
    "    tamBloque = int(datos.numDatos / self.k)\n",
    "\n",
    "    datosSobran = datos.numDatos - (tamBloque * self.k)\n",
    "    count = 0\n",
    "    for i in range(self.k):\n",
    "\n",
    "      train = np.delete(indicesAleatorios, range(i * tamBloque, (i + 1) * tamBloque))\n",
    "      test = indicesAleatorios[i * tamBloque:(i + 1) * tamBloque]\n",
    "\n",
    "      # Caso en el que la cuenta es justa\n",
    "      if datosSobran == 0:\n",
    "        self.particiones.append(Particion(train, test))\n",
    "\n",
    "      # Contemplamos el caso de que la division para sacar el numero de subconjuntos no fuese entera\n",
    "      if datosSobran > 0:\n",
    "        count += 1\n",
    "        particionTest = np.append(test, train[(datos.numDatos - tamBloque) - i - 1])\n",
    "        particionTrain = np.delete(train, (datos.numDatos - tamBloque) - i - 1)\n",
    "        datosSobran -= 1\n",
    "        self.particiones.append(Particion(particionTrain, particionTest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>A continuación, mostraremos la ejecución de nuestra estrategia de partcionado validación simple o también llamada K-fold, con los diferentes conjuntos de datos y con diferentes K's. Vamos poder observar en la salida de nuestra celda que todos los valores van a estar una vez en la partición de Test.</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 942,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -- VALIDACIÓN CRUZADA CON LENSES DATA K=5 -- \n",
      "Train: [23 19  7 12  5  1  4  9  3  8 18 10 22 17 13 16  0 11 21]\n",
      "Test:  [15 14  6  2 20]\n",
      "Train: [15 14  6  2  5  1  4  9  3  8 18 10 22 17 13 16  0 11 20]\n",
      "Test:  [23 19  7 12 21]\n",
      "Train: [15 14  6  2 23 19  7 12  3  8 18 10 22 17 13 16  0 21 20]\n",
      "Test:  [ 5  1  4  9 11]\n",
      "Train: [15 14  6  2 23 19  7 12  5  1  4  9 22 17 13 16 11 21 20]\n",
      "Test:  [ 3  8 18 10  0]\n",
      "Train: [15 14  6  2 23 19  7 12  5  1  4  9  3  8 18 10  0 11 21 20]\n",
      "Test:  [22 17 13 16]\n"
     ]
    }
   ],
   "source": [
    "print(\" -- VALIDACIÓN CRUZADA CON LENSES DATA K=5 -- \")\n",
    "estrategia = ValidacionCruzada(5)\n",
    "estrategia.creaParticiones(dataset1)\n",
    "for particion in estrategia.particiones:\n",
    "    print(particion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 943,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -- VALIDACIÓN CRUZADA CON TIC-TAC-TOE DATA K=4 -- \n",
      "Train: [170 552 825 358 594 676 943 578 479 470 824 593 679 569 883 214 325 472\n",
      " 844 868 819 719 428 639 851 464  79  12 525 401   2 462 127 128 282 618\n",
      " 495 206 600  72  52 754 190 941 801 604 211  80 776 218 134 224 384 278\n",
      " 288 777 544 634 275 865 924 138 745 755 104 633 699 753 891 923 931 362\n",
      " 673 571 869 286 829 222  71 123 107 625 424 260 367 626 452 368  37 149\n",
      " 305 610 460 559   5 649 512 304 327 277 212  38 257   8 292 296 765 946\n",
      "  32 697 888  78 348 576 711 417 778 871 480  48 320 620  92  20  10  86\n",
      " 528 875 900 567 560 700 328 514 249 572 596 656 137 381 459 172 481 245\n",
      " 437 779 194 131 642 496  11 342 650 323 308 219 827 252 716 549 202 498\n",
      " 337  19 795 863  42 326 167 756 272 379 706 785 758 944  55 743  88 757\n",
      "  69 768 907 313 804 484 261  24 925 105 936 341 457   3 562 786  65 165\n",
      " 161 740 688 289 471 784 114 436 396   1  25 468 741  62 112 636 823 773\n",
      " 441 483 407 391  49 246 630 248 347  87 474 780 335 232  99 199 505 707\n",
      " 590 385 942 110 147 720 651 599 524 704 279 659  94 432 254 511 136 103\n",
      " 811  21 306  27 895 912 781 575 698 208 879 461 545 867  57 264 833 624\n",
      " 276 393 908 178 916   0 274 443 638 668  45 592 265 150 179 271 184 595\n",
      " 363 663 601 574 356 244 106 526 301 478 834 122 809 332 815 489  26 196\n",
      " 100 680 491 397 940 111 629 536 632 797 608  35 770 413 510 369 383 802\n",
      " 873 854 919 718 838 774 800 317 516 794  16 376 733 654 887 622 392  91\n",
      " 598 806 315 135 921 690 898 640 456 856 903  46 933 336 645 451 708 343\n",
      " 674 660 939  23 166 799 423 140 852 102 529 889 193 420 318 820   9  33\n",
      " 537 519 255 546 192 897 399 253 508 878 841 119 909 581  29 488 299 792\n",
      " 532 226 767 803  22 143  60 845 582 486 647 435 195 364 926 586 771 345\n",
      " 284  83 357 746 422 591 387  76 418 390 221 446 409 331 164   4 502 492\n",
      " 216 507 533 731  58 787 310 694 465 482 144 609 300 859 298 597 670 382\n",
      " 228 652 894 124  54 712 744 949 534 747  13 262 497 434 438 840 400  47\n",
      " 587 454 287 737 527 751 388 605 191 948 155 913  59 886 911 730 568 729\n",
      " 256 531 108 360 444 759 866 749 251 613  43 242 762 421 835 812 473 808\n",
      " 269 775 843 355 405 685 116 739 513  98 406 732 938 394 918 880 791 542\n",
      " 893 717 500 565 885 408 539 929 200 872 957 419 234 449 501 475 312  41\n",
      " 588 126 543  66 615 403 934 669 742 439 727 504 793 563 736 506 426 129\n",
      "  81 810 217 188 509 455 210 294 487 239 231 721 133 361 892  67 445 954\n",
      "  44 846 189 207 266 346 577 951 490 485 121 555 201 666 453 158 258 953\n",
      " 850  68 906 247 159 160  34 215   7  56 476 766 646 518 890 113 316 561\n",
      " 303 927 631  30 831 197 297 517 238 714 682 705 876  93 223 425 564 187\n",
      " 538 653 881 156 280  77 334  74 324 684 932   6 788 621  84 530 225  89\n",
      " 329 411 607 772 817 101 440 726 321 146 701 373 469 142 148 782 378 354\n",
      "  70 603 213  73 648 667 176 952 442 896 904  90 338 644 130 655 349 340\n",
      " 606 842 477 268 185 230 157 404 554 398 836 463 139 319 783 627 855 821\n",
      " 678 259 584 662 713 372 928 429 450 734 579 585 370  28 448 293 715 696\n",
      " 541 664 311 760 763 344 847 617 175 307 915 467 493 920 583 905 748 725\n",
      " 683 515 566 882 864 154 955 830 818 602 769 359 220 227 956 860 728 371\n",
      " 580 550 351 120 241  82 250 535  61 837]\n",
      "Test:  [173 553 466 671  40 350 291  36 814 947 857 910 619 171 235  17  64 695\n",
      " 243 522 290 628 267 935 612 661 377 551 273 151  39 641 380 169  63 302\n",
      " 822 828 686 693 570 789 109 556 309 558 689 163  14 589 523 635 870 410\n",
      " 162 548 236 853 735 950 764 691 415 861 884 402  53 186 723 675 263 503\n",
      " 877 416 412 314 687 902 805 174 917 181 240 427 703 389 366 209 798 521\n",
      " 637 447  97 611 281 198 796  18 832 738  50 930 499 365 430 816 540 724\n",
      " 182 750 352 433 458 677 848 937  75  96 125 395 117 901 283  15 374 807\n",
      " 702 183 414 858 386 547 826 520 914 573 839 614 709 237 813 874 205 431\n",
      " 270 722  31 330 945 180 285 132 849 141 557 899  85 623 177 681 233  51\n",
      "  95 657 665 658 339 616 115 494 353 790 168 203 204 761 862 375 752 145\n",
      " 118 643 295 153 322 152 672 710 922 333 692 229]\n",
      "Train: [173 553 466 671  40 350 291  36 814 947 857 910 619 171 235  17  64 695\n",
      " 243 522 290 628 267 935 612 661 377 551 273 151  39 641 380 169  63 302\n",
      " 822 828 686 693 570 789 109 556 309 558 689 163  14 589 523 635 870 410\n",
      " 162 548 236 853 735 950 764 691 415 861 884 402  53 186 723 675 263 503\n",
      " 877 416 412 314 687 902 805 174 917 181 240 427 703 389 366 209 798 521\n",
      " 637 447  97 611 281 198 796  18 832 738  50 930 499 365 430 816 540 724\n",
      " 182 750 352 433 458 677 848 937  75  96 125 395 117 901 283  15 374 807\n",
      " 702 183 414 858 386 547 826 520 914 573 839 614 709 237 813 874 205 431\n",
      " 270 722  31 330 945 180 285 132 849 141 557 899  85 623 177 681 233  51\n",
      "  95 657 665 658 339 616 115 494 353 790 168 203 204 761 862 375 752 145\n",
      " 118 643 295 153 322 152 672 710 922 333 692 341 457   3 562 786  65 165\n",
      " 161 740 688 289 471 784 114 436 396   1  25 468 741  62 112 636 823 773\n",
      " 441 483 407 391  49 246 630 248 347  87 474 780 335 232  99 199 505 707\n",
      " 590 385 942 110 147 720 651 599 524 704 279 659  94 432 254 511 136 103\n",
      " 811  21 306  27 895 912 781 575 698 208 879 461 545 867  57 264 833 624\n",
      " 276 393 908 178 916   0 274 443 638 668  45 592 265 150 179 271 184 595\n",
      " 363 663 601 574 356 244 106 526 301 478 834 122 809 332 815 489  26 196\n",
      " 100 680 491 397 940 111 629 536 632 797 608  35 770 413 510 369 383 802\n",
      " 873 854 919 718 838 774 800 317 516 794  16 376 733 654 887 622 392  91\n",
      " 598 806 315 135 921 690 898 640 456 856 903  46 933 336 645 451 708 343\n",
      " 674 660 939  23 166 799 423 140 852 102 529 889 193 420 318 820   9  33\n",
      " 537 519 255 546 192 897 399 253 508 878 841 119 909 581  29 488 299 792\n",
      " 532 226 767 803  22 143  60 845 582 486 647 435 195 364 926 586 771 345\n",
      " 284  83 357 746 422 591 387  76 418 390 221 446 409 331 164   4 502 492\n",
      " 216 507 533 731  58 787 310 694 465 482 144 609 300 859 298 597 670 382\n",
      " 228 652 894 124  54 712 744 949 534 747  13 262 497 434 438 840 400  47\n",
      " 587 454 287 737 527 751 388 605 191 948 155 913  59 886 911 730 568 729\n",
      " 256 531 108 360 444 759 866 749 251 613  43 242 762 421 835 812 473 808\n",
      " 269 775 843 355 405 685 116 739 513  98 406 732 938 394 918 880 791 542\n",
      " 893 717 500 565 885 408 539 929 200 872 957 419 234 449 501 475 312  41\n",
      " 588 126 543  66 615 403 934 669 742 439 727 504 793 563 736 506 426 129\n",
      "  81 810 217 188 509 455 210 294 487 239 231 721 133 361 892  67 445 954\n",
      "  44 846 189 207 266 346 577 951 490 485 121 555 201 666 453 158 258 953\n",
      " 850  68 906 247 159 160  34 215   7  56 476 766 646 518 890 113 316 561\n",
      " 303 927 631  30 831 197 297 517 238 714 682 705 876  93 223 425 564 187\n",
      " 538 653 881 156 280  77 334  74 324 684 932   6 788 621  84 530 225  89\n",
      " 329 411 607 772 817 101 440 726 321 146 701 373 469 142 148 782 378 354\n",
      "  70 603 213  73 648 667 176 952 442 896 904  90 338 644 130 655 349 340\n",
      " 606 842 477 268 185 230 157 404 554 398 836 463 139 319 783 627 855 821\n",
      " 678 259 584 662 713 372 928 429 450 734 579 585 370  28 448 293 715 696\n",
      " 541 664 311 760 763 344 847 617 175 307 915 467 493 920 583 905 748 725\n",
      " 683 515 566 882 864 154 955 830 818 602 769 359 220 227 956 860 728 371\n",
      " 580 550 351 120 241  82 250 535  61 229]\n",
      "Test:  [170 552 825 358 594 676 943 578 479 470 824 593 679 569 883 214 325 472\n",
      " 844 868 819 719 428 639 851 464  79  12 525 401   2 462 127 128 282 618\n",
      " 495 206 600  72  52 754 190 941 801 604 211  80 776 218 134 224 384 278\n",
      " 288 777 544 634 275 865 924 138 745 755 104 633 699 753 891 923 931 362\n",
      " 673 571 869 286 829 222  71 123 107 625 424 260 367 626 452 368  37 149\n",
      " 305 610 460 559   5 649 512 304 327 277 212  38 257   8 292 296 765 946\n",
      "  32 697 888  78 348 576 711 417 778 871 480  48 320 620  92  20  10  86\n",
      " 528 875 900 567 560 700 328 514 249 572 596 656 137 381 459 172 481 245\n",
      " 437 779 194 131 642 496  11 342 650 323 308 219 827 252 716 549 202 498\n",
      " 337  19 795 863  42 326 167 756 272 379 706 785 758 944  55 743  88 757\n",
      "  69 768 907 313 804 484 261  24 925 105 936 837]\n",
      "Train: [173 553 466 671  40 350 291  36 814 947 857 910 619 171 235  17  64 695\n",
      " 243 522 290 628 267 935 612 661 377 551 273 151  39 641 380 169  63 302\n",
      " 822 828 686 693 570 789 109 556 309 558 689 163  14 589 523 635 870 410\n",
      " 162 548 236 853 735 950 764 691 415 861 884 402  53 186 723 675 263 503\n",
      " 877 416 412 314 687 902 805 174 917 181 240 427 703 389 366 209 798 521\n",
      " 637 447  97 611 281 198 796  18 832 738  50 930 499 365 430 816 540 724\n",
      " 182 750 352 433 458 677 848 937  75  96 125 395 117 901 283  15 374 807\n",
      " 702 183 414 858 386 547 826 520 914 573 839 614 709 237 813 874 205 431\n",
      " 270 722  31 330 945 180 285 132 849 141 557 899  85 623 177 681 233  51\n",
      "  95 657 665 658 339 616 115 494 353 790 168 203 204 761 862 375 752 145\n",
      " 118 643 295 153 322 152 672 710 922 333 692 170 552 825 358 594 676 943\n",
      " 578 479 470 824 593 679 569 883 214 325 472 844 868 819 719 428 639 851\n",
      " 464  79  12 525 401   2 462 127 128 282 618 495 206 600  72  52 754 190\n",
      " 941 801 604 211  80 776 218 134 224 384 278 288 777 544 634 275 865 924\n",
      " 138 745 755 104 633 699 753 891 923 931 362 673 571 869 286 829 222  71\n",
      " 123 107 625 424 260 367 626 452 368  37 149 305 610 460 559   5 649 512\n",
      " 304 327 277 212  38 257   8 292 296 765 946  32 697 888  78 348 576 711\n",
      " 417 778 871 480  48 320 620  92  20  10  86 528 875 900 567 560 700 328\n",
      " 514 249 572 596 656 137 381 459 172 481 245 437 779 194 131 642 496  11\n",
      " 342 650 323 308 219 827 252 716 549 202 498 337  19 795 863  42 326 167\n",
      " 756 272 379 706 785 758 944  55 743  88 757  69 768 907 313 804 484 261\n",
      "  24 925 105 936 192 897 399 253 508 878 841 119 909 581  29 488 299 792\n",
      " 532 226 767 803  22 143  60 845 582 486 647 435 195 364 926 586 771 345\n",
      " 284  83 357 746 422 591 387  76 418 390 221 446 409 331 164   4 502 492\n",
      " 216 507 533 731  58 787 310 694 465 482 144 609 300 859 298 597 670 382\n",
      " 228 652 894 124  54 712 744 949 534 747  13 262 497 434 438 840 400  47\n",
      " 587 454 287 737 527 751 388 605 191 948 155 913  59 886 911 730 568 729\n",
      " 256 531 108 360 444 759 866 749 251 613  43 242 762 421 835 812 473 808\n",
      " 269 775 843 355 405 685 116 739 513  98 406 732 938 394 918 880 791 542\n",
      " 893 717 500 565 885 408 539 929 200 872 957 419 234 449 501 475 312  41\n",
      " 588 126 543  66 615 403 934 669 742 439 727 504 793 563 736 506 426 129\n",
      "  81 810 217 188 509 455 210 294 487 239 231 721 133 361 892  67 445 954\n",
      "  44 846 189 207 266 346 577 951 490 485 121 555 201 666 453 158 258 953\n",
      " 850  68 906 247 159 160  34 215   7  56 476 766 646 518 890 113 316 561\n",
      " 303 927 631  30 831 197 297 517 238 714 682 705 876  93 223 425 564 187\n",
      " 538 653 881 156 280  77 334  74 324 684 932   6 788 621  84 530 225  89\n",
      " 329 411 607 772 817 101 440 726 321 146 701 373 469 142 148 782 378 354\n",
      "  70 603 213  73 648 667 176 952 442 896 904  90 338 644 130 655 349 340\n",
      " 606 842 477 268 185 230 157 404 554 398 836 463 139 319 783 627 855 821\n",
      " 678 259 584 662 713 372 928 429 450 734 579 585 370  28 448 293 715 696\n",
      " 541 664 311 760 763 344 847 617 175 307 915 467 493 920 583 905 748 725\n",
      " 683 515 566 882 864 154 955 830 818 602 769 359 220 227 956 860 728 371\n",
      " 580 550 351 120 241  82 250 535 837 229]\n",
      "Test:  [341 457   3 562 786  65 165 161 740 688 289 471 784 114 436 396   1  25\n",
      " 468 741  62 112 636 823 773 441 483 407 391  49 246 630 248 347  87 474\n",
      " 780 335 232  99 199 505 707 590 385 942 110 147 720 651 599 524 704 279\n",
      " 659  94 432 254 511 136 103 811  21 306  27 895 912 781 575 698 208 879\n",
      " 461 545 867  57 264 833 624 276 393 908 178 916   0 274 443 638 668  45\n",
      " 592 265 150 179 271 184 595 363 663 601 574 356 244 106 526 301 478 834\n",
      " 122 809 332 815 489  26 196 100 680 491 397 940 111 629 536 632 797 608\n",
      "  35 770 413 510 369 383 802 873 854 919 718 838 774 800 317 516 794  16\n",
      " 376 733 654 887 622 392  91 598 806 315 135 921 690 898 640 456 856 903\n",
      "  46 933 336 645 451 708 343 674 660 939  23 166 799 423 140 852 102 529\n",
      " 889 193 420 318 820   9  33 537 519 255 546  61]\n",
      "Train: [173 553 466 671  40 350 291  36 814 947 857 910 619 171 235  17  64 695\n",
      " 243 522 290 628 267 935 612 661 377 551 273 151  39 641 380 169  63 302\n",
      " 822 828 686 693 570 789 109 556 309 558 689 163  14 589 523 635 870 410\n",
      " 162 548 236 853 735 950 764 691 415 861 884 402  53 186 723 675 263 503\n",
      " 877 416 412 314 687 902 805 174 917 181 240 427 703 389 366 209 798 521\n",
      " 637 447  97 611 281 198 796  18 832 738  50 930 499 365 430 816 540 724\n",
      " 182 750 352 433 458 677 848 937  75  96 125 395 117 901 283  15 374 807\n",
      " 702 183 414 858 386 547 826 520 914 573 839 614 709 237 813 874 205 431\n",
      " 270 722  31 330 945 180 285 132 849 141 557 899  85 623 177 681 233  51\n",
      "  95 657 665 658 339 616 115 494 353 790 168 203 204 761 862 375 752 145\n",
      " 118 643 295 153 322 152 672 710 922 333 692 170 552 825 358 594 676 943\n",
      " 578 479 470 824 593 679 569 883 214 325 472 844 868 819 719 428 639 851\n",
      " 464  79  12 525 401   2 462 127 128 282 618 495 206 600  72  52 754 190\n",
      " 941 801 604 211  80 776 218 134 224 384 278 288 777 544 634 275 865 924\n",
      " 138 745 755 104 633 699 753 891 923 931 362 673 571 869 286 829 222  71\n",
      " 123 107 625 424 260 367 626 452 368  37 149 305 610 460 559   5 649 512\n",
      " 304 327 277 212  38 257   8 292 296 765 946  32 697 888  78 348 576 711\n",
      " 417 778 871 480  48 320 620  92  20  10  86 528 875 900 567 560 700 328\n",
      " 514 249 572 596 656 137 381 459 172 481 245 437 779 194 131 642 496  11\n",
      " 342 650 323 308 219 827 252 716 549 202 498 337  19 795 863  42 326 167\n",
      " 756 272 379 706 785 758 944  55 743  88 757  69 768 907 313 804 484 261\n",
      "  24 925 105 936 341 457   3 562 786  65 165 161 740 688 289 471 784 114\n",
      " 436 396   1  25 468 741  62 112 636 823 773 441 483 407 391  49 246 630\n",
      " 248 347  87 474 780 335 232  99 199 505 707 590 385 942 110 147 720 651\n",
      " 599 524 704 279 659  94 432 254 511 136 103 811  21 306  27 895 912 781\n",
      " 575 698 208 879 461 545 867  57 264 833 624 276 393 908 178 916   0 274\n",
      " 443 638 668  45 592 265 150 179 271 184 595 363 663 601 574 356 244 106\n",
      " 526 301 478 834 122 809 332 815 489  26 196 100 680 491 397 940 111 629\n",
      " 536 632 797 608  35 770 413 510 369 383 802 873 854 919 718 838 774 800\n",
      " 317 516 794  16 376 733 654 887 622 392  91 598 806 315 135 921 690 898\n",
      " 640 456 856 903  46 933 336 645 451 708 343 674 660 939  23 166 799 423\n",
      " 140 852 102 529 889 193 420 318 820   9  33 537 519 255 546  67 445 954\n",
      "  44 846 189 207 266 346 577 951 490 485 121 555 201 666 453 158 258 953\n",
      " 850  68 906 247 159 160  34 215   7  56 476 766 646 518 890 113 316 561\n",
      " 303 927 631  30 831 197 297 517 238 714 682 705 876  93 223 425 564 187\n",
      " 538 653 881 156 280  77 334  74 324 684 932   6 788 621  84 530 225  89\n",
      " 329 411 607 772 817 101 440 726 321 146 701 373 469 142 148 782 378 354\n",
      "  70 603 213  73 648 667 176 952 442 896 904  90 338 644 130 655 349 340\n",
      " 606 842 477 268 185 230 157 404 554 398 836 463 139 319 783 627 855 821\n",
      " 678 259 584 662 713 372 928 429 450 734 579 585 370  28 448 293 715 696\n",
      " 541 664 311 760 763 344 847 617 175 307 915 467 493 920 583 905 748 725\n",
      " 683 515 566 882 864 154 955 830 818 602 769 359 220 227 956 860 728 371\n",
      " 580 550 351 120 241  82 250 535  61 837 229]\n",
      "Test:  [192 897 399 253 508 878 841 119 909 581  29 488 299 792 532 226 767 803\n",
      "  22 143  60 845 582 486 647 435 195 364 926 586 771 345 284  83 357 746\n",
      " 422 591 387  76 418 390 221 446 409 331 164   4 502 492 216 507 533 731\n",
      "  58 787 310 694 465 482 144 609 300 859 298 597 670 382 228 652 894 124\n",
      "  54 712 744 949 534 747  13 262 497 434 438 840 400  47 587 454 287 737\n",
      " 527 751 388 605 191 948 155 913  59 886 911 730 568 729 256 531 108 360\n",
      " 444 759 866 749 251 613  43 242 762 421 835 812 473 808 269 775 843 355\n",
      " 405 685 116 739 513  98 406 732 938 394 918 880 791 542 893 717 500 565\n",
      " 885 408 539 929 200 872 957 419 234 449 501 475 312  41 588 126 543  66\n",
      " 615 403 934 669 742 439 727 504 793 563 736 506 426 129  81 810 217 188\n",
      " 509 455 210 294 487 239 231 721 133 361 892]\n",
      "Train: [173 553 466 671  40 350 291  36 814 947 857 910 619 171 235  17  64 695\n",
      " 243 522 290 628 267 935 612 661 377 551 273 151  39 641 380 169  63 302\n",
      " 822 828 686 693 570 789 109 556 309 558 689 163  14 589 523 635 870 410\n",
      " 162 548 236 853 735 950 764 691 415 861 884 402  53 186 723 675 263 503\n",
      " 877 416 412 314 687 902 805 174 917 181 240 427 703 389 366 209 798 521\n",
      " 637 447  97 611 281 198 796  18 832 738  50 930 499 365 430 816 540 724\n",
      " 182 750 352 433 458 677 848 937  75  96 125 395 117 901 283  15 374 807\n",
      " 702 183 414 858 386 547 826 520 914 573 839 614 709 237 813 874 205 431\n",
      " 270 722  31 330 945 180 285 132 849 141 557 899  85 623 177 681 233  51\n",
      "  95 657 665 658 339 616 115 494 353 790 168 203 204 761 862 375 752 145\n",
      " 118 643 295 153 322 152 672 710 922 333 692 170 552 825 358 594 676 943\n",
      " 578 479 470 824 593 679 569 883 214 325 472 844 868 819 719 428 639 851\n",
      " 464  79  12 525 401   2 462 127 128 282 618 495 206 600  72  52 754 190\n",
      " 941 801 604 211  80 776 218 134 224 384 278 288 777 544 634 275 865 924\n",
      " 138 745 755 104 633 699 753 891 923 931 362 673 571 869 286 829 222  71\n",
      " 123 107 625 424 260 367 626 452 368  37 149 305 610 460 559   5 649 512\n",
      " 304 327 277 212  38 257   8 292 296 765 946  32 697 888  78 348 576 711\n",
      " 417 778 871 480  48 320 620  92  20  10  86 528 875 900 567 560 700 328\n",
      " 514 249 572 596 656 137 381 459 172 481 245 437 779 194 131 642 496  11\n",
      " 342 650 323 308 219 827 252 716 549 202 498 337  19 795 863  42 326 167\n",
      " 756 272 379 706 785 758 944  55 743  88 757  69 768 907 313 804 484 261\n",
      "  24 925 105 936 341 457   3 562 786  65 165 161 740 688 289 471 784 114\n",
      " 436 396   1  25 468 741  62 112 636 823 773 441 483 407 391  49 246 630\n",
      " 248 347  87 474 780 335 232  99 199 505 707 590 385 942 110 147 720 651\n",
      " 599 524 704 279 659  94 432 254 511 136 103 811  21 306  27 895 912 781\n",
      " 575 698 208 879 461 545 867  57 264 833 624 276 393 908 178 916   0 274\n",
      " 443 638 668  45 592 265 150 179 271 184 595 363 663 601 574 356 244 106\n",
      " 526 301 478 834 122 809 332 815 489  26 196 100 680 491 397 940 111 629\n",
      " 536 632 797 608  35 770 413 510 369 383 802 873 854 919 718 838 774 800\n",
      " 317 516 794  16 376 733 654 887 622 392  91 598 806 315 135 921 690 898\n",
      " 640 456 856 903  46 933 336 645 451 708 343 674 660 939  23 166 799 423\n",
      " 140 852 102 529 889 193 420 318 820   9  33 537 519 255 546 192 897 399\n",
      " 253 508 878 841 119 909 581  29 488 299 792 532 226 767 803  22 143  60\n",
      " 845 582 486 647 435 195 364 926 586 771 345 284  83 357 746 422 591 387\n",
      "  76 418 390 221 446 409 331 164   4 502 492 216 507 533 731  58 787 310\n",
      " 694 465 482 144 609 300 859 298 597 670 382 228 652 894 124  54 712 744\n",
      " 949 534 747  13 262 497 434 438 840 400  47 587 454 287 737 527 751 388\n",
      " 605 191 948 155 913  59 886 911 730 568 729 256 531 108 360 444 759 866\n",
      " 749 251 613  43 242 762 421 835 812 473 808 269 775 843 355 405 685 116\n",
      " 739 513  98 406 732 938 394 918 880 791 542 893 717 500 565 885 408 539\n",
      " 929 200 872 957 419 234 449 501 475 312  41 588 126 543  66 615 403 934\n",
      " 669 742 439 727 504 793 563 736 506 426 129  81 810 217 188 509 455 210\n",
      " 294 487 239 231 721 133 361 892  61 837 229]\n",
      "Test:  [ 67 445 954  44 846 189 207 266 346 577 951 490 485 121 555 201 666 453\n",
      " 158 258 953 850  68 906 247 159 160  34 215   7  56 476 766 646 518 890\n",
      " 113 316 561 303 927 631  30 831 197 297 517 238 714 682 705 876  93 223\n",
      " 425 564 187 538 653 881 156 280  77 334  74 324 684 932   6 788 621  84\n",
      " 530 225  89 329 411 607 772 817 101 440 726 321 146 701 373 469 142 148\n",
      " 782 378 354  70 603 213  73 648 667 176 952 442 896 904  90 338 644 130\n",
      " 655 349 340 606 842 477 268 185 230 157 404 554 398 836 463 139 319 783\n",
      " 627 855 821 678 259 584 662 713 372 928 429 450 734 579 585 370  28 448\n",
      " 293 715 696 541 664 311 760 763 344 847 617 175 307 915 467 493 920 583\n",
      " 905 748 725 683 515 566 882 864 154 955 830 818 602 769 359 220 227 956\n",
      " 860 728 371 580 550 351 120 241  82 250 535]\n"
     ]
    }
   ],
   "source": [
    "print(\" -- VALIDACIÓN CRUZADA CON TIC-TAC-TOE DATA K=4 -- \")\n",
    "estrategia = ValidacionCruzada(5)\n",
    "estrategia.creaParticiones(dataset2)\n",
    "for particion in estrategia.particiones:\n",
    "    print(particion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 944,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -- VALIDACIÓN CRUZADA CON GERMAN DATA K=4 -- \n",
      "Train: [ 18  92 694 888 452 726 641 860 301 706 945 949 464 465 910 751 976 588\n",
      " 230 602 253 101 380  31 928 338 210 788 164 415 607 361 916 989 431 618\n",
      " 349 865 174 421  86 836 975 768 612  45 500 911 796 130 625 934 383 311\n",
      " 778 175 761 168 590  58 963 355 769 727 571 718 100 555 703 548 771  83\n",
      " 109 966 523 497 192 867 461 743 426 151 732 483 398 981 547 940 515 856\n",
      " 809  99 960 959  97 988 319 429 919 845 848 240 972 819 360 929 806 927\n",
      " 258 315 628 576 345 843   0 884 636 290   4 512  54 146 887 247 705  11\n",
      " 930  78 249 558 931 518 654 148 953 886 920 239 979 637 573 691 179 690\n",
      " 858 632 291 219 651 399 575 987 801 813 938 951 237 792  40 119 829 781\n",
      " 730 344 619 763 165 266 683 810 485 354 877 561 698 488 630 950 587 525\n",
      " 104 529 394 702 572 747 847 593   6 454 814 740 451  32 166 557 816 455\n",
      " 800 685  35 830 403 121  87 269 430 106 872 370 591 185 284  96 376 840\n",
      " 655 970 400 815 138 762 221 335 644 735 484 915 528 811 849 839 855 760\n",
      " 372 753 719 994 435 866 446 333 520 853 777 826 216 326 957 450 657 812\n",
      " 213 141 774 384 508 717  46 389 734 791 453   2 943 265 274 478 687 861\n",
      " 123 116  91 736 112 752 874 707 170 259  53 466 842 638  71  60  94 428\n",
      " 495 480 377 256 171 458 402 715 664 704 346  98 554 410 139 386 882 890\n",
      " 708 254 303 739  93 903 215 456 608 933 300 334 287 578  76 549 892 232\n",
      "  49 883 378 765 896 643 351 288 263  82 594 441 133 195 971 562 492 234\n",
      " 513 468 621 268 731 406 660 248 184  36 857 519 203 808 153 585 653  72\n",
      " 932   9 479 908 656 941 128 924 530  89 723 880 766 714 140 336 442 517\n",
      " 531  79 737 395 392 712  62 310 283 397 209  61 481 629 250 746 204 894\n",
      " 224 294  22 581 510 419 645  69  57 362 569 524 463 783  27 514  25 538\n",
      " 767 681 438 494 545 779 648 782 331 275 670 697 954  15 309 436 667 194\n",
      "  12 144 183 198 983 413 490 639 526 570 582 182 680 460 324 382 107 611\n",
      " 432 158 700 595 271 347  42 127 371 977 330 764 328 770 412 120 617 457\n",
      "  47 912 634 173 881 214 600 317 631 921 597 110 537 666 260 359  37 756\n",
      " 947 401 489 901 535 276 620 350  19 635 939 190 835 425 486 550 329 367\n",
      "  44 623 374 952 599 533   5 846 935 821 281 135 559 755 228 633 640 869\n",
      " 652 834 125 448 307 678 467 375 824 604 292 688 605  13 893 729 102 906\n",
      " 907 658 137 178 285 974 661 964 475 962 188 534 298 741 507 738 863 169\n",
      " 754 473 564 876 437 679 108   1 418 471 784 131  80 499 142 565 790 364\n",
      " 613 616 827 889 155 327 583 822 823 306 407 709 676 918 427 948  64  81\n",
      " 286 420 225 733 440 937 482 759 172 337 838 999 798 469 417 978 984 995\n",
      " 601 802 794 904 897 504 122 358 161 132 551 318 832 682  74 196 837 566\n",
      " 965 546  63 577 443 539 584 270 891 305 212 993 222 900 986 985 343 246\n",
      " 261 828 552  33 387 439 150 543 156 396  55 279 787 873 385 293 748 176\n",
      " 390 668 393 899 540 509 563 200 854 677 187 749 646 229 603  84 449 695\n",
      " 850 202 967 521 299  88 147 115 574 669 312 180 560  48 236 414 532 231\n",
      " 308   8 459 416 186 665 304 322 875  39 973 235 750 674  14 472 722 793\n",
      " 803 505  52 227 363 785 162 160 673 701 580 423 624 223  59  73 373 340\n",
      " 422 404 262 721 627  43  51 659 476 280 825 136]\n",
      "Test:  [615 353  34 118 313 289 710 851 365 316 556 493 650 586 409 775 113 609\n",
      " 477  68 117 103 238 208 323 961 264 220 157 917 197 692 909 111 226   7\n",
      " 424 862 503 462 990 622 946 332 797 267 272 134 342 724  95 820 693 672\n",
      " 255  41 925 997 177 589 206 804 598 852 716 996 501 251 696 663 252 914\n",
      " 699 942 314  24 233 348 958 211 502 470  77 273 789  38 567 297 191 217\n",
      " 955 411 522 149 902 496 114 542  29 870 357 444 926 799 379 905 982 686\n",
      " 805  20  50 366 885 282  26 242  90  66 321 498 278 408 339 159 807  17\n",
      " 143 381 817 913 341 592  23  30 207  10 969 126 105 152 568 243 780 193\n",
      " 878 163 647 675 649 145  67 610 859  75 474 711 277 795 181  85 980 895\n",
      " 742 295 868 786 356 871 720 998 844 434 369 713  28 757 527 944 818 388\n",
      " 773 728 936 296 201 991 167 671  16  56 516 776 642 864 189 205 544 325\n",
      "   3 391 662 841 368 257 758 154 898 923 199 536 445 879  70  21 352 487\n",
      " 129 506 725 320 684 553  65 596 968 956 614 689 579 922 245 433 511 244\n",
      " 491 606 745 833 302 405 831 124 744 241 447 772 218 992 541 626]\n",
      "Train: [615 353  34 118 313 289 710 851 365 316 556 493 650 586 409 775 113 609\n",
      " 477  68 117 103 238 208 323 961 264 220 157 917 197 692 909 111 226   7\n",
      " 424 862 503 462 990 622 946 332 797 267 272 134 342 724  95 820 693 672\n",
      " 255  41 925 997 177 589 206 804 598 852 716 996 501 251 696 663 252 914\n",
      " 699 942 314  24 233 348 958 211 502 470  77 273 789  38 567 297 191 217\n",
      " 955 411 522 149 902 496 114 542  29 870 357 444 926 799 379 905 982 686\n",
      " 805  20  50 366 885 282  26 242  90  66 321 498 278 408 339 159 807  17\n",
      " 143 381 817 913 341 592  23  30 207  10 969 126 105 152 568 243 780 193\n",
      " 878 163 647 675 649 145  67 610 859  75 474 711 277 795 181  85 980 895\n",
      " 742 295 868 786 356 871 720 998 844 434 369 713  28 757 527 944 818 388\n",
      " 773 728 936 296 201 991 167 671  16  56 516 776 642 864 189 205 544 325\n",
      "   3 391 662 841 368 257 758 154 898 923 199 536 445 879  70  21 352 487\n",
      " 129 506 725 320 684 553  65 596 968 956 614 689 579 922 245 433 511 244\n",
      " 491 606 745 833 302 405 831 124 744 241 447 772 218 992 541 626 657 812\n",
      " 213 141 774 384 508 717  46 389 734 791 453   2 943 265 274 478 687 861\n",
      " 123 116  91 736 112 752 874 707 170 259  53 466 842 638  71  60  94 428\n",
      " 495 480 377 256 171 458 402 715 664 704 346  98 554 410 139 386 882 890\n",
      " 708 254 303 739  93 903 215 456 608 933 300 334 287 578  76 549 892 232\n",
      "  49 883 378 765 896 643 351 288 263  82 594 441 133 195 971 562 492 234\n",
      " 513 468 621 268 731 406 660 248 184  36 857 519 203 808 153 585 653  72\n",
      " 932   9 479 908 656 941 128 924 530  89 723 880 766 714 140 336 442 517\n",
      " 531  79 737 395 392 712  62 310 283 397 209  61 481 629 250 746 204 894\n",
      " 224 294  22 581 510 419 645  69  57 362 569 524 463 783  27 514  25 538\n",
      " 767 681 438 494 545 779 648 782 331 275 670 697 954  15 309 436 667 194\n",
      "  12 144 183 198 983 413 490 639 526 570 582 182 680 460 324 382 107 611\n",
      " 432 158 700 595 271 347  42 127 371 977 330 764 328 770 412 120 617 457\n",
      "  47 912 634 173 881 214 600 317 631 921 597 110 537 666 260 359  37 756\n",
      " 947 401 489 901 535 276 620 350  19 635 939 190 835 425 486 550 329 367\n",
      "  44 623 374 952 599 533   5 846 935 821 281 135 559 755 228 633 640 869\n",
      " 652 834 125 448 307 678 467 375 824 604 292 688 605  13 893 729 102 906\n",
      " 907 658 137 178 285 974 661 964 475 962 188 534 298 741 507 738 863 169\n",
      " 754 473 564 876 437 679 108   1 418 471 784 131  80 499 142 565 790 364\n",
      " 613 616 827 889 155 327 583 822 823 306 407 709 676 918 427 948  64  81\n",
      " 286 420 225 733 440 937 482 759 172 337 838 999 798 469 417 978 984 995\n",
      " 601 802 794 904 897 504 122 358 161 132 551 318 832 682  74 196 837 566\n",
      " 965 546  63 577 443 539 584 270 891 305 212 993 222 900 986 985 343 246\n",
      " 261 828 552  33 387 439 150 543 156 396  55 279 787 873 385 293 748 176\n",
      " 390 668 393 899 540 509 563 200 854 677 187 749 646 229 603  84 449 695\n",
      " 850 202 967 521 299  88 147 115 574 669 312 180 560  48 236 414 532 231\n",
      " 308   8 459 416 186 665 304 322 875  39 973 235 750 674  14 472 722 793\n",
      " 803 505  52 227 363 785 162 160 673 701 580 423 624 223  59  73 373 340\n",
      " 422 404 262 721 627  43  51 659 476 280 825 136]\n",
      "Test:  [ 18  92 694 888 452 726 641 860 301 706 945 949 464 465 910 751 976 588\n",
      " 230 602 253 101 380  31 928 338 210 788 164 415 607 361 916 989 431 618\n",
      " 349 865 174 421  86 836 975 768 612  45 500 911 796 130 625 934 383 311\n",
      " 778 175 761 168 590  58 963 355 769 727 571 718 100 555 703 548 771  83\n",
      " 109 966 523 497 192 867 461 743 426 151 732 483 398 981 547 940 515 856\n",
      " 809  99 960 959  97 988 319 429 919 845 848 240 972 819 360 929 806 927\n",
      " 258 315 628 576 345 843   0 884 636 290   4 512  54 146 887 247 705  11\n",
      " 930  78 249 558 931 518 654 148 953 886 920 239 979 637 573 691 179 690\n",
      " 858 632 291 219 651 399 575 987 801 813 938 951 237 792  40 119 829 781\n",
      " 730 344 619 763 165 266 683 810 485 354 877 561 698 488 630 950 587 525\n",
      " 104 529 394 702 572 747 847 593   6 454 814 740 451  32 166 557 816 455\n",
      " 800 685  35 830 403 121  87 269 430 106 872 370 591 185 284  96 376 840\n",
      " 655 970 400 815 138 762 221 335 644 735 484 915 528 811 849 839 855 760\n",
      " 372 753 719 994 435 866 446 333 520 853 777 826 216 326 957 450]\n",
      "Train: [615 353  34 118 313 289 710 851 365 316 556 493 650 586 409 775 113 609\n",
      " 477  68 117 103 238 208 323 961 264 220 157 917 197 692 909 111 226   7\n",
      " 424 862 503 462 990 622 946 332 797 267 272 134 342 724  95 820 693 672\n",
      " 255  41 925 997 177 589 206 804 598 852 716 996 501 251 696 663 252 914\n",
      " 699 942 314  24 233 348 958 211 502 470  77 273 789  38 567 297 191 217\n",
      " 955 411 522 149 902 496 114 542  29 870 357 444 926 799 379 905 982 686\n",
      " 805  20  50 366 885 282  26 242  90  66 321 498 278 408 339 159 807  17\n",
      " 143 381 817 913 341 592  23  30 207  10 969 126 105 152 568 243 780 193\n",
      " 878 163 647 675 649 145  67 610 859  75 474 711 277 795 181  85 980 895\n",
      " 742 295 868 786 356 871 720 998 844 434 369 713  28 757 527 944 818 388\n",
      " 773 728 936 296 201 991 167 671  16  56 516 776 642 864 189 205 544 325\n",
      "   3 391 662 841 368 257 758 154 898 923 199 536 445 879  70  21 352 487\n",
      " 129 506 725 320 684 553  65 596 968 956 614 689 579 922 245 433 511 244\n",
      " 491 606 745 833 302 405 831 124 744 241 447 772 218 992 541 626  18  92\n",
      " 694 888 452 726 641 860 301 706 945 949 464 465 910 751 976 588 230 602\n",
      " 253 101 380  31 928 338 210 788 164 415 607 361 916 989 431 618 349 865\n",
      " 174 421  86 836 975 768 612  45 500 911 796 130 625 934 383 311 778 175\n",
      " 761 168 590  58 963 355 769 727 571 718 100 555 703 548 771  83 109 966\n",
      " 523 497 192 867 461 743 426 151 732 483 398 981 547 940 515 856 809  99\n",
      " 960 959  97 988 319 429 919 845 848 240 972 819 360 929 806 927 258 315\n",
      " 628 576 345 843   0 884 636 290   4 512  54 146 887 247 705  11 930  78\n",
      " 249 558 931 518 654 148 953 886 920 239 979 637 573 691 179 690 858 632\n",
      " 291 219 651 399 575 987 801 813 938 951 237 792  40 119 829 781 730 344\n",
      " 619 763 165 266 683 810 485 354 877 561 698 488 630 950 587 525 104 529\n",
      " 394 702 572 747 847 593   6 454 814 740 451  32 166 557 816 455 800 685\n",
      "  35 830 403 121  87 269 430 106 872 370 591 185 284  96 376 840 655 970\n",
      " 400 815 138 762 221 335 644 735 484 915 528 811 849 839 855 760 372 753\n",
      " 719 994 435 866 446 333 520 853 777 826 216 326 957 450 486 550 329 367\n",
      "  44 623 374 952 599 533   5 846 935 821 281 135 559 755 228 633 640 869\n",
      " 652 834 125 448 307 678 467 375 824 604 292 688 605  13 893 729 102 906\n",
      " 907 658 137 178 285 974 661 964 475 962 188 534 298 741 507 738 863 169\n",
      " 754 473 564 876 437 679 108   1 418 471 784 131  80 499 142 565 790 364\n",
      " 613 616 827 889 155 327 583 822 823 306 407 709 676 918 427 948  64  81\n",
      " 286 420 225 733 440 937 482 759 172 337 838 999 798 469 417 978 984 995\n",
      " 601 802 794 904 897 504 122 358 161 132 551 318 832 682  74 196 837 566\n",
      " 965 546  63 577 443 539 584 270 891 305 212 993 222 900 986 985 343 246\n",
      " 261 828 552  33 387 439 150 543 156 396  55 279 787 873 385 293 748 176\n",
      " 390 668 393 899 540 509 563 200 854 677 187 749 646 229 603  84 449 695\n",
      " 850 202 967 521 299  88 147 115 574 669 312 180 560  48 236 414 532 231\n",
      " 308   8 459 416 186 665 304 322 875  39 973 235 750 674  14 472 722 793\n",
      " 803 505  52 227 363 785 162 160 673 701 580 423 624 223  59  73 373 340\n",
      " 422 404 262 721 627  43  51 659 476 280 825 136]\n",
      "Test:  [657 812 213 141 774 384 508 717  46 389 734 791 453   2 943 265 274 478\n",
      " 687 861 123 116  91 736 112 752 874 707 170 259  53 466 842 638  71  60\n",
      "  94 428 495 480 377 256 171 458 402 715 664 704 346  98 554 410 139 386\n",
      " 882 890 708 254 303 739  93 903 215 456 608 933 300 334 287 578  76 549\n",
      " 892 232  49 883 378 765 896 643 351 288 263  82 594 441 133 195 971 562\n",
      " 492 234 513 468 621 268 731 406 660 248 184  36 857 519 203 808 153 585\n",
      " 653  72 932   9 479 908 656 941 128 924 530  89 723 880 766 714 140 336\n",
      " 442 517 531  79 737 395 392 712  62 310 283 397 209  61 481 629 250 746\n",
      " 204 894 224 294  22 581 510 419 645  69  57 362 569 524 463 783  27 514\n",
      "  25 538 767 681 438 494 545 779 648 782 331 275 670 697 954  15 309 436\n",
      " 667 194  12 144 183 198 983 413 490 639 526 570 582 182 680 460 324 382\n",
      " 107 611 432 158 700 595 271 347  42 127 371 977 330 764 328 770 412 120\n",
      " 617 457  47 912 634 173 881 214 600 317 631 921 597 110 537 666 260 359\n",
      "  37 756 947 401 489 901 535 276 620 350  19 635 939 190 835 425]\n",
      "Train: [615 353  34 118 313 289 710 851 365 316 556 493 650 586 409 775 113 609\n",
      " 477  68 117 103 238 208 323 961 264 220 157 917 197 692 909 111 226   7\n",
      " 424 862 503 462 990 622 946 332 797 267 272 134 342 724  95 820 693 672\n",
      " 255  41 925 997 177 589 206 804 598 852 716 996 501 251 696 663 252 914\n",
      " 699 942 314  24 233 348 958 211 502 470  77 273 789  38 567 297 191 217\n",
      " 955 411 522 149 902 496 114 542  29 870 357 444 926 799 379 905 982 686\n",
      " 805  20  50 366 885 282  26 242  90  66 321 498 278 408 339 159 807  17\n",
      " 143 381 817 913 341 592  23  30 207  10 969 126 105 152 568 243 780 193\n",
      " 878 163 647 675 649 145  67 610 859  75 474 711 277 795 181  85 980 895\n",
      " 742 295 868 786 356 871 720 998 844 434 369 713  28 757 527 944 818 388\n",
      " 773 728 936 296 201 991 167 671  16  56 516 776 642 864 189 205 544 325\n",
      "   3 391 662 841 368 257 758 154 898 923 199 536 445 879  70  21 352 487\n",
      " 129 506 725 320 684 553  65 596 968 956 614 689 579 922 245 433 511 244\n",
      " 491 606 745 833 302 405 831 124 744 241 447 772 218 992 541 626  18  92\n",
      " 694 888 452 726 641 860 301 706 945 949 464 465 910 751 976 588 230 602\n",
      " 253 101 380  31 928 338 210 788 164 415 607 361 916 989 431 618 349 865\n",
      " 174 421  86 836 975 768 612  45 500 911 796 130 625 934 383 311 778 175\n",
      " 761 168 590  58 963 355 769 727 571 718 100 555 703 548 771  83 109 966\n",
      " 523 497 192 867 461 743 426 151 732 483 398 981 547 940 515 856 809  99\n",
      " 960 959  97 988 319 429 919 845 848 240 972 819 360 929 806 927 258 315\n",
      " 628 576 345 843   0 884 636 290   4 512  54 146 887 247 705  11 930  78\n",
      " 249 558 931 518 654 148 953 886 920 239 979 637 573 691 179 690 858 632\n",
      " 291 219 651 399 575 987 801 813 938 951 237 792  40 119 829 781 730 344\n",
      " 619 763 165 266 683 810 485 354 877 561 698 488 630 950 587 525 104 529\n",
      " 394 702 572 747 847 593   6 454 814 740 451  32 166 557 816 455 800 685\n",
      "  35 830 403 121  87 269 430 106 872 370 591 185 284  96 376 840 655 970\n",
      " 400 815 138 762 221 335 644 735 484 915 528 811 849 839 855 760 372 753\n",
      " 719 994 435 866 446 333 520 853 777 826 216 326 957 450 657 812 213 141\n",
      " 774 384 508 717  46 389 734 791 453   2 943 265 274 478 687 861 123 116\n",
      "  91 736 112 752 874 707 170 259  53 466 842 638  71  60  94 428 495 480\n",
      " 377 256 171 458 402 715 664 704 346  98 554 410 139 386 882 890 708 254\n",
      " 303 739  93 903 215 456 608 933 300 334 287 578  76 549 892 232  49 883\n",
      " 378 765 896 643 351 288 263  82 594 441 133 195 971 562 492 234 513 468\n",
      " 621 268 731 406 660 248 184  36 857 519 203 808 153 585 653  72 932   9\n",
      " 479 908 656 941 128 924 530  89 723 880 766 714 140 336 442 517 531  79\n",
      " 737 395 392 712  62 310 283 397 209  61 481 629 250 746 204 894 224 294\n",
      "  22 581 510 419 645  69  57 362 569 524 463 783  27 514  25 538 767 681\n",
      " 438 494 545 779 648 782 331 275 670 697 954  15 309 436 667 194  12 144\n",
      " 183 198 983 413 490 639 526 570 582 182 680 460 324 382 107 611 432 158\n",
      " 700 595 271 347  42 127 371 977 330 764 328 770 412 120 617 457  47 912\n",
      " 634 173 881 214 600 317 631 921 597 110 537 666 260 359  37 756 947 401\n",
      " 489 901 535 276 620 350  19 635 939 190 835 425]\n",
      "Test:  [486 550 329 367  44 623 374 952 599 533   5 846 935 821 281 135 559 755\n",
      " 228 633 640 869 652 834 125 448 307 678 467 375 824 604 292 688 605  13\n",
      " 893 729 102 906 907 658 137 178 285 974 661 964 475 962 188 534 298 741\n",
      " 507 738 863 169 754 473 564 876 437 679 108   1 418 471 784 131  80 499\n",
      " 142 565 790 364 613 616 827 889 155 327 583 822 823 306 407 709 676 918\n",
      " 427 948  64  81 286 420 225 733 440 937 482 759 172 337 838 999 798 469\n",
      " 417 978 984 995 601 802 794 904 897 504 122 358 161 132 551 318 832 682\n",
      "  74 196 837 566 965 546  63 577 443 539 584 270 891 305 212 993 222 900\n",
      " 986 985 343 246 261 828 552  33 387 439 150 543 156 396  55 279 787 873\n",
      " 385 293 748 176 390 668 393 899 540 509 563 200 854 677 187 749 646 229\n",
      " 603  84 449 695 850 202 967 521 299  88 147 115 574 669 312 180 560  48\n",
      " 236 414 532 231 308   8 459 416 186 665 304 322 875  39 973 235 750 674\n",
      "  14 472 722 793 803 505  52 227 363 785 162 160 673 701 580 423 624 223\n",
      "  59  73 373 340 422 404 262 721 627  43  51 659 476 280 825 136]\n"
     ]
    }
   ],
   "source": [
    "print(\" -- VALIDACIÓN CRUZADA CON GERMAN DATA K=4 -- \")\n",
    "estrategia = ValidacionCruzada(4)\n",
    "estrategia.creaParticiones(dataset3)\n",
    "for particion in estrategia.particiones:\n",
    "    print(particion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Apartado 2: Naive-Bayes</h3>\n",
    "<p>Es un clasificador de datos que se basa en la regla de Bayes. Donde primero vamos a dividir el conjunto de datos y poder particionarlo en dos subconunto de datos con validación simple o en varios subconjunto de datos con validación cruzada. En el código que mostramos a continuación es lo que van hacer todos los clasificadores que podemos implementar. Los métodos de entrenamiento y clasifica cada clasificador lo hace el clasificador debido a que los métodos que van a utilizar son únicos y los tenemos que implementar en las clases especificas de cada clasificador.</p>\n",
    "<p>El método <strong>error</strong> va a comprobar los errores que ha obtenido nuestro clasificador, para ver el error que hemos obtenido se hace comparando la última columna de nuestra matriz de datos con la predicción que hemos obtenido en el método de clasifica del clasificador. Si es dintinto la predicción con la última columna de los datos le sumamos uno y lo vamos a dividir entre el número de lineas que tiene el subconjunto de datos Test.</p>\n",
    "<p>El método <strong>validación</strong> lo que va hacer es realizar los métodos de entrenamiento, clasifica y calcula error del clasificador seguido sin ninguna interrupcion. El método va a comprobar si le estan pasando validación simple o cruzada con el número de partciones que tiene. Si el tamaño es igual a 1 va a llamar a los métodos mencionados anteriormente y va a devolver el error que ha obtenido tras el entrenamiento y la clasificación. Si el tamaño es mayor a 1 hacemos un bucle que cubra todas las partciones para entrenarlas, clasificarlas y obtener su error, despues de la realización de esos métodos vamos a hacer la media aritmetica de todos los errores obtenidos con las diferentes particiones que tenemos en nuestra estrategia.</p>\n",
    "<p>El método <strong>matrizConfusion</strong> genera la matriz de confusión utlizando el método confusion_matriz() de SKLearn y calcula las tasas de falsos negativos, verdaderos positivos, verdaderos negativos y falsos positivos, añadiendo las dos tasas necesarias para el análisis ROC a unas listas.</p>\n",
    "<p>El método <strong>curvaROC</strong> únicamente pinta la curva ROC correspondiente a la clasificación realizada, utliza los datos de las listas generadas en <strong>matrizConfusion</strong> para obtener los valores.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 945,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class Clasificador:\n",
    "  # Clase abstracta\n",
    "  __metaclass__ = ABCMeta\n",
    "\n",
    "  # Metodos abstractos que se implementan en casa clasificador concreto\n",
    "  @abstractmethod\n",
    "  # TODO: esta funcion debe ser implementada en cada clasificador concreto\n",
    "  # datosTrain: matriz numpy con los datos de entrenamiento\n",
    "  # atributosDiscretos: array bool con la indicatriz de los atributos nominales\n",
    "  # diccionario: array de diccionarios de la estructura Datos utilizados para la codificacion de variables discretas\n",
    "  def entrenamiento(self, datos, datosTrain, atributosDiscretos, diccionario):\n",
    "    pass\n",
    "\n",
    "  @abstractmethod\n",
    "  # TODO: esta funcion debe ser implementada en cada clasificador concreto\n",
    "  # devuelve un numpy array con las predicciones\n",
    "  def clasifica(self, datosTest, atributosDiscretos, diccionario):\n",
    "    pass\n",
    "\n",
    "  # Obtiene el numero de aciertos y errores para calcular la tasa de fallo\n",
    "  # TODO: implementar\n",
    "  def error(self, datos, pred):\n",
    "    # Aqui se compara la prediccion (pred) con las clases reales y se calcula el error\n",
    "    i = 0\n",
    "    real = datos[:, -1]\n",
    "    error = 0\n",
    "    for i in range(len(real)):\n",
    "      if real[i] != pred[i]:\n",
    "        error += 1\n",
    "    err = (error) / (len(real) + 0.0)\n",
    "    return err\n",
    "\n",
    "  # Realiza una clasificacion utilizando una estrategia de particionado determinada\n",
    "  # particionado : estrategia de validacion que queremos utilizar\n",
    "  # dataset : clase de tipo Datos que utilizamos para entrenar y clasificar el modelo\n",
    "  # clasificador: instancia del clasificador que se va a usar\n",
    "  # TODO: implementar esta funcion\n",
    "  def validacion(self, particionado, dataset, clasificador, seed: object = None):\n",
    "\n",
    "    # Creamos las particiones siguiendo la estrategia llamando a particionado.creaParticiones\n",
    "    # - Para validacion cruzada: en el bucle hasta nv entrenamos el clasificador con la particion de train i\n",
    "    # y obtenemos el error en la particion de test i\n",
    "    # - Para validacion simple (hold-out): entrenamos el clasificador con la particion de train\n",
    "    # y obtenemos el error en la particion test. Otra opci�n es repetir la validaci�n simple un n�mero especificado de veces, obteniendo en cada una un error. Finalmente se calcular�a la media.\n",
    "    errores = 0\n",
    "    # particionado.creaParticiones(dataset, seed)\n",
    "    # Comprobamos si es por validación cruzada o simple, por la longitud de la lista de particiones\n",
    "    particionado.particiones = []\n",
    "    particionado.creaParticiones(dataset)\n",
    "   \n",
    "    # Validación Simple\n",
    "    if len(particionado.particiones) == 1:\n",
    "      clasificador.entrenamiento(dataset, particionado.particiones[0].indicesTrain)\n",
    "      pred = clasificador.clasifica(dataset, particionado.particiones[0].indicesTest)\n",
    "      ret = self.error(dataset.extraeDatos(particionado.particiones[0].indicesTest), pred)\n",
    "      if ret > 0:\n",
    "        return ret\n",
    "      else:\n",
    "        return 0\n",
    "      \n",
    "    # Validación Cruzada\n",
    "    else:\n",
    "      lista_error = []\n",
    "      for particion in particionado.particiones:\n",
    "        clasificador.entrenamiento(dataset, particion.indicesTrain)\n",
    "        pred = clasificador.clasifica(dataset, particion.indicesTest)\n",
    "        ret = self.error(dataset.extraeDatos(particion.indicesTest), pred)\n",
    "        lista_error.append(ret)\n",
    "      return np.mean(lista_error),np.std(lista_error)\n",
    "\n",
    "\n",
    "  def matrizConfusion(self, dataset, datosTest, prediccion):\n",
    "\n",
    "    # Calculamos la matriz de confusion utlizando sk-learn. Solo se calcula en el caso de que la clasificacion sea binaria.\n",
    "    testData = dataset.extraeDatos(datosTest)\n",
    "    clase_real = testData[:, -1]\n",
    "\n",
    "    matriz = confusion_matrix(prediccion, clase_real)\n",
    "\n",
    "    # La funcion ravel() devuelve todas las estadisticas relacionadas con la matriz de confusion\n",
    "    tn, fp, fn, tp = matriz.ravel()\n",
    "\n",
    "\n",
    "    # Calculamos las tasas extraídas de la matriz de confusión\n",
    "    tpr = tp / (tp + fn)\n",
    "    fpr = fp / (fp + fn)\n",
    "\n",
    "    self.lista_tpr.append(tpr)\n",
    "    self.lista_fpr.append(fpr)\n",
    "\n",
    "    return matriz\n",
    "\n",
    "  def curvaROC(self):\n",
    "\n",
    "    x = np.linspace(0, 1, 100)\n",
    "    plt.plot(x, x, c='blue')\n",
    "    for i in range(len(self.lista_fpr)):\n",
    "        plt.plot(self.lista_fpr[i],self.lista_tpr[i],'ro')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Implementación del Clasificador Naive-Bayes</h3>\n",
    "<p>En la siguiente celda vamos a poder observar el codigo de entrenamiento y clasifica del clasificador de Naive-Bayes, a continuación explicaremos brevemente el funcionamiento de cada uno de los métodos especificos del clasificador. En este claisificador podemos aplicar la regla de Laplace, que es si obtenemos un cero en algunas de las mátrices de conteos de los datos, es decir, a la hora de calcular P(D|H), tendremos que sumar 1 a todas las celdas de conteos de esa P(D|H).</p>\n",
    "<p>El método <strong>entrenamiento</strong>, lo primero que hace este método es obtener las probabilidades a priori de las clases que hay en el subconjunto de Test. Al calcular esas probabbilades las introducimos en un diccionario para poder utilizarlas más tarde. Ahora vamos a calcular la P(D|H)que se va a calcular de diferentes maneras para datos continuos o discretos\n",
    "    <ol>\n",
    "        <li><strong>Atributos discretos</strong>: se calcula haciendo los conteos de las veces que sale la P(D|H) en el subconjunto de datos Train.</li>\n",
    "        <li><strong>Atributos continuos</strong>: se calcula la media y desviación tipica del subconjunto de datos Train.</li>\n",
    "    </ol>\n",
    "Todo esto se mete en una matriz que tiene los diferentes datos y las diferentes clases del conjunto de datos, donde esa matriz la vamos a utilizar para calcular las probabilidades a posteriori con todos las datos que tiene las matrices que hemos creado.</p>\n",
    "<p>El método <strong> clasifica</strong> dependiendo de los datos que vamos obteniendo del subconjunto de datos de Test, y vamos a obtener las diferentes probabilidades de las distintas clases que tenemos el problema si nos llega ese dato. Esas probabilidades las guardamos en una lista para luego multiplicarlas por los a priori y poder coger la clase que de más probabilidad para guardarla en la lista de las predicciones de nuestro clasificador Naive-Bayes y así despues obtener el error que hemos obtenenido. Este método tambien clasifica de manera distinta los atributos discretos y continuos:\n",
    "    <ol>\n",
    "        <li><strong>Atributos discretos</strong>: se calcula obteniendo el número que hay en la matriz de probabilidades a posteriori que hemos creado anteriormente.</li>\n",
    "        <li><strong>Atributos continuos</strong>: se calcula haciendo la ecuación de la distribución normal.\n",
    "   </ol>\n",
    " </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 946,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class ClasificadorNaiveBayes(Clasificador):\n",
    "\n",
    "  def __init__(self, laplace):\n",
    "    self.laplace = laplace\n",
    "    self.lista_fpr = []\n",
    "    self.lista_tpr = []\n",
    "\n",
    "  def entrenamiento(self, dataset, datosTrain):\n",
    "\n",
    "    # Cargamos todos los datos de la clase del dataset desde la matriz de datos\n",
    "    clasesTrain = dataset.extraeDatos(datosTrain)\n",
    "    self.numClases = clasesTrain[:, -1]\n",
    "\n",
    "    # Contamos las apariciones de cada uno para luego calcular la probabilidad a priori de cada clase\n",
    "    counter = Counter(self.numClases)\n",
    "\n",
    "    # Calculamos la probabilidad de la clase y lo metemos en un diccionario ordenado segun el numero\n",
    "    # correspondiente a cada clase asignado en el diccionario\n",
    "    self.dictPrioris = {}\n",
    "    for k in counter:\n",
    "      k = int(k)\n",
    "      counter[k] = counter[k] / len(self.numClases)\n",
    "      self.dictPrioris[k] = counter[k]\n",
    "\n",
    "    # Aqui ordenamos el diccionario para que esten en el mismo orden de como extraemos los datos del dataset\n",
    "    self.dictPrioris = SortedDict(self.dictPrioris)\n",
    "\n",
    "    # Calcular tablas de probabilidades del entrenamiento. Tenemos que calcular por cada atributo una cuenta\n",
    "    # de las apariciones en cada clase\n",
    "    # Creamos una lista de matrices, donde vamos almacenar todos los datos que hemos obtenido en los datos de Test\n",
    "    self.posteriori = np.zeros(len(dataset.nombreAtributos) - 1, dtype=object)\n",
    "\n",
    "    # Recorremos todos los datos de la matriz sin llegar a la clase\n",
    "    for i in range(len(dataset.nombreAtributos) - 1):\n",
    "\n",
    "      # Si el dato que obtenemos es Nominal haremos el recuento de todas las veces que sale la P(D|H)\n",
    "      if dataset.nominalAtributos[i] == True:\n",
    "\n",
    "        # Creamos una matriz de tamaño X: Número de Atributos menos la clase Y: Número de clases\n",
    "        post = np.zeros((len(dataset.listaDicts[i]), len(dataset.listaDicts[-1])))\n",
    "\n",
    "        # Aqui contamos todos las datos que queremos del datos Train para construir la matriz de entrenamiento\n",
    "        for c in range(len(dataset.listaDicts[-1])):\n",
    "          datosEnt = dataset.extraeDatos(datosTrain)\n",
    "          dat = datosEnt[:, i]\n",
    "          repes = Counter(dat[datosEnt[:, -1] == c])\n",
    "          for r in repes:\n",
    "            post[int(r), c] = repes[r]\n",
    "          if self.laplace == True:\n",
    "            self.posteriori[i] = post + 1\n",
    "          else:\n",
    "            self.posteriori[i] = post\n",
    "\n",
    "      # Si el dato es Continuo obtendremos la media y la desviación tipica de la clase\n",
    "      else:\n",
    "\n",
    "        # Creamos una matriz de X: Los datos de Media y Desivación típica Y: Número de clases\n",
    "        post = np.zeros((2, len(dataset.listaDicts[-1])))\n",
    "\n",
    "        # Aqui obtenemos la media y desviación tipica de cada clase, despues de tener los datos de entrenamiento\n",
    "        for c in range(len(dataset.listaDicts[-1])):\n",
    "          datosEnt = dataset.extraeDatos(datosTrain)\n",
    "          dat = datosEnt[:, i]\n",
    "          datos = dat[datosEnt[:, -1] == c]\n",
    "          post[0][c] = np.mean(datos)\n",
    "          post[1][c] = np.std(datos)\n",
    "        self.posteriori[i] = post\n",
    "\n",
    "\n",
    "    # Calculamos los valores de los posteriori de todos las tablas anteriores\n",
    "    for i in range(len(dataset.listaDicts) - 1):\n",
    "      if dataset.nominalAtributos[i] == True:\n",
    "        self.posteriori[i] /= sum(self.posteriori[i])\n",
    "\n",
    "  def clasifica(self, dataset, datosTest):\n",
    "    acum_probs = 1\n",
    "    self.prediccion = []\n",
    "    datTest = dataset.extraeDatos(datosTest)\n",
    "\n",
    "    # Ahora vamos a estudiar la probabilidad de la clase con los datos obtenidos en el entrenamiento\n",
    "    # Recorremos todos las datos de la matriz de los datos Test\n",
    "    for dato in datTest:\n",
    "      mapa = []\n",
    "      # Aqui obtenemos los prioris de cada clase para poder obtener la probabilidad de cada una\n",
    "      for clase in range(len(self.dictPrioris)):\n",
    "        listaVerosimilitudes = []\n",
    "        # Aqui obtenemos cada valor posteriori de nuestro entrenamiento de los datos, es decir, P(D|H)\n",
    "        for atributo in range(len(self.posteriori)):\n",
    "          if dataset.nominalAtributos[atributo] == True:\n",
    "            prob = self.posteriori[atributo][int(dato[atributo])][clase]\n",
    "            listaVerosimilitudes.append(prob)\n",
    "\n",
    "          # Aqui obtenemos la probabilidad de los atibutos continuos\n",
    "          else:\n",
    "            # Hacemos la formula de la distribucion normal\n",
    "            exp1 = 1 / (self.posteriori[atributo][1][clase] * math.sqrt(2 * math.pi))\n",
    "            exp2 = np.power((dato[atributo] - self.posteriori[atributo][0][clase]), 2)\n",
    "            exp3 = np.power(self.posteriori[atributo][1][clase], 2)\n",
    "            exp4 = exp2 / exp3\n",
    "            exp4 = math.exp((-1 / 2) * exp4)\n",
    "            prob = exp1 * exp4\n",
    "            listaVerosimilitudes.append(prob)\n",
    "\n",
    "        for verosimilitud in listaVerosimilitudes:\n",
    "          acum_probs *= verosimilitud\n",
    "        acum_probs *= self.dictPrioris.get(clase)\n",
    "        mapa.append(acum_probs)\n",
    "        acum_probs = 1\n",
    "\n",
    "      # Aqui obtenemos la predicción de mayor probabilidad y la guardamos en nuestra lista de predicciones\n",
    "      self.prediccion.append(np.argmax(mapa))\n",
    "\n",
    "\n",
    "    # Devolvemos la lista con la predicción de nuestro clasifica\n",
    "    return self.prediccion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>A continuación, vamos a mostrar una ejecución del claisificador de Naive-Bayes con las diferentes validaciones y los diferentes conjuntos de datos que tenemos, al final de esto mostraremos la probabilidad que hemos obtenido.</p>\n",
    "<p>Destacar que en el caso de <Strong>German.data</Strong> es en el único dataset en el que se realiza el cálculo de probabilidades utilizando también la distribución Normal, ya que cuenta con atributos continuos.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generamos las estrategias de validación de nuevo y se aplicaran para todos los casos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 947,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -- NAIVE-BAYES CON LENSES DATA -- \n",
      " -- CON LAPLACE Y VALIDACION SIMPLE -- \n",
      "ERROR OBTENIDO: 0.375\n",
      " -- SIN LAPLACE Y VALIDACION SIMPLE -- \n",
      "ERROR OBTENIDO: 0.125\n",
      "\n",
      " -- NAIVE-BAYES CON TIC-TAC-TOE DATA -- \n",
      " -- CON LAPLACE Y VALIDACION SIMPLE -- \n",
      "ERROR OBTENIDO: 0.2951388888888889\n",
      " -- SIN LAPLACE Y VALIDACION SIMPLE -- \n",
      "ERROR OBTENIDO: 0.3090277777777778\n",
      "\n",
      " -- NAIVE-BAYES CON GERMAN DATA -- \n",
      " -- CON LAPLACE Y VALIDACION SIMPLE -- \n",
      "ERROR OBTENIDO: 0.24\n",
      " -- SIN LAPLACE Y VALIDACION SIMPLE --\n",
      "ERROR OBTENIDO: 0.24333333333333335\n"
     ]
    }
   ],
   "source": [
    "estrategia_simple_1 = ValidacionSimple(0.7)\n",
    "\n",
    "estrategia_simple_2 = ValidacionSimple(0.7)\n",
    "\n",
    "estrategia_simple_3 = ValidacionSimple(0.7)\n",
    "\n",
    "print(\" -- NAIVE-BAYES CON LENSES DATA -- \")\n",
    "print(\" -- CON LAPLACE Y VALIDACION SIMPLE -- \")\n",
    "nb = ClasificadorNaiveBayes(True)\n",
    "error = nb.validacion(estrategia_simple_1,dataset1,nb)\n",
    "print(\"ERROR OBTENIDO:\",error)\n",
    "print(\" -- SIN LAPLACE Y VALIDACION SIMPLE -- \")\n",
    "nb = ClasificadorNaiveBayes(False)\n",
    "error = nb.validacion(estrategia_simple_1,dataset1,nb)\n",
    "print(\"ERROR OBTENIDO:\",error)\n",
    "print()\n",
    "print(\" -- NAIVE-BAYES CON TIC-TAC-TOE DATA -- \")\n",
    "print(\" -- CON LAPLACE Y VALIDACION SIMPLE -- \")\n",
    "nb = ClasificadorNaiveBayes(True)\n",
    "error = nb.validacion(estrategia_simple_2,dataset2,nb)\n",
    "print(\"ERROR OBTENIDO:\",error)\n",
    "print(\" -- SIN LAPLACE Y VALIDACION SIMPLE -- \")\n",
    "nb = ClasificadorNaiveBayes(False)\n",
    "error = nb.validacion(estrategia_simple_2,dataset2,nb)\n",
    "print(\"ERROR OBTENIDO:\",error)\n",
    "print()\n",
    "print(\" -- NAIVE-BAYES CON GERMAN DATA -- \")\n",
    "print(\" -- CON LAPLACE Y VALIDACION SIMPLE -- \")\n",
    "nb = ClasificadorNaiveBayes(True)\n",
    "error = nb.validacion(estrategia_simple_3,dataset3,nb)\n",
    "print(\"ERROR OBTENIDO:\",error)\n",
    "print(\" -- SIN LAPLACE Y VALIDACION SIMPLE --\")\n",
    "nb = ClasificadorNaiveBayes(False)\n",
    "error = nb.validacion(estrategia_simple_3,dataset3,nb)\n",
    "print(\"ERROR OBTENIDO:\",error)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 948,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -- NAIVE-BAYES CON LENSES DATA -- \n",
      " -- CON LAPLACE Y VALIDACION CRUZADA -- \n",
      "ERROR OBTENIDO: 0.25\n",
      "DESV. TIPICA OBTENIDA: 0.08333333333333333\n",
      " -- SIN LAPLACE Y VALIDACION CRUZADA -- \n",
      "ERROR OBTENIDO: 0.41666666666666663\n",
      "DESV. TIPICA OBTENIDA: 0.18633899812498247\n",
      "\n",
      " -- NAIVE-BAYES CON TIC-TAC-TOE DATA -- \n",
      " -- CON LAPLACE Y VALIDACION CRUZADA -- \n",
      "ERROR OBTENIDO: 0.2964260808926081\n",
      "DESV. TIPICA OBTENIDA: 0.026638048149289276\n",
      "-- SIN LAPLACE Y VALIDACION CRUZADA --\n",
      "ERROR OBTENIDO: 0.2860094142259414\n",
      "DESV. TIPICA OBTENIDA: 0.003312883050380025\n",
      "\n",
      "-- NAIVE-BAYES CON GERMAN DATA --\n",
      "-- CON LAPLACE Y VALIDACION CRUZADA --\n",
      "ERROR OBTENIDO: 0.244\n",
      "DESV. TIPICA OBTENIDA: 0.0316227766016838\n",
      "-- SIN LAPLACE Y VALIDACION CRUZADA --\n",
      "ERROR OBTENIDO: 0.254\n",
      "DESV. TIPICA OBTENIDA: 0.015362291495737219\n"
     ]
    }
   ],
   "source": [
    "estrategia_cruzada_1 = ValidacionCruzada(4)\n",
    "\n",
    "estrategia_cruzada_2 = ValidacionCruzada(4)\n",
    "\n",
    "estrategia_cruzada_3 = ValidacionCruzada(4)\n",
    "\n",
    "\n",
    "print(\" -- NAIVE-BAYES CON LENSES DATA -- \")\n",
    "print(\" -- CON LAPLACE Y VALIDACION CRUZADA -- \")\n",
    "nb = ClasificadorNaiveBayes(True)\n",
    "error, desv = nb.validacion(estrategia_cruzada_1,dataset1,nb)\n",
    "print(\"ERROR OBTENIDO:\",error)\n",
    "print(\"DESV. TIPICA OBTENIDA:\",desv)\n",
    "print(\" -- SIN LAPLACE Y VALIDACION CRUZADA -- \")\n",
    "nb = ClasificadorNaiveBayes(False)\n",
    "error, desv = nb.validacion(estrategia_cruzada_1,dataset1,nb)\n",
    "print(\"ERROR OBTENIDO:\",error)\n",
    "print(\"DESV. TIPICA OBTENIDA:\",desv)\n",
    "print()\n",
    "\n",
    "print(\" -- NAIVE-BAYES CON TIC-TAC-TOE DATA -- \")\n",
    "print(\" -- CON LAPLACE Y VALIDACION CRUZADA -- \")\n",
    "nb = ClasificadorNaiveBayes(True)\n",
    "error, desv = nb.validacion(estrategia_cruzada_2,dataset2,nb)\n",
    "print(\"ERROR OBTENIDO:\",error)\n",
    "print(\"DESV. TIPICA OBTENIDA:\",desv)\n",
    "print(\"-- SIN LAPLACE Y VALIDACION CRUZADA --\")\n",
    "nb = ClasificadorNaiveBayes(False)\n",
    "error, desv = nb.validacion(estrategia_cruzada_2,dataset2,nb)\n",
    "print(\"ERROR OBTENIDO:\",error)\n",
    "print(\"DESV. TIPICA OBTENIDA:\",desv)\n",
    "print()\n",
    "\n",
    "print(\"-- NAIVE-BAYES CON GERMAN DATA --\")\n",
    "print(\"-- CON LAPLACE Y VALIDACION CRUZADA --\")\n",
    "nb = ClasificadorNaiveBayes(True)\n",
    "error, desv = nb.validacion(estrategia_cruzada_3,dataset3,nb)\n",
    "print(\"ERROR OBTENIDO:\",error)\n",
    "print(\"DESV. TIPICA OBTENIDA:\",desv)\n",
    "print(\"-- SIN LAPLACE Y VALIDACION CRUZADA --\")\n",
    "nb = ClasificadorNaiveBayes(False)\n",
    "error, desv = nb.validacion(estrategia_cruzada_3,dataset3,nb)\n",
    "print(\"ERROR OBTENIDO:\",error)\n",
    "print(\"DESV. TIPICA OBTENIDA:\",desv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Apartado 3: Clasificador Naive Bayes con SKLearn</h3>\n",
    "<p>En este apartado se va a seguir el mismo procedimiento que para el anterior, primero realizaremos pruebas para la validación simple y cruzada usando SKlearn y posteriormente el entrenamiento de nuestro modelo y clasificación de los ejemplos definidos en la partición de test.</p>\n",
    "<p>Una indicación a tener en cuenta, hemos creado la validación cruzada utilizando la clase <strong>KFold</strong> pero posteriormente, se utiliza <strong>cross_val_score</strong> para entrenar y clasificar los ejemplos para validación cruzada. Es la manera más sencilla que hemos encontrado de hacerlo.</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 949,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold, cross_val_predict\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from EstrategiaParticionado import Particion\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "def validacion_simple_sklearn(dataset, porcentaje):\n",
    "    # Matriz con los atributos\n",
    "    X = dataset.datos[:, :-1]\n",
    "\n",
    "    # Array con las clases\n",
    "    y = dataset.datos[:, -1]\n",
    "\n",
    "    # Realizamos la divison en train-test, X_train es la partición sobre la que se va a entrenar e X_test sobre la que se va a clasificar\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=porcentaje, shuffle=True)\n",
    "\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "\n",
    "def validacion_cruzada_sklearn(dataset, k):\n",
    "    # Matriz con los atributos\n",
    "    X = dataset.datos[:, :-1]\n",
    "\n",
    "    # Array con las clases\n",
    "    y = dataset.datos[:, -1]\n",
    "\n",
    "    kf = KFold(n_splits=k, shuffle=True)\n",
    "\n",
    "    particiones = []\n",
    "\n",
    "    for train_index, test_index in kf.split(X, y):\n",
    "        particiones.append(Particion(train_index, test_index))\n",
    "\n",
    "    return particiones\n",
    "\n",
    "\n",
    "def nb_sklearn(x_train, y_train, x_test, tipo=\"Multinomial\", laplace=True):\n",
    "    if tipo == \"Gaussian\":\n",
    "        clf = GaussianNB()\n",
    "    elif tipo == \"Multinomial\":\n",
    "        if laplace == True:\n",
    "            clf = MultinomialNB(alpha=1.0, fit_prior=True, class_prior=None)\n",
    "        else:\n",
    "            clf = MultinomialNB(alpha=0.01, fit_prior=True, class_prior=None)\n",
    "    else:\n",
    "        print(\"Error, clasificador no valido. Utilizar GaussianNB o MultinomialNB\")\n",
    "        return\n",
    "\n",
    "    # Entrenamos el modelo\n",
    "    clf.fit(x_train, y_train)\n",
    "\n",
    "    # Clasificacion\n",
    "    prediccion = clf.predict(x_test)\n",
    "\n",
    "    return prediccion\n",
    "\n",
    "\n",
    "def nb_sklearn_validacion_cruzada(x_train, y_train, k, tipo=\"Multinomial\", laplace=True):\n",
    "    if tipo == \"Gaussian\":\n",
    "        clf = GaussianNB()\n",
    "\n",
    "    elif tipo == \"Multinomial\":\n",
    "        if laplace == True:\n",
    "            clf = MultinomialNB(alpha=1.0, fit_prior=True, class_prior=None)\n",
    "        else:\n",
    "            clf = MultinomialNB(alpha=0.01, fit_prior=True, class_prior=None)\n",
    "    else:\n",
    "        print(\"Error, clasificador no valido. Utilizar GaussianNB o MultinomialNB\")\n",
    "        return\n",
    "\n",
    "    acierto = cross_val_score(clf, x_train, y_train, cv=k)\n",
    "\n",
    "    return acierto, acierto.std()\n",
    "\n",
    "\n",
    "def matriz_confusion_sklearn(prediccion, clase_real):\n",
    "    matriz = confusion_matrix(clase_real, prediccion)\n",
    "    tn, fp, fn, tp = matriz.ravel()\n",
    "\n",
    "    # Calculamos las tasas extraídas de la matriz de confusión\n",
    "    tpr = tp / (tp + fn)\n",
    "    fpr = fp / (fp + fn)\n",
    "\n",
    "    return matriz, tpr, fpr\n",
    "\n",
    "\n",
    "def curvaROC_sklearn(fpr, tpr):\n",
    "    x = np.linspace(0, 1, 100)\n",
    "    plt.plot(x, x, c='blue')\n",
    "    plt.plot(fpr, tpr, 'ro')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def error(clases_predichas, clases_reales):\n",
    "\n",
    "    error =  1 - (np.sum(np.equal(clases_predichas, clases_reales)) / len(clases_predichas))\n",
    "\n",
    "    return error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Ejemplos de la validación simple y cruzada para todos los datasets, utilizando SKLearn : </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 950,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -- SKLEARN VALIDACIÓN SIMPLE 70% LENSES DATA -- \n",
      "TRAIN:\n",
      " [[2. 1. 1. 0.]\n",
      " [2. 0. 1. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [2. 1. 0. 0.]\n",
      " [0. 1. 1. 0.]\n",
      " [2. 0. 0. 1.]\n",
      " [0. 1. 1. 1.]\n",
      " [2. 1. 1. 1.]\n",
      " [1. 1. 1. 1.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 1. 1.]\n",
      " [1. 0. 1. 1.]\n",
      " [1. 0. 1. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [1. 1. 0. 1.]\n",
      " [0. 1. 0. 0.]]\n",
      "TEST:\n",
      " [[2. 0. 0. 0.]\n",
      " [2. 0. 1. 1.]\n",
      " [1. 1. 1. 0.]\n",
      " [0. 1. 0. 1.]\n",
      " [1. 1. 0. 0.]\n",
      " [2. 1. 0. 1.]\n",
      " [1. 0. 0. 1.]\n",
      " [0. 0. 1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(\" -- SKLEARN VALIDACIÓN SIMPLE 70% LENSES DATA -- \")\n",
    "x_train, x_test, y_train, y_test = validacion_simple_sklearn(dataset, 0.7)\n",
    "print(\"TRAIN:\\n\", x_train)\n",
    "print(\"TEST:\\n\", x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 951,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -- SKLEARN VALIDACIÓN CRUZADA K=5 LENSES DATA -- \n",
      "Train: [ 0  1  2  3  5  6  7  9 10 12 13 14 15 16 17 19 21 22 23]\n",
      "Test:  [ 4  8 11 18 20]\n",
      "Train: [ 0  2  3  4  5  7  8 11 12 13 14 15 16 17 18 20 21 22 23]\n",
      "Test:  [ 1  6  9 10 19]\n",
      "Train: [ 1  2  3  4  5  6  8  9 10 11 13 15 16 17 18 19 20 21 23]\n",
      "Test:  [ 0  7 12 14 22]\n",
      "Train: [ 0  1  3  4  6  7  8  9 10 11 12 14 15 16 18 19 20 22 23]\n",
      "Test:  [ 2  5 13 17 21]\n",
      "Train: [ 0  1  2  4  5  6  7  8  9 10 11 12 13 14 17 18 19 20 21 22]\n",
      "Test:  [ 3 15 16 23]\n"
     ]
    }
   ],
   "source": [
    "print(\" -- SKLEARN VALIDACIÓN CRUZADA K=5 LENSES DATA -- \")\n",
    "particiones = validacion_cruzada_sklearn(dataset,5)\n",
    "for particion in particiones:\n",
    "    print(particion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 952,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -- SKLEARN VALIDACIÓN SIMPLE 80% TIC-TAC-TOE DATA -- \n",
      "TRAIN:\n",
      " [[0. 1. 2. ... 0. 2. 2.]\n",
      " [0. 2. 1. ... 2. 2. 1.]\n",
      " [1. 0. 2. ... 2. 0. 1.]\n",
      " ...\n",
      " [2. 1. 1. ... 0. 2. 2.]\n",
      " [1. 2. 1. ... 1. 2. 0.]\n",
      " [2. 1. 0. ... 2. 1. 2.]]\n",
      "TEST:\n",
      " [[0. 1. 1. ... 1. 0. 2.]\n",
      " [2. 2. 1. ... 2. 0. 1.]\n",
      " [1. 2. 0. ... 1. 2. 2.]\n",
      " ...\n",
      " [1. 0. 2. ... 2. 2. 0.]\n",
      " [2. 0. 1. ... 0. 1. 1.]\n",
      " [1. 2. 1. ... 2. 2. 2.]]\n"
     ]
    }
   ],
   "source": [
    "print(\" -- SKLEARN VALIDACIÓN SIMPLE 80% TIC-TAC-TOE DATA -- \")\n",
    "x_train, x_test, y_train, y_test = validacion_simple_sklearn(dataset2, 0.8)\n",
    "print(\"TRAIN:\\n\", x_train)\n",
    "print(\"TEST:\\n\", x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 953,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -- SKLEARN VALIDACIÓN CRUZADA K=4 TIC-TAC-TOE DATA --\n",
      "Train: [  0   1   2   3   5   7   8   9  10  11  12  13  14  15  16  17  20  21\n",
      "  22  23  26  28  29  30  31  32  33  34  37  38  39  40  41  43  44  45\n",
      "  46  47  48  49  51  54  57  58  59  60  61  63  64  65  67  70  71  72\n",
      "  74  76  77  78  80  81  82  83  84  85  86  87  89  91  94  96  98  99\n",
      " 101 102 103 104 105 106 107 109 110 111 112 113 114 115 116 117 118 119\n",
      " 120 121 123 124 125 129 132 134 136 138 139 140 141 142 143 144 145 146\n",
      " 147 148 149 152 153 154 155 156 158 159 160 161 163 164 165 167 168 169\n",
      " 170 171 172 173 175 176 178 179 180 182 183 184 185 186 187 188 189 191\n",
      " 192 193 195 196 197 198 199 201 202 203 204 207 208 210 211 212 213 215\n",
      " 216 217 218 222 223 224 225 226 227 228 229 230 232 233 234 236 238 240\n",
      " 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 258 259\n",
      " 260 262 263 264 265 267 270 271 274 275 276 278 280 281 282 283 284 285\n",
      " 286 287 288 289 290 291 292 295 297 298 301 302 303 304 305 306 307 310\n",
      " 312 313 314 315 316 317 318 319 320 322 324 326 327 329 330 331 332 334\n",
      " 335 336 341 342 343 344 345 346 347 348 349 350 351 352 353 354 356 357\n",
      " 358 360 361 363 364 366 367 368 369 370 371 372 376 377 378 379 380 381\n",
      " 382 383 384 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400\n",
      " 401 402 404 405 406 407 409 410 411 412 413 414 415 416 417 418 419 420\n",
      " 421 422 423 424 425 426 427 429 432 433 435 436 437 438 439 440 441 442\n",
      " 443 444 445 447 448 449 450 451 452 453 455 457 458 459 461 462 463 464\n",
      " 465 467 468 471 475 478 479 480 482 483 485 489 490 491 492 493 494 495\n",
      " 497 498 499 500 501 502 504 505 506 507 509 511 514 516 517 518 519 521\n",
      " 522 523 524 525 526 527 528 529 530 531 533 535 536 539 540 542 543 546\n",
      " 547 548 549 551 553 554 555 557 559 560 561 564 565 566 568 570 572 573\n",
      " 574 575 576 577 579 581 582 583 584 585 589 590 591 593 594 595 596 598\n",
      " 599 601 603 605 606 608 609 610 612 614 617 618 619 623 624 625 627 629\n",
      " 630 632 633 636 637 638 639 640 644 645 646 647 648 649 650 651 652 655\n",
      " 656 657 658 660 661 662 663 664 665 667 668 669 670 671 672 675 676 677\n",
      " 678 679 680 681 682 689 690 691 693 695 697 698 699 700 702 703 704 705\n",
      " 706 707 708 709 710 711 713 715 717 718 719 720 723 724 725 726 729 730\n",
      " 731 732 733 735 737 738 739 741 742 743 745 746 747 748 750 751 753 754\n",
      " 755 756 757 759 760 761 762 765 766 769 770 771 772 773 775 777 778 779\n",
      " 781 782 783 784 785 787 788 789 790 791 792 794 795 797 798 800 801 802\n",
      " 804 806 807 808 810 811 812 815 816 817 818 820 821 822 823 824 825 826\n",
      " 827 828 829 830 831 833 834 835 836 837 839 840 841 842 843 844 846 848\n",
      " 850 851 852 854 855 856 857 858 859 861 863 864 865 866 867 869 870 871\n",
      " 872 873 874 876 877 878 879 880 881 882 883 884 885 886 887 889 890 891\n",
      " 893 894 895 896 897 898 899 900 901 902 903 905 906 907 908 909 910 912\n",
      " 913 914 915 916 917 919 920 922 923 924 926 928 929 930 931 932 933 935\n",
      " 936 937 938 939 942 943 944 945 947 949 950 952 953 955 956 957]\n",
      "Test:  [  4   6  18  19  24  25  27  35  36  42  50  52  53  55  56  62  66  68\n",
      "  69  73  75  79  88  90  92  93  95  97 100 108 122 126 127 128 130 131\n",
      " 133 135 137 150 151 157 162 166 174 177 181 190 194 200 205 206 209 214\n",
      " 219 220 221 231 235 237 239 257 261 266 268 269 272 273 277 279 293 294\n",
      " 296 299 300 308 309 311 321 323 325 328 333 337 338 339 340 355 359 362\n",
      " 365 373 374 375 385 403 408 428 430 431 434 446 454 456 460 466 469 470\n",
      " 472 473 474 476 477 481 484 486 487 488 496 503 508 510 512 513 515 520\n",
      " 532 534 537 538 541 544 545 550 552 556 558 562 563 567 569 571 578 580\n",
      " 586 587 588 592 597 600 602 604 607 611 613 615 616 620 621 622 626 628\n",
      " 631 634 635 641 642 643 653 654 659 666 673 674 683 684 685 686 687 688\n",
      " 692 694 696 701 712 714 716 721 722 727 728 734 736 740 744 749 752 758\n",
      " 763 764 767 768 774 776 780 786 793 796 799 803 805 809 813 814 819 832\n",
      " 838 845 847 849 853 860 862 868 875 888 892 904 911 918 921 925 927 934\n",
      " 940 941 946 948 951 954]\n",
      "Train: [  0   2   3   4   5   6   8   9  10  12  13  17  18  19  22  23  24  25\n",
      "  27  28  29  31  32  33  34  35  36  39  40  41  42  43  45  46  48  50\n",
      "  51  52  53  54  55  56  57  58  60  62  64  65  66  68  69  70  71  72\n",
      "  73  75  76  77  78  79  82  83  84  85  88  89  90  91  92  93  95  97\n",
      "  98 100 102 103 104 105 106 108 109 110 111 112 115 117 118 119 120 122\n",
      " 123 125 126 127 128 130 131 132 133 134 135 136 137 138 141 142 144 146\n",
      " 147 150 151 153 154 155 156 157 158 159 160 161 162 163 164 165 166 169\n",
      " 170 171 174 175 176 177 179 180 181 183 184 185 186 187 188 190 191 194\n",
      " 195 196 198 199 200 201 202 203 204 205 206 207 209 211 212 213 214 215\n",
      " 216 217 219 220 221 222 224 225 226 227 229 231 232 235 237 238 239 241\n",
      " 243 244 245 247 248 249 251 252 253 254 255 257 258 260 261 262 263 264\n",
      " 265 266 267 268 269 270 271 272 273 274 276 277 278 279 280 281 284 285\n",
      " 287 288 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306\n",
      " 307 308 309 310 311 313 314 317 318 320 321 322 323 324 325 326 327 328\n",
      " 329 330 331 332 333 334 336 337 338 339 340 341 342 344 345 348 350 351\n",
      " 353 355 356 357 358 359 361 362 365 367 368 369 370 371 372 373 374 375\n",
      " 376 378 379 381 382 383 384 385 386 387 388 389 392 393 394 396 397 400\n",
      " 401 402 403 405 406 407 408 409 411 412 413 414 415 417 418 421 422 423\n",
      " 424 427 428 429 430 431 432 434 435 436 437 438 439 441 443 444 446 447\n",
      " 448 449 452 453 454 456 457 460 462 463 464 465 466 468 469 470 471 472\n",
      " 473 474 475 476 477 478 480 481 482 484 485 486 487 488 489 490 491 496\n",
      " 498 501 503 504 506 508 510 512 513 514 515 516 517 518 519 520 523 524\n",
      " 525 526 530 532 533 534 536 537 538 540 541 542 544 545 546 548 550 552\n",
      " 553 554 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571\n",
      " 574 575 576 577 578 579 580 581 585 586 587 588 589 590 591 592 595 597\n",
      " 598 599 600 601 602 603 604 606 607 608 609 611 613 614 615 616 618 619\n",
      " 620 621 622 623 626 627 628 629 630 631 632 633 634 635 636 637 638 639\n",
      " 640 641 642 643 644 645 647 648 649 650 653 654 657 659 660 662 663 666\n",
      " 668 670 672 673 674 675 677 678 680 681 683 684 685 686 687 688 689 690\n",
      " 692 693 694 696 697 698 699 700 701 702 703 704 706 707 711 712 713 714\n",
      " 715 716 717 718 719 721 722 723 724 727 728 730 731 732 734 735 736 740\n",
      " 741 742 743 744 745 749 750 752 754 755 756 757 758 761 762 763 764 766\n",
      " 767 768 771 773 774 775 776 778 779 780 781 782 783 785 786 787 788 790\n",
      " 791 793 796 797 798 799 803 804 805 806 807 808 809 810 811 812 813 814\n",
      " 815 816 817 819 820 821 822 823 824 825 826 827 828 829 830 831 832 833\n",
      " 834 835 836 837 838 839 843 844 845 846 847 848 849 850 851 852 853 855\n",
      " 858 859 860 862 863 864 865 866 867 868 869 871 873 874 875 876 877 879\n",
      " 880 881 882 885 887 888 889 890 892 898 899 900 903 904 906 908 909 910\n",
      " 911 914 915 917 918 919 921 924 925 926 927 929 930 931 932 934 935 936\n",
      " 937 938 939 940 941 942 943 946 947 948 949 950 951 953 954 955]\n",
      "Test:  [  1   7  11  14  15  16  20  21  26  30  37  38  44  47  49  59  61  63\n",
      "  67  74  80  81  86  87  94  96  99 101 107 113 114 116 121 124 129 139\n",
      " 140 143 145 148 149 152 167 168 172 173 178 182 189 192 193 197 208 210\n",
      " 218 223 228 230 233 234 236 240 242 246 250 256 259 275 282 283 286 289\n",
      " 290 312 315 316 319 335 343 346 347 349 352 354 360 363 364 366 377 380\n",
      " 390 391 395 398 399 404 410 416 419 420 425 426 433 440 442 445 450 451\n",
      " 455 458 459 461 467 479 483 492 493 494 495 497 499 500 502 505 507 509\n",
      " 511 521 522 527 528 529 531 535 539 543 547 549 551 555 572 573 582 583\n",
      " 584 593 594 596 605 610 612 617 624 625 646 651 652 655 656 658 661 664\n",
      " 665 667 669 671 676 679 682 691 695 705 708 709 710 720 725 726 729 733\n",
      " 737 738 739 746 747 748 751 753 759 760 765 769 770 772 777 784 789 792\n",
      " 794 795 800 801 802 818 840 841 842 854 856 857 861 870 872 878 883 884\n",
      " 886 891 893 894 895 896 897 901 902 905 907 912 913 916 920 922 923 928\n",
      " 933 944 945 952 956 957]\n",
      "Train: [  0   1   3   4   6   7   9  11  13  14  15  16  17  18  19  20  21  22\n",
      "  23  24  25  26  27  29  30  31  34  35  36  37  38  42  44  45  46  47\n",
      "  49  50  52  53  54  55  56  57  58  59  61  62  63  64  65  66  67  68\n",
      "  69  70  71  72  73  74  75  79  80  81  84  86  87  88  89  90  92  93\n",
      "  94  95  96  97  98  99 100 101 106 107 108 110 111 113 114 116 118 119\n",
      " 121 122 124 125 126 127 128 129 130 131 133 135 137 139 140 143 145 146\n",
      " 147 148 149 150 151 152 156 157 160 162 163 165 166 167 168 169 171 172\n",
      " 173 174 175 177 178 181 182 183 188 189 190 192 193 194 196 197 198 200\n",
      " 203 204 205 206 208 209 210 211 213 214 216 218 219 220 221 222 223 226\n",
      " 228 230 231 232 233 234 235 236 237 239 240 241 242 244 245 246 247 250\n",
      " 252 253 254 255 256 257 258 259 260 261 263 264 265 266 268 269 270 271\n",
      " 272 273 274 275 277 278 279 282 283 284 285 286 287 289 290 291 293 294\n",
      " 295 296 299 300 303 308 309 311 312 313 315 316 317 318 319 320 321 323\n",
      " 325 327 328 329 330 331 333 334 335 336 337 338 339 340 342 343 345 346\n",
      " 347 349 352 354 355 357 358 359 360 361 362 363 364 365 366 367 372 373\n",
      " 374 375 377 378 380 381 382 384 385 386 387 388 389 390 391 392 393 395\n",
      " 397 398 399 400 401 403 404 407 408 410 411 412 413 414 416 418 419 420\n",
      " 425 426 427 428 430 431 432 433 434 435 437 438 440 441 442 444 445 446\n",
      " 447 448 450 451 452 454 455 456 457 458 459 460 461 464 465 466 467 468\n",
      " 469 470 472 473 474 476 477 478 479 481 482 483 484 486 487 488 490 492\n",
      " 493 494 495 496 497 498 499 500 501 502 503 505 507 508 509 510 511 512\n",
      " 513 514 515 516 520 521 522 523 524 526 527 528 529 530 531 532 534 535\n",
      " 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 555 556\n",
      " 558 559 562 563 565 566 567 568 569 570 571 572 573 574 576 578 580 581\n",
      " 582 583 584 585 586 587 588 591 592 593 594 595 596 597 598 599 600 601\n",
      " 602 603 604 605 607 608 609 610 611 612 613 615 616 617 618 619 620 621\n",
      " 622 623 624 625 626 628 629 631 632 633 634 635 637 638 641 642 643 646\n",
      " 647 650 651 652 653 654 655 656 658 659 661 663 664 665 666 667 669 671\n",
      " 672 673 674 676 677 678 679 680 682 683 684 685 686 687 688 691 692 693\n",
      " 694 695 696 698 699 701 703 704 705 707 708 709 710 711 712 714 716 717\n",
      " 720 721 722 723 725 726 727 728 729 731 732 733 734 735 736 737 738 739\n",
      " 740 741 742 744 746 747 748 749 751 752 753 754 756 757 758 759 760 761\n",
      " 763 764 765 766 767 768 769 770 771 772 774 776 777 779 780 781 784 786\n",
      " 789 790 792 793 794 795 796 798 799 800 801 802 803 804 805 806 808 809\n",
      " 810 811 812 813 814 818 819 820 822 830 831 832 834 835 836 837 838 839\n",
      " 840 841 842 845 846 847 848 849 850 853 854 856 857 858 859 860 861 862\n",
      " 863 864 865 866 867 868 870 871 872 874 875 878 880 883 884 886 888 890\n",
      " 891 892 893 894 895 896 897 899 901 902 904 905 906 907 908 910 911 912\n",
      " 913 914 915 916 917 918 920 921 922 923 925 927 928 929 930 933 934 937\n",
      " 939 940 941 942 943 944 945 946 948 949 950 951 952 954 955 956 957]\n",
      "Test:  [  2   5   8  10  12  28  32  33  39  40  41  43  48  51  60  76  77  78\n",
      "  82  83  85  91 102 103 104 105 109 112 115 117 120 123 132 134 136 138\n",
      " 141 142 144 153 154 155 158 159 161 164 170 176 179 180 184 185 186 187\n",
      " 191 195 199 201 202 207 212 215 217 224 225 227 229 238 243 248 249 251\n",
      " 262 267 276 280 281 288 292 297 298 301 302 304 305 306 307 310 314 322\n",
      " 324 326 332 341 344 348 350 351 353 356 368 369 370 371 376 379 383 394\n",
      " 396 402 405 406 409 415 417 421 422 423 424 429 436 439 443 449 453 462\n",
      " 463 471 475 480 485 489 491 504 506 517 518 519 525 533 536 553 554 557\n",
      " 560 561 564 575 577 579 589 590 606 614 627 630 636 639 640 644 645 648\n",
      " 649 657 660 662 668 670 675 681 689 690 697 700 702 706 713 715 718 719\n",
      " 724 730 743 745 750 755 762 773 775 778 782 783 785 787 788 791 797 807\n",
      " 815 816 817 821 823 824 825 826 827 828 829 833 843 844 851 852 855 869\n",
      " 873 876 877 879 881 882 885 887 889 898 900 903 909 919 924 926 931 932\n",
      " 935 936 938 947 953]\n",
      "Train: [  1   2   4   5   6   7   8  10  11  12  14  15  16  18  19  20  21  24\n",
      "  25  26  27  28  30  32  33  35  36  37  38  39  40  41  42  43  44  47\n",
      "  48  49  50  51  52  53  55  56  59  60  61  62  63  66  67  68  69  73\n",
      "  74  75  76  77  78  79  80  81  82  83  85  86  87  88  90  91  92  93\n",
      "  94  95  96  97  99 100 101 102 103 104 105 107 108 109 112 113 114 115\n",
      " 116 117 120 121 122 123 124 126 127 128 129 130 131 132 133 134 135 136\n",
      " 137 138 139 140 141 142 143 144 145 148 149 150 151 152 153 154 155 157\n",
      " 158 159 161 162 164 166 167 168 170 172 173 174 176 177 178 179 180 181\n",
      " 182 184 185 186 187 189 190 191 192 193 194 195 197 199 200 201 202 205\n",
      " 206 207 208 209 210 212 214 215 217 218 219 220 221 223 224 225 227 228\n",
      " 229 230 231 233 234 235 236 237 238 239 240 242 243 246 248 249 250 251\n",
      " 256 257 259 261 262 266 267 268 269 272 273 275 276 277 279 280 281 282\n",
      " 283 286 288 289 290 292 293 294 296 297 298 299 300 301 302 304 305 306\n",
      " 307 308 309 310 311 312 314 315 316 319 321 322 323 324 325 326 328 332\n",
      " 333 335 337 338 339 340 341 343 344 346 347 348 349 350 351 352 353 354\n",
      " 355 356 359 360 362 363 364 365 366 368 369 370 371 373 374 375 376 377\n",
      " 379 380 383 385 390 391 394 395 396 398 399 402 403 404 405 406 408 409\n",
      " 410 415 416 417 419 420 421 422 423 424 425 426 428 429 430 431 433 434\n",
      " 436 439 440 442 443 445 446 449 450 451 453 454 455 456 458 459 460 461\n",
      " 462 463 466 467 469 470 471 472 473 474 475 476 477 479 480 481 483 484\n",
      " 485 486 487 488 489 491 492 493 494 495 496 497 499 500 502 503 504 505\n",
      " 506 507 508 509 510 511 512 513 515 517 518 519 520 521 522 525 527 528\n",
      " 529 531 532 533 534 535 536 537 538 539 541 543 544 545 547 549 550 551\n",
      " 552 553 554 555 556 557 558 560 561 562 563 564 567 569 571 572 573 575\n",
      " 577 578 579 580 582 583 584 586 587 588 589 590 592 593 594 596 597 600\n",
      " 602 604 605 606 607 610 611 612 613 614 615 616 617 620 621 622 624 625\n",
      " 626 627 628 630 631 634 635 636 639 640 641 642 643 644 645 646 648 649\n",
      " 651 652 653 654 655 656 657 658 659 660 661 662 664 665 666 667 668 669\n",
      " 670 671 673 674 675 676 679 681 682 683 684 685 686 687 688 689 690 691\n",
      " 692 694 695 696 697 700 701 702 705 706 708 709 710 712 713 714 715 716\n",
      " 718 719 720 721 722 724 725 726 727 728 729 730 733 734 736 737 738 739\n",
      " 740 743 744 745 746 747 748 749 750 751 752 753 755 758 759 760 762 763\n",
      " 764 765 767 768 769 770 772 773 774 775 776 777 778 780 782 783 784 785\n",
      " 786 787 788 789 791 792 793 794 795 796 797 799 800 801 802 803 805 807\n",
      " 809 813 814 815 816 817 818 819 821 823 824 825 826 827 828 829 832 833\n",
      " 838 840 841 842 843 844 845 847 849 851 852 853 854 855 856 857 860 861\n",
      " 862 868 869 870 872 873 875 876 877 878 879 881 882 883 884 885 886 887\n",
      " 888 889 891 892 893 894 895 896 897 898 900 901 902 903 904 905 907 909\n",
      " 911 912 913 916 918 919 920 921 922 923 924 925 926 927 928 931 932 933\n",
      " 934 935 936 938 940 941 944 945 946 947 948 951 952 953 954 956 957]\n",
      "Test:  [  0   3   9  13  17  22  23  29  31  34  45  46  54  57  58  64  65  70\n",
      "  71  72  84  89  98 106 110 111 118 119 125 146 147 156 160 163 165 169\n",
      " 171 175 183 188 196 198 203 204 211 213 216 222 226 232 241 244 245 247\n",
      " 252 253 254 255 258 260 263 264 265 270 271 274 278 284 285 287 291 295\n",
      " 303 313 317 318 320 327 329 330 331 334 336 342 345 357 358 361 367 372\n",
      " 378 381 382 384 386 387 388 389 392 393 397 400 401 407 411 412 413 414\n",
      " 418 427 432 435 437 438 441 444 447 448 452 457 464 465 468 478 482 490\n",
      " 498 501 514 516 523 524 526 530 540 542 546 548 559 565 566 568 570 574\n",
      " 576 581 585 591 595 598 599 601 603 608 609 618 619 623 629 632 633 637\n",
      " 638 647 650 663 672 677 678 680 693 698 699 703 704 707 711 717 723 731\n",
      " 732 735 741 742 754 756 757 761 766 771 779 781 790 798 804 806 808 810\n",
      " 811 812 820 822 830 831 834 835 836 837 839 846 848 850 858 859 863 864\n",
      " 865 866 867 871 874 880 890 899 906 908 910 914 915 917 929 930 937 939\n",
      " 942 943 949 950 955]\n"
     ]
    }
   ],
   "source": [
    "print(\" -- SKLEARN VALIDACIÓN CRUZADA K=4 TIC-TAC-TOE DATA --\")\n",
    "particiones = validacion_cruzada_sklearn(dataset2,4)\n",
    "for particion in particiones:\n",
    "    print(particion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 954,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -- SKLEARN VALIDACIÓN SIMPLE 75% GERMAN DATA -- \n",
      "TRAIN:\n",
      " [[ 0. 30.  0. ...  1.  0.  0.]\n",
      " [ 0. 12.  2. ...  1.  0.  0.]\n",
      " [ 0. 24.  1. ...  1.  1.  0.]\n",
      " ...\n",
      " [ 3. 36.  3. ...  2.  1.  0.]\n",
      " [ 3. 18.  4. ...  1.  0.  0.]\n",
      " [ 3. 48.  2. ...  1.  1.  0.]]\n",
      "TEST:\n",
      " [[ 3. 36.  2. ...  1.  1.  0.]\n",
      " [ 1. 24.  3. ...  1.  1.  0.]\n",
      " [ 3. 15.  3. ...  1.  0.  0.]\n",
      " ...\n",
      " [ 1. 12.  2. ...  1.  1.  0.]\n",
      " [ 1. 36.  3. ...  2.  1.  0.]\n",
      " [ 0. 48.  2. ...  1.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "print(\" -- SKLEARN VALIDACIÓN SIMPLE 75% GERMAN DATA -- \")\n",
    "x_train, x_test, y_train, y_test = validacion_simple_sklearn(dataset3, 0.75)\n",
    "print(\"TRAIN:\\n\", x_train)\n",
    "print(\"TEST:\\n\", x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 955,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -- SKLEARN VALIDACIÓN CRUZADA K=4 GERMAN DATA -- \n",
      "Train: [  0   1   3   4   6   7   8   9  10  11  12  16  18  20  21  22  24  25\n",
      "  26  27  29  30  31  32  33  34  35  36  37  38  39  40  42  43  44  46\n",
      "  47  48  50  51  53  55  56  57  58  59  60  61  62  63  64  65  67  68\n",
      "  69  72  73  74  77  78  79  82  84  86  87  91  92  93  94  95  96  97\n",
      "  99 100 101 103 105 106 107 108 109 110 112 113 114 116 117 118 119 122\n",
      " 123 124 126 127 129 130 131 132 133 135 136 138 139 140 141 143 144 145\n",
      " 146 147 148 149 150 151 152 154 155 156 157 158 159 161 162 163 165 166\n",
      " 168 170 171 172 175 177 181 182 183 184 185 186 187 188 189 190 191 192\n",
      " 193 194 195 196 197 198 199 200 201 202 204 205 207 208 209 211 212 213\n",
      " 214 215 216 219 222 223 224 225 226 229 230 231 232 233 234 235 237 238\n",
      " 242 244 245 246 247 248 250 251 252 253 255 256 257 258 259 260 261 262\n",
      " 263 265 266 267 268 269 271 272 273 274 275 276 277 278 279 280 282 283\n",
      " 285 286 287 288 289 290 293 294 295 297 299 300 302 303 304 305 308 309\n",
      " 310 311 312 313 316 318 320 322 323 324 325 326 327 329 330 332 334 335\n",
      " 336 337 338 339 340 341 342 343 344 345 346 349 350 352 353 354 356 357\n",
      " 358 360 362 363 364 367 368 369 370 373 374 375 376 377 378 379 380 381\n",
      " 382 384 385 386 387 388 391 392 394 396 397 399 401 402 403 404 405 408\n",
      " 411 412 413 415 416 417 418 419 420 421 422 423 424 425 426 428 429 430\n",
      " 431 432 433 435 436 438 441 442 443 444 445 447 449 450 451 452 454 455\n",
      " 456 458 461 462 463 465 466 467 468 469 470 471 473 475 476 477 479 480\n",
      " 481 483 485 486 487 488 489 490 492 493 494 495 497 498 500 501 503 505\n",
      " 506 507 508 509 510 512 513 514 515 516 517 518 519 520 522 523 525 527\n",
      " 528 529 530 531 532 533 534 535 536 537 538 542 543 546 547 548 550 552\n",
      " 553 554 555 556 557 558 559 560 562 564 565 566 567 568 569 573 574 575\n",
      " 576 577 578 579 581 584 585 586 587 588 589 591 592 593 596 597 598 600\n",
      " 602 603 604 605 606 607 609 610 612 615 616 617 618 619 620 621 622 623\n",
      " 625 627 628 629 630 631 632 633 634 635 636 638 640 641 642 643 644 645\n",
      " 647 649 650 651 652 654 655 656 657 659 661 662 663 664 665 666 667 669\n",
      " 670 671 672 673 674 675 676 677 678 680 681 682 683 684 685 686 687 688\n",
      " 689 690 692 694 696 697 699 702 703 704 708 709 710 712 713 714 717 718\n",
      " 719 721 722 723 724 725 726 728 729 730 731 732 734 735 736 737 738 739\n",
      " 740 742 743 744 745 746 747 748 750 751 752 753 754 755 756 757 758 762\n",
      " 763 764 765 767 768 772 773 775 776 777 779 781 782 784 785 786 787 788\n",
      " 789 790 791 792 793 794 795 796 797 798 799 800 801 802 806 807 808 809\n",
      " 810 812 813 814 815 818 820 821 822 823 825 826 827 828 829 830 831 833\n",
      " 835 836 837 838 839 840 842 843 844 845 846 849 850 851 853 855 857 858\n",
      " 859 860 861 862 863 864 865 866 869 871 873 874 875 876 877 878 880 881\n",
      " 883 884 885 886 887 890 891 893 894 896 898 899 900 901 902 904 905 906\n",
      " 907 909 910 913 914 915 916 919 924 925 927 929 930 931 933 934 935 936\n",
      " 937 939 940 941 942 943 944 945 947 948 949 950 951 952 953 955 956 958\n",
      " 961 962 965 966 968 969 970 971 973 974 976 977 978 979 983 984 985 986\n",
      " 987 988 989 990 991 993 994 995 996 997 998 999]\n",
      "Test:  [  2   5  13  14  15  17  19  23  28  41  45  49  52  54  66  70  71  75\n",
      "  76  80  81  83  85  88  89  90  98 102 104 111 115 120 121 125 128 134\n",
      " 137 142 153 160 164 167 169 173 174 176 178 179 180 203 206 210 217 218\n",
      " 220 221 227 228 236 239 240 241 243 249 254 264 270 281 284 291 292 296\n",
      " 298 301 306 307 314 315 317 319 321 328 331 333 347 348 351 355 359 361\n",
      " 365 366 371 372 383 389 390 393 395 398 400 406 407 409 410 414 427 434\n",
      " 437 439 440 446 448 453 457 459 460 464 472 474 478 482 484 491 496 499\n",
      " 502 504 511 521 524 526 539 540 541 544 545 549 551 561 563 570 571 572\n",
      " 580 582 583 590 594 595 599 601 608 611 613 614 624 626 637 639 646 648\n",
      " 653 658 660 668 679 691 693 695 698 700 701 705 706 707 711 715 716 720\n",
      " 727 733 741 749 759 760 761 766 769 770 771 774 778 780 783 803 804 805\n",
      " 811 816 817 819 824 832 834 841 847 848 852 854 856 867 868 870 872 879\n",
      " 882 888 889 892 895 897 903 908 911 912 917 918 920 921 922 923 926 928\n",
      " 932 938 946 954 957 959 960 963 964 967 972 975 980 981 982 992]\n",
      "Train: [  2   4   5   6   8   9  10  11  12  13  14  15  16  17  18  19  23  24\n",
      "  25  26  27  28  29  30  32  37  38  39  40  41  42  43  44  45  46  47\n",
      "  48  49  52  54  55  56  57  58  59  60  61  62  64  65  66  67  68  70\n",
      "  71  72  73  74  75  76  80  81  82  83  84  85  86  88  89  90  91  92\n",
      "  93  94  95  96  97  98  99 100 101 102 103 104 105 106 110 111 112 113\n",
      " 115 119 120 121 122 123 125 128 130 131 132 133 134 135 137 138 139 142\n",
      " 143 144 145 146 148 150 151 152 153 154 155 158 159 160 163 164 165 166\n",
      " 167 169 171 173 174 175 176 177 178 179 180 182 183 185 186 188 189 190\n",
      " 192 193 195 196 197 198 200 201 202 203 204 205 206 207 210 211 212 213\n",
      " 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 231 232 233\n",
      " 235 236 237 238 239 240 241 243 244 245 247 249 250 253 254 256 258 260\n",
      " 261 262 263 264 266 267 268 270 271 273 275 277 278 280 281 282 283 284\n",
      " 288 289 290 291 292 293 295 296 298 301 302 304 305 306 307 308 309 311\n",
      " 312 313 314 315 316 317 319 320 321 323 324 326 327 328 329 330 331 332\n",
      " 333 334 337 341 343 345 346 347 348 349 350 351 354 355 356 357 359 360\n",
      " 361 362 363 364 365 366 368 371 372 374 375 376 377 380 381 382 383 385\n",
      " 386 387 389 390 391 392 393 394 395 396 397 398 399 400 401 402 404 406\n",
      " 407 409 410 411 413 414 415 416 417 418 419 420 422 423 424 426 427 428\n",
      " 429 431 432 434 435 437 439 440 441 442 443 444 445 446 447 448 449 450\n",
      " 452 453 455 457 458 459 460 461 462 463 464 465 466 468 469 472 473 474\n",
      " 475 476 477 478 479 482 483 484 485 486 488 491 492 493 495 496 497 498\n",
      " 499 500 501 502 503 504 505 506 507 509 510 511 513 515 516 517 519 521\n",
      " 522 523 524 526 527 530 531 533 535 537 538 539 540 541 544 545 547 549\n",
      " 551 552 553 554 555 556 557 558 560 561 562 563 564 566 567 569 570 571\n",
      " 572 573 576 578 579 580 581 582 583 585 586 587 588 589 590 591 592 594\n",
      " 595 597 599 600 601 603 604 605 606 608 610 611 613 614 615 616 617 618\n",
      " 619 620 622 623 624 625 626 629 630 632 633 634 636 637 638 639 640 641\n",
      " 643 644 645 646 647 648 650 651 653 654 658 659 660 662 665 668 670 671\n",
      " 672 673 674 675 678 679 680 682 684 687 688 690 691 693 694 695 697 698\n",
      " 699 700 701 702 703 704 705 706 707 709 711 713 714 715 716 717 720 721\n",
      " 722 723 724 727 728 729 730 731 732 733 734 735 736 737 739 741 742 743\n",
      " 747 748 749 750 751 752 753 755 756 757 758 759 760 761 762 764 765 766\n",
      " 767 768 769 770 771 772 773 774 775 777 778 780 783 784 785 789 790 791\n",
      " 792 793 795 796 797 798 799 801 802 803 804 805 806 807 809 811 812 813\n",
      " 814 816 817 818 819 820 821 822 823 824 826 828 830 831 832 833 834 835\n",
      " 837 840 841 843 844 845 846 847 848 849 852 853 854 856 857 861 862 864\n",
      " 865 867 868 869 870 871 872 873 874 875 877 879 881 882 883 886 888 889\n",
      " 891 892 894 895 896 897 898 899 901 902 903 906 907 908 911 912 917 918\n",
      " 919 920 921 922 923 924 925 926 928 929 930 932 933 935 936 937 938 939\n",
      " 940 941 942 943 946 947 948 950 951 952 953 954 955 956 957 958 959 960\n",
      " 961 962 963 964 966 967 968 970 971 972 973 975 976 978 980 981 982 984\n",
      " 987 988 989 991 992 993 994 995 996 997 998 999]\n",
      "Test:  [  0   1   3   7  20  21  22  31  33  34  35  36  50  51  53  63  69  77\n",
      "  78  79  87 107 108 109 114 116 117 118 124 126 127 129 136 140 141 147\n",
      " 149 156 157 161 162 168 170 172 181 184 187 191 194 199 208 209 229 230\n",
      " 234 242 246 248 251 252 255 257 259 265 269 272 274 276 279 285 286 287\n",
      " 294 297 299 300 303 310 318 322 325 335 336 338 339 340 342 344 352 353\n",
      " 358 367 369 370 373 378 379 384 388 403 405 408 412 421 425 430 433 436\n",
      " 438 451 454 456 467 470 471 480 481 487 489 490 494 508 512 514 518 520\n",
      " 525 528 529 532 534 536 542 543 546 548 550 559 565 568 574 575 577 584\n",
      " 593 596 598 602 607 609 612 621 627 628 631 635 642 649 652 655 656 657\n",
      " 661 663 664 666 667 669 676 677 681 683 685 686 689 692 696 708 710 712\n",
      " 718 719 725 726 738 740 744 745 746 754 763 776 779 781 782 786 787 788\n",
      " 794 800 808 810 815 825 827 829 836 838 839 842 850 851 855 858 859 860\n",
      " 863 866 876 878 880 884 885 887 890 893 900 904 905 909 910 913 914 915\n",
      " 916 927 931 934 944 945 949 965 969 974 977 979 983 985 986 990]\n",
      "Train: [  0   1   2   3   5   6   7  10  12  13  14  15  17  19  20  21  22  23\n",
      "  25  27  28  29  30  31  32  33  34  35  36  37  38  39  41  42  44  45\n",
      "  47  48  49  50  51  52  53  54  56  57  59  60  62  63  64  65  66  67\n",
      "  68  69  70  71  72  73  74  75  76  77  78  79  80  81  82  83  84  85\n",
      "  86  87  88  89  90  94  95  96  97  98  99 102 103 104 106 107 108 109\n",
      " 110 111 114 115 116 117 118 120 121 122 124 125 126 127 128 129 131 134\n",
      " 135 136 137 138 139 140 141 142 144 147 148 149 150 151 152 153 154 155\n",
      " 156 157 158 159 160 161 162 164 166 167 168 169 170 172 173 174 175 176\n",
      " 178 179 180 181 182 184 185 187 188 189 191 194 195 196 198 199 200 201\n",
      " 203 204 205 206 208 209 210 211 215 216 217 218 220 221 222 225 226 227\n",
      " 228 229 230 233 234 236 237 238 239 240 241 242 243 244 246 248 249 250\n",
      " 251 252 254 255 256 257 259 260 264 265 266 269 270 271 272 273 274 275\n",
      " 276 277 278 279 280 281 282 284 285 286 287 289 290 291 292 294 296 297\n",
      " 298 299 300 301 302 303 304 305 306 307 310 311 313 314 315 316 317 318\n",
      " 319 321 322 323 325 326 328 331 333 335 336 338 339 340 341 342 343 344\n",
      " 345 346 347 348 349 350 351 352 353 354 355 356 358 359 360 361 362 364\n",
      " 365 366 367 368 369 370 371 372 373 375 376 378 379 382 383 384 387 388\n",
      " 389 390 392 393 394 395 396 397 398 399 400 403 404 405 406 407 408 409\n",
      " 410 412 413 414 417 421 424 425 426 427 428 429 430 431 432 433 434 436\n",
      " 437 438 439 440 445 446 447 448 451 453 454 455 456 457 459 460 464 465\n",
      " 467 470 471 472 473 474 476 477 478 480 481 482 483 484 487 489 490 491\n",
      " 492 493 494 496 497 499 501 502 504 508 510 511 512 513 514 517 518 519\n",
      " 520 521 524 525 526 527 528 529 531 532 534 535 536 539 540 541 542 543\n",
      " 544 545 546 547 548 549 550 551 553 554 557 559 561 563 565 567 568 569\n",
      " 570 571 572 574 575 577 579 580 582 583 584 585 586 587 588 589 590 593\n",
      " 594 595 596 597 598 599 600 601 602 607 608 609 610 611 612 613 614 620\n",
      " 621 622 623 624 626 627 628 630 631 632 635 636 637 639 640 642 646 647\n",
      " 648 649 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667\n",
      " 668 669 670 675 676 677 679 681 682 683 684 685 686 688 689 690 691 692\n",
      " 693 695 696 698 700 701 702 704 705 706 707 708 710 711 712 714 715 716\n",
      " 717 718 719 720 721 722 724 725 726 727 728 729 730 733 735 738 739 740\n",
      " 741 742 743 744 745 746 747 748 749 750 752 753 754 755 756 757 759 760\n",
      " 761 762 763 766 767 769 770 771 774 775 776 778 779 780 781 782 783 786\n",
      " 787 788 790 792 794 795 796 800 803 804 805 806 807 808 810 811 814 815\n",
      " 816 817 818 819 820 822 824 825 826 827 828 829 831 832 834 835 836 837\n",
      " 838 839 841 842 845 847 848 850 851 852 853 854 855 856 858 859 860 861\n",
      " 862 863 865 866 867 868 870 872 873 876 878 879 880 882 883 884 885 887\n",
      " 888 889 890 892 893 895 896 897 898 900 902 903 904 905 907 908 909 910\n",
      " 911 912 913 914 915 916 917 918 919 920 921 922 923 924 926 927 928 929\n",
      " 931 932 933 934 935 937 938 939 940 941 944 945 946 947 948 949 953 954\n",
      " 955 957 958 959 960 963 964 965 967 969 972 974 975 976 977 979 980 981\n",
      " 982 983 984 985 986 989 990 992 993 996 998 999]\n",
      "Test:  [  4   8   9  11  16  18  24  26  40  43  46  55  58  61  91  92  93 100\n",
      " 101 105 112 113 119 123 130 132 133 143 145 146 163 165 171 177 183 186\n",
      " 190 192 193 197 202 207 212 213 214 219 223 224 231 232 235 245 247 253\n",
      " 258 261 262 263 267 268 283 288 293 295 308 309 312 320 324 327 329 330\n",
      " 332 334 337 357 363 374 377 380 381 385 386 391 401 402 411 415 416 418\n",
      " 419 420 422 423 435 441 442 443 444 449 450 452 458 461 462 463 466 468\n",
      " 469 475 479 485 486 488 495 498 500 503 505 506 507 509 515 516 522 523\n",
      " 530 533 537 538 552 555 556 558 560 562 564 566 573 576 578 581 591 592\n",
      " 603 604 605 606 615 616 617 618 619 625 629 633 634 638 641 643 644 645\n",
      " 650 651 671 672 673 674 678 680 687 694 697 699 703 709 713 723 731 732\n",
      " 734 736 737 751 758 764 765 768 772 773 777 784 785 789 791 793 797 798\n",
      " 799 801 802 809 812 813 821 823 830 833 840 843 844 846 849 857 864 869\n",
      " 871 874 875 877 881 886 891 894 899 901 906 925 930 936 942 943 950 951\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 952 956 961 962 966 968 970 971 973 978 987 988 991 994 995 997]\n",
      "Train: [  0   1   2   3   4   5   7   8   9  11  13  14  15  16  17  18  19  20\n",
      "  21  22  23  24  26  28  31  33  34  35  36  40  41  43  45  46  49  50\n",
      "  51  52  53  54  55  58  61  63  66  69  70  71  75  76  77  78  79  80\n",
      "  81  83  85  87  88  89  90  91  92  93  98 100 101 102 104 105 107 108\n",
      " 109 111 112 113 114 115 116 117 118 119 120 121 123 124 125 126 127 128\n",
      " 129 130 132 133 134 136 137 140 141 142 143 145 146 147 149 153 156 157\n",
      " 160 161 162 163 164 165 167 168 169 170 171 172 173 174 176 177 178 179\n",
      " 180 181 183 184 186 187 190 191 192 193 194 197 199 202 203 206 207 208\n",
      " 209 210 212 213 214 217 218 219 220 221 223 224 227 228 229 230 231 232\n",
      " 234 235 236 239 240 241 242 243 245 246 247 248 249 251 252 253 254 255\n",
      " 257 258 259 261 262 263 264 265 267 268 269 270 272 274 276 279 281 283\n",
      " 284 285 286 287 288 291 292 293 294 295 296 297 298 299 300 301 303 306\n",
      " 307 308 309 310 312 314 315 317 318 319 320 321 322 324 325 327 328 329\n",
      " 330 331 332 333 334 335 336 337 338 339 340 342 344 347 348 351 352 353\n",
      " 355 357 358 359 361 363 365 366 367 369 370 371 372 373 374 377 378 379\n",
      " 380 381 383 384 385 386 388 389 390 391 393 395 398 400 401 402 403 405\n",
      " 406 407 408 409 410 411 412 414 415 416 418 419 420 421 422 423 425 427\n",
      " 430 433 434 435 436 437 438 439 440 441 442 443 444 446 448 449 450 451\n",
      " 452 453 454 456 457 458 459 460 461 462 463 464 466 467 468 469 470 471\n",
      " 472 474 475 478 479 480 481 482 484 485 486 487 488 489 490 491 494 495\n",
      " 496 498 499 500 502 503 504 505 506 507 508 509 511 512 514 515 516 518\n",
      " 520 521 522 523 524 525 526 528 529 530 532 533 534 536 537 538 539 540\n",
      " 541 542 543 544 545 546 548 549 550 551 552 555 556 558 559 560 561 562\n",
      " 563 564 565 566 568 570 571 572 573 574 575 576 577 578 580 581 582 583\n",
      " 584 590 591 592 593 594 595 596 598 599 601 602 603 604 605 606 607 608\n",
      " 609 611 612 613 614 615 616 617 618 619 621 624 625 626 627 628 629 631\n",
      " 633 634 635 637 638 639 641 642 643 644 645 646 648 649 650 651 652 653\n",
      " 655 656 657 658 660 661 663 664 666 667 668 669 671 672 673 674 676 677\n",
      " 678 679 680 681 683 685 686 687 689 691 692 693 694 695 696 697 698 699\n",
      " 700 701 703 705 706 707 708 709 710 711 712 713 715 716 718 719 720 723\n",
      " 725 726 727 731 732 733 734 736 737 738 740 741 744 745 746 749 751 754\n",
      " 758 759 760 761 763 764 765 766 768 769 770 771 772 773 774 776 777 778\n",
      " 779 780 781 782 783 784 785 786 787 788 789 791 793 794 797 798 799 800\n",
      " 801 802 803 804 805 808 809 810 811 812 813 815 816 817 819 821 823 824\n",
      " 825 827 829 830 832 833 834 836 838 839 840 841 842 843 844 846 847 848\n",
      " 849 850 851 852 854 855 856 857 858 859 860 863 864 866 867 868 869 870\n",
      " 871 872 874 875 876 877 878 879 880 881 882 884 885 886 887 888 889 890\n",
      " 891 892 893 894 895 897 899 900 901 903 904 905 906 908 909 910 911 912\n",
      " 913 914 915 916 917 918 920 921 922 923 925 926 927 928 930 931 932 934\n",
      " 936 938 942 943 944 945 946 949 950 951 952 954 956 957 959 960 961 962\n",
      " 963 964 965 966 967 968 969 970 971 972 973 974 975 977 978 979 980 981\n",
      " 982 983 985 986 987 988 990 991 992 994 995 997]\n",
      "Test:  [  6  10  12  25  27  29  30  32  37  38  39  42  44  47  48  56  57  59\n",
      "  60  62  64  65  67  68  72  73  74  82  84  86  94  95  96  97  99 103\n",
      " 106 110 122 131 135 138 139 144 148 150 151 152 154 155 158 159 166 175\n",
      " 182 185 188 189 195 196 198 200 201 204 205 211 215 216 222 225 226 233\n",
      " 237 238 244 250 256 260 266 271 273 275 277 278 280 282 289 290 302 304\n",
      " 305 311 313 316 323 326 341 343 345 346 349 350 354 356 360 362 364 368\n",
      " 375 376 382 387 392 394 396 397 399 404 413 417 424 426 428 429 431 432\n",
      " 445 447 455 465 473 476 477 483 492 493 497 501 510 513 517 519 527 531\n",
      " 535 547 553 554 557 567 569 579 585 586 587 588 589 597 600 610 620 622\n",
      " 623 630 632 636 640 647 654 659 662 665 670 675 682 684 688 690 702 704\n",
      " 714 717 721 722 724 728 729 730 735 739 742 743 747 748 750 752 753 755\n",
      " 756 757 762 767 775 790 792 795 796 806 807 814 818 820 822 826 828 831\n",
      " 835 837 845 853 861 862 865 873 883 896 898 902 907 919 924 929 933 935\n",
      " 937 939 940 941 947 948 953 955 958 976 984 989 993 996 998 999]\n"
     ]
    }
   ],
   "source": [
    "print(\" -- SKLEARN VALIDACIÓN CRUZADA K=4 GERMAN DATA -- \")\n",
    "particiones = validacion_cruzada_sklearn(dataset3,4)\n",
    "for particion in particiones:\n",
    "    print(particion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Ejemplos del clasificador Naive Bayes SKLearn - Validación Simple : </h3>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 969,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Lenses.data -- \n",
      "-- CON LAPLACE -- \n",
      "ERROR OBTENIDO: 0.25\n",
      "-- SIN LAPLACE -- \n",
      "ERROR OBTENIDO: 0.25\n"
     ]
    }
   ],
   "source": [
    "print(\"-- Lenses.data -- \")\n",
    "print(\"-- CON LAPLACE -- \")\n",
    "x_train, x_test, y_train, y_test = validacion_simple_sklearn(dataset1, 0.7)\n",
    "pred = nb_sklearn(x_train, y_train, x_test)\n",
    "errornb = error(pred, y_test)\n",
    "print(\"ERROR OBTENIDO:\", errornb)\n",
    "\n",
    "print(\"-- SIN LAPLACE -- \")\n",
    "pred = nb_sklearn(x_train, y_train, x_test)\n",
    "errornb = error(pred, y_test)\n",
    "print(\"ERROR OBTENIDO:\", errornb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 957,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Tic-tac-toe.data -- \n",
      "-- CON LAPLACE -- \n",
      "ERROR OBTENIDO: 0.36458333333333337\n",
      "-- SIN LAPLACE -- \n",
      "ERROR OBTENIDO: 0.36458333333333337\n"
     ]
    }
   ],
   "source": [
    "print(\"-- Tic-tac-toe.data -- \")\n",
    "print(\"-- CON LAPLACE -- \")\n",
    "x_train, x_test, y_train, y_test = validacion_simple_sklearn(dataset2, 0.8)\n",
    "pred = nb_sklearn(x_train, y_train, x_test)\n",
    "errornb = error(pred, y_test)\n",
    "print(\"ERROR OBTENIDO:\", errornb)\n",
    "\n",
    "print(\"-- SIN LAPLACE -- \")\n",
    "pred = nb_sklearn(x_train, y_train, x_test, \"Multinomial\",False)\n",
    "errornb = error(pred, y_test)\n",
    "print(\"ERROR OBTENIDO:\", errornb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 958,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -- German.data -- \n",
      "ERROR OBTENIDO: 0.256\n"
     ]
    }
   ],
   "source": [
    "print(\" -- German.data -- \")\n",
    "x_train, x_test, y_train, y_test = validacion_simple_sklearn(dataset3, 0.75)\n",
    "pred = nb_sklearn(x_train, y_train, x_test,\"Gaussian\")\n",
    "errornb = error(pred, y_test)\n",
    "print(\"ERROR OBTENIDO:\", errornb)\n",
    "\n",
    "# En este caso no hay distincion entre laplace o no, ya que el Gaussian no lo utiliza.\n",
    "# Utilizamos Gaussian porque tiene atributos Continuos\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Ejemplos del clasificador Naive Bayes SKLearn - Validación Cruzada : </h3>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 970,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Lenses.data -- \n",
      " -- CON LAPLACE -- \n",
      "ERROR OBTENIDO 0.3380952380952381\n",
      "DESV. TIPICA 0.04068573212055968\n",
      " -- SIN LAPLACE -- \n",
      "ERROR OBTENIDO 0.2607142857142857\n",
      "DESV. TIPICA 0.108868380455661\n"
     ]
    }
   ],
   "source": [
    "print(\"-- Lenses.data -- \")\n",
    "X = dataset.datos[:,:-1]\n",
    "y = dataset.datos[:,-1]\n",
    "\n",
    "print(\" -- CON LAPLACE -- \")\n",
    "aciertos, desv = nb_sklearn_validacion_cruzada(X, y, 4, \"Multinomial\")\n",
    "print(\"ERROR OBTENIDO\",1 - np.mean(aciertos))\n",
    "print(\"DESV. TIPICA\", desv)\n",
    "\n",
    "print(\" -- SIN LAPLACE -- \")\n",
    "aciertos, desv = nb_sklearn_validacion_cruzada(X, y, 4, \"Multinomial\", False)\n",
    "print(\"ERROR OBTENIDO\",1 - np.mean(aciertos))\n",
    "print(\"DESV. TIPICA\", desv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 960,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -- Tic-tac-toe.data -- \n",
      " -- CON LAPLACE -- \n",
      "ERROR OBTENIDO 0.3507017085076708\n",
      "DESV. TIPICA 0.014643056198736864\n",
      " -- SIN LAPLACE -- \n",
      "ERROR OBTENIDO 0.3507017085076708\n",
      "DESV. TIPICA 0.014643056198736864\n"
     ]
    }
   ],
   "source": [
    "print(\" -- Tic-tac-toe.data -- \")\n",
    "X = dataset2.datos[:,:-1]\n",
    "y = dataset2.datos[:,-1]\n",
    "\n",
    "print(\" -- CON LAPLACE -- \")\n",
    "aciertos, desv = nb_sklearn_validacion_cruzada(X, y, 4, \"Multinomial\")\n",
    "print(\"ERROR OBTENIDO\",1 - np.mean(aciertos))\n",
    "print(\"DESV. TIPICA\", desv)\n",
    "\n",
    "print(\" -- SIN LAPLACE -- \")\n",
    "aciertos, desv = nb_sklearn_validacion_cruzada(X, y, 4, \"Multinomial\", False)\n",
    "print(\"ERROR OBTENIDO\",1 - np.mean(aciertos))\n",
    "print(\"DESV. TIPICA\", desv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 961,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -- German.data --\n",
      "ERROR OBTENIDO 0.266\n",
      "DESV. TIPICA 0.024248711305964305\n"
     ]
    }
   ],
   "source": [
    "print(\" -- German.data --\")\n",
    "X = dataset3.datos[:,:-1]\n",
    "y = dataset3.datos[:,-1]\n",
    "\n",
    "aciertos, desv = nb_sklearn_validacion_cruzada(X, y, 4, \"Gaussian\")\n",
    "print(\"ERROR OBTENIDO\",1 - np.mean(aciertos))\n",
    "print(\"DESV. TIPICA\", desv)\n",
    "\n",
    "# De nuevo no hay distincio entre Laplace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong>Puntualización:</strong>\n",
    "<p>En el caso de \"german.data\" los atributos no son continuos todos ellos, para refinar la clasificación lo máximo posible sería necesario clasificar utilizando MultinomialNB los atributos discretos y GaussianNB clasificaría los continuos. En nuestro caso, no hemos llevado a cabo esa división por falta de tiempo, hemos utilizado la implementación de MultinomialNB(), como en los demás casos en los que solo hay atributos discretos.</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Apartado 4:Evaluación de hipótesis mediante Análisis ROC</h3>\n",
    "<p>La curva ROC es una representación gráfica de la sensibilidad a la especifidad de un clasificador, en esta práctica este análisis lo vamos a realizar del clasificador implementado que es Naive-Bayes. En este gráfico se representan los verdaderos postivos frente a los falsos positivos.</p>\n",
    "<p>Es una herramienta que nos proporciona la selección de modelos más óptimos y descartar los menos óptimos.</p>\n",
    "<p>A continuación, mostraremos la implementación que hemos realizado en para crear este análisis ROC:</p>\n",
    "<ol>\n",
    "    <li>El primer paso es crear la <strong> matriz de confusión</strong>, donde esta matriz la hemos utilizado con un método de la libreria de sklearn que nos dibuja la matriz de confusion para los datos que queremos del conjunto de datos. Despues de haber creado la matriz calculamos los valores de verdaderos positivos, falsos positivos, falsos negativos y verdaderos negativos y, por último, calculamos las tasas de la matriz de confusion y las guardamos en una lista. </li>\n",
    "    <li>En segundo lugar, debemos sacar la gráfica de la curva ROC. Esta gráfica lo sacamos con la libreria pyplot, mas concretamente, con matplotlib. Donde en el eje Y pondremos los valores TPR y en el eje de las X pondremos los valores de FPR.\n",
    "</li>\n",
    "</ol>\n",
    "\n",
    "La manera de ejecutar el clasificador varía con respecto a cuando solamente queremos obtener el error, debido a nuestra implementación de la matriz de confusión y la curva ROC.\n",
    "\n",
    "Vamos a generar nuevas particiones para que así pueda haber más variedad."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> CURVAS ROC - Clasificador Naive Bayes propio </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 972,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -- CURVA ROC TIC-TAC-TOE VAL. SIMPLE --\n",
      " -- MATRIZ DE CONFUSION -- \n",
      "[[ 41  28]\n",
      " [ 60 159]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\" -- CURVA ROC TIC-TAC-TOE VAL. SIMPLE --\")\n",
    "nb = ClasificadorNaiveBayes(True)\n",
    "nb.entrenamiento(dataset2,estrategia_simple_2.particiones[0].indicesTrain)\n",
    "pred = nb.clasifica(dataset2,estrategia_simple_2.particiones[0].indicesTest)\n",
    "matriz = nb.matrizConfusion(dataset2,estrategia_simple_2.particiones[0].indicesTest,pred)\n",
    "print(\" -- MATRIZ DE CONFUSION -- \")\n",
    "print(matriz)\n",
    "nb.curvaROC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 974,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -- CURVA ROC GERMAN VAL. SIMPLE --\n",
      " -- MATRIZ DE CONFUSION -- \n",
      "[[177  44]\n",
      " [ 31  48]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\" -- CURVA ROC GERMAN VAL. SIMPLE --\")\n",
    "nb = ClasificadorNaiveBayes(True)\n",
    "nb.entrenamiento(dataset3,estrategia_simple_3.particiones[0].indicesTrain)\n",
    "pred = nb.clasifica(dataset3,estrategia_simple_3.particiones[0].indicesTest)\n",
    "matriz = nb.matrizConfusion(dataset3,estrategia_simple_3.particiones[0].indicesTest,pred)\n",
    "print(\" -- MATRIZ DE CONFUSION -- \")\n",
    "print(matriz)\n",
    "nb.curvaROC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 983,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -- CURVA ROC TIC-TAC-TOE VAL. CRUZADA --\n",
      "MATRIZ DE CONFUSION 1\n",
      "[[ 36  18]\n",
      " [ 51 135]]\n",
      "MATRIZ DE CONFUSION 2\n",
      "[[ 41  30]\n",
      " [ 39 130]]\n",
      "MATRIZ DE CONFUSION 3\n",
      "[[ 36  17]\n",
      " [ 50 136]]\n",
      "MATRIZ DE CONFUSION 4\n",
      "[[ 27  18]\n",
      " [ 52 142]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\" -- CURVA ROC TIC-TAC-TOE VAL. CRUZADA --\")\n",
    "nb = ClasificadorNaiveBayes(True)\n",
    "\n",
    "i = 1\n",
    "for particion in estrategia_cruzada_2.particiones:\n",
    "    nb.entrenamiento(dataset2, particion.indicesTrain)\n",
    "    pred = nb.clasifica(dataset2, particion.indicesTest)\n",
    "    matriz = nb.matrizConfusion(dataset2, particion.indicesTest, pred)\n",
    "    print(\"MATRIZ DE CONFUSION\" , str(i))\n",
    "    print(matriz)\n",
    "    i += 1\n",
    "nb.curvaROC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 982,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -- CURVA ROC GERMAN VAL. CRUZADA --\n",
      "MATRIZ DE CONFUSION 1\n",
      "[[153  41]\n",
      " [ 20  36]]\n",
      "MATRIZ DE CONFUSION 2\n",
      "[[141  46]\n",
      " [ 24  39]]\n",
      "MATRIZ DE CONFUSION 3\n",
      "[[156  29]\n",
      " [ 29  36]]\n",
      "MATRIZ DE CONFUSION 4\n",
      "[[146  35]\n",
      " [ 31  38]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\" -- CURVA ROC GERMAN VAL. CRUZADA --\")\n",
    "nb = ClasificadorNaiveBayes(True)\n",
    "i = 1\n",
    "for particion in estrategia_cruzada_3.particiones:\n",
    "    nb.entrenamiento(dataset3, particion.indicesTrain)\n",
    "    pred = nb.clasifica(dataset3, particion.indicesTest)\n",
    "    matriz = nb.matrizConfusion(dataset3, particion.indicesTest, pred)\n",
    "    print(\"MATRIZ DE CONFUSION\" , str(i))\n",
    "    print(matriz)\n",
    "    i += 1\n",
    "nb.curvaROC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 992,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -- ANÁLISIS ROC GERMAN SKLEARN VAL. SIMPLE -- \n",
      "MATRIZ CONFUSION\n",
      "[[ 15  53]\n",
      " [  0 124]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\" -- ANÁLISIS ROC GERMAN SKLEARN VAL. SIMPLE -- \")\n",
    "X_train, X_test, y_train, y_test = validacion_simple_sklearn(dataset2, 0.8)\n",
    "pred =  nb_sklearn(X_train, y_train, X_test, \"Gaussian\")\n",
    "\n",
    "matriz, tpr, fpr = matriz_confusion_sklearn(pred,y_test)\n",
    "print(\"MATRIZ CONFUSION\")\n",
    "print(matriz)\n",
    "curvaROC_sklearn(fpr,tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 991,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -- ANÁLISIS ROC TIC-TAC-TOE SKLEARN VAL. SIMPLE -- \n",
      "MATRIZ CONFUSION\n",
      "[[105  35]\n",
      " [ 32  28]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print (\" -- ANÁLISIS ROC TIC-TAC-TOE SKLEARN VAL. SIMPLE -- \")\n",
    "X_train, X_test, y_train, y_test = validacion_simple_sklearn(dataset3, 0.8)\n",
    "pred =  nb_sklearn(X_train, y_train, X_test)\n",
    "\n",
    "matriz, tpr, fpr = matriz_confusion_sklearn(pred,y_test)\n",
    "print(\"MATRIZ CONFUSION\")\n",
    "print(matriz)\n",
    "curvaROC_sklearn(fpr,tpr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solo se han realizado análisis ROC para el caso de los datasets \"tic-tac-toe\" y \"german\", ya que la clasificación en ambos casos es binaria, mientras que el dataset \"lenses.data\" clasifica entre 3 clases distintas.\n",
    "\n",
    "En todos los casos se ha utilizado la corrección de Laplace, ya que generamos menos gráficas y los valores aplicando Laplace en \"German.data\" y \"Tic-tac-toe.data\" son muy similares que en el caso de no aplicarlos.\n",
    "\n",
    "Para el caso de SKLearn, solamente se ha analizado utilizando validación simple, ya que para el caso de validación cruzada, no hemos visto ninguna función que nos de la predicción por partición y sería necesario para estimar correctamente las tasas de falsos positivos y verdaderos positivos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> NAIVE BAYES PROPIO - VAL. SIMPLE </h4>\n",
    "<br></br>\n",
    "<style type=\"text/css\">\n",
    ".tg  {border-collapse:collapse;border-spacing:0;border-color:#ccc;}\n",
    ".tg td{font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:#ccc;color:#333;background-color:#fff;}\n",
    ".tg th{font-family:Arial, sans-serif;font-size:14px;font-weight:normal;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:#ccc;color:#333;background-color:#f0f0f0;}\n",
    ".tg .tg-c3ow{border-color:inherit;text-align:center;vertical-align:top}\n",
    ".tg .tg-7btt{font-weight:bold;border-color:inherit;text-align:center;vertical-align:top}\n",
    ".tg .tg-uzvj{font-weight:bold;border-color:inherit;text-align:center;vertical-align:middle}\n",
    "</style>\n",
    "<table class=\"tg\">\n",
    "  <tr>\n",
    "    <th class=\"tg-7btt\">DATOS<br></th>\n",
    "    <th class=\"tg-7btt\">LAPLACE</th>\n",
    "    <th class=\"tg-7btt\">ERROR</th>\n",
    "    <th class=\"tg-7btt\">DESV. TÍPICA</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-uzvj\" rowspan=\"2\">Lenses</td>\n",
    "    <td class=\"tg-c3ow\">Si</td>\n",
    "    <td class=\"tg-c3ow\">0.375</td>\n",
    "    <td class=\"tg-c3ow\">-</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-c3ow\">No</td>\n",
    "    <td class=\"tg-c3ow\">0.125</td>\n",
    "    <td class=\"tg-c3ow\">-</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-uzvj\" rowspan=\"2\">Tic-Tac-Toe</td>\n",
    "    <td class=\"tg-c3ow\">Si</td>\n",
    "    <td class=\"tg-c3ow\">0.295<br></td>\n",
    "    <td class=\"tg-c3ow\">-</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-c3ow\">No</td>\n",
    "    <td class=\"tg-c3ow\">0.309</td>\n",
    "    <td class=\"tg-c3ow\">-</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-uzvj\" rowspan=\"2\">German</td>\n",
    "    <td class=\"tg-c3ow\">Si</td>\n",
    "    <td class=\"tg-c3ow\">0.24</td>\n",
    "    <td class=\"tg-c3ow\">-</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-c3ow\">No</td>\n",
    "    <td class=\"tg-c3ow\">0.243<br></td>\n",
    "    <td class=\"tg-c3ow\">-</td>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>NAIVE BAYES PROPIO - VAL. CRUZADA</h4>\n",
    "<br></br>\n",
    "<style type=\"text/css\">\n",
    ".tg  {border-collapse:collapse;border-spacing:0;border-color:#ccc;}\n",
    ".tg td{font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:#ccc;color:#333;background-color:#fff;}\n",
    ".tg th{font-family:Arial, sans-serif;font-size:14px;font-weight:normal;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:#ccc;color:#333;background-color:#f0f0f0;}\n",
    ".tg .tg-c3ow{border-color:inherit;text-align:center;vertical-align:top}\n",
    ".tg .tg-7btt{font-weight:bold;border-color:inherit;text-align:center;vertical-align:top}\n",
    ".tg .tg-uzvj{font-weight:bold;border-color:inherit;text-align:center;vertical-align:middle}\n",
    "</style>\n",
    "<table class=\"tg\">\n",
    "  <tr>\n",
    "    <th class=\"tg-7btt\">DATOS<br></th>\n",
    "    <th class=\"tg-7btt\">LAPLACE</th>\n",
    "    <th class=\"tg-7btt\">ERROR</th>\n",
    "    <th class=\"tg-7btt\">DESV. TÍPICA</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-uzvj\" rowspan=\"2\">Lenses</td>\n",
    "    <td class=\"tg-c3ow\">Si</td>\n",
    "    <td class=\"tg-c3ow\">0.25</td>\n",
    "    <td class=\"tg-c3ow\">0.083<br></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-c3ow\">No</td>\n",
    "    <td class=\"tg-c3ow\">0.417<br></td>\n",
    "    <td class=\"tg-c3ow\">0.186<br></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-uzvj\" rowspan=\"2\">Tic-Tac-Toe</td>\n",
    "    <td class=\"tg-c3ow\">Si</td>\n",
    "    <td class=\"tg-c3ow\">0.296</td>\n",
    "    <td class=\"tg-c3ow\">0.027<br></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-c3ow\">No</td>\n",
    "    <td class=\"tg-c3ow\">0.286</td>\n",
    "    <td class=\"tg-c3ow\">0.003<br></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-uzvj\" rowspan=\"2\">German</td>\n",
    "    <td class=\"tg-c3ow\">Si</td>\n",
    "    <td class=\"tg-c3ow\">0.244</td>\n",
    "    <td class=\"tg-c3ow\">0.032<br></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-c3ow\">No</td>\n",
    "    <td class=\"tg-c3ow\">0.254</td>\n",
    "    <td class=\"tg-c3ow\">0.015<br></td>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>NAIVE BAYES SKLEARN - VALIDACION SIMPLE </h4>\n",
    "<br></br>\n",
    "<style type=\"text/css\">\n",
    ".tg  {border-collapse:collapse;border-spacing:0;border-color:#ccc;}\n",
    ".tg td{font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:#ccc;color:#333;background-color:#fff;}\n",
    ".tg th{font-family:Arial, sans-serif;font-size:14px;font-weight:normal;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:#ccc;color:#333;background-color:#f0f0f0;}\n",
    ".tg .tg-c3ow{border-color:inherit;text-align:center;vertical-align:top}\n",
    ".tg .tg-7btt{font-weight:bold;border-color:inherit;text-align:center;vertical-align:top}\n",
    ".tg .tg-uzvj{font-weight:bold;border-color:inherit;text-align:center;vertical-align:middle}\n",
    "</style>\n",
    "<table class=\"tg\">\n",
    "  <tr>\n",
    "    <th class=\"tg-7btt\">DATOS<br></th>\n",
    "    <th class=\"tg-7btt\">LAPLACE</th>\n",
    "    <th class=\"tg-7btt\">ERROR</th>\n",
    "    <th class=\"tg-7btt\">DESV. TÍPICA</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-uzvj\" rowspan=\"2\">Lenses</td>\n",
    "    <td class=\"tg-c3ow\">Si</td>\n",
    "    <td class=\"tg-c3ow\">0.25</td>\n",
    "    <td class=\"tg-c3ow\">-<br></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-c3ow\">No</td>\n",
    "    <td class=\"tg-c3ow\">0.25<br></td>\n",
    "    <td class=\"tg-c3ow\">-<br></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-uzvj\" rowspan=\"2\">Tic-Tac-Toe</td>\n",
    "    <td class=\"tg-c3ow\">Si</td>\n",
    "    <td class=\"tg-c3ow\">0.365</td>\n",
    "    <td class=\"tg-c3ow\">-<br></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-c3ow\">No</td>\n",
    "    <td class=\"tg-c3ow\">0.365</td>\n",
    "    <td class=\"tg-c3ow\">-<br></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-uzvj\" rowspan=\"2\">German</td>\n",
    "    <td class=\"tg-c3ow\">Si</td>\n",
    "    <td class=\"tg-c3ow\">0.256</td>\n",
    "    <td class=\"tg-c3ow\">-<br></td>\n",
    "  </tr>\n",
    "    <tr>\n",
    "    <td class=\"tg-c3ow\">No</td>\n",
    "    <td class=\"tg-c3ow\">-</td>\n",
    "    <td class=\"tg-c3ow\">-<br></td>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>NAIVE BAYES SKLEARN - VALIDACION CRUZADA </h4>\n",
    "<br></br>\n",
    "<style type=\"text/css\">\n",
    ".tg  {border-collapse:collapse;border-spacing:0;border-color:#ccc;}\n",
    ".tg td{font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:#ccc;color:#333;background-color:#fff;}\n",
    ".tg th{font-family:Arial, sans-serif;font-size:14px;font-weight:normal;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:#ccc;color:#333;background-color:#f0f0f0;}\n",
    ".tg .tg-c3ow{border-color:inherit;text-align:center;vertical-align:top}\n",
    ".tg .tg-7btt{font-weight:bold;border-color:inherit;text-align:center;vertical-align:top}\n",
    ".tg .tg-uzvj{font-weight:bold;border-color:inherit;text-align:center;vertical-align:middle}\n",
    "</style>\n",
    "<table class=\"tg\">\n",
    "  <tr>\n",
    "    <th class=\"tg-7btt\">DATOS<br></th>\n",
    "    <th class=\"tg-7btt\">LAPLACE</th>\n",
    "    <th class=\"tg-7btt\">ERROR</th>\n",
    "    <th class=\"tg-7btt\">DESV. TÍPICA</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-uzvj\" rowspan=\"2\">Lenses</td>\n",
    "    <td class=\"tg-c3ow\">Si</td>\n",
    "    <td class=\"tg-c3ow\">0.338</td>\n",
    "    <td class=\"tg-c3ow\">0.041<br></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-c3ow\">No</td>\n",
    "    <td class=\"tg-c3ow\">0.261<br></td>\n",
    "    <td class=\"tg-c3ow\">0.109<br></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-uzvj\" rowspan=\"2\">Tic-Tac-Toe</td>\n",
    "    <td class=\"tg-c3ow\">Si</td>\n",
    "    <td class=\"tg-c3ow\">0.351</td>\n",
    "    <td class=\"tg-c3ow\">0.015<br></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-c3ow\">No</td>\n",
    "    <td class=\"tg-c3ow\">0.351</td>\n",
    "    <td class=\"tg-c3ow\">0.015<br></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-uzvj\" rowspan=\"2\">German</td>\n",
    "    <td class=\"tg-c3ow\">Si</td>\n",
    "    <td class=\"tg-c3ow\">0.266</td>\n",
    "    <td class=\"tg-c3ow\">0.024<br></td>\n",
    "  </tr>\n",
    "        <tr>\n",
    "    <td class=\"tg-c3ow\">No</td>\n",
    "    <td class=\"tg-c3ow\">-</td>\n",
    "    <td class=\"tg-c3ow\">-<br></td>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> CONCLUSIONES </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En vista a los resultados obtenidos podemos observar que nuestro clasificador obtiene mejores resultados que la implementación de SKLearn, en general. Esto puede ser debido a que no hemos sabido explotar la potencia que tiene SKLearn, ya que las funciones tienen muchos parámetros que no hemos tenido tiempo para comprender bien y probarlos. Seguramente, si hubiesemos optimizado esas funcionalidades para cada uno de los datasets los resultados habrían sido muy parecidos o ligeramente mejores que los nuestros.\n",
    "\n",
    "En cuanto a las ejecuciones con Laplace y sin él, en el único dataset en el que es realmente relevante es en el de <strong>Lenses.data</strong> ya que tiene pocos ejemplos y es fácil que alguno de ellos haga que aparezca un 0 en alguna de las casillas de las tablas de probabilidades. En el caso de SKLearn, podemos observar que hay veces que cuando no tenemos tablas de probabilidad por atributo con 0's hay veces que incluso penaliza un poco, aunque la diferencia es mínima.\n",
    "\n",
    "Por último, en cuanto a las curvas ROC, creemos que son algo engañosas, ya que hay casos en los que los <Strong>Verdaderos negativos</Strong> provocan que el punto se encuentre por debajo de la línea. Esto nos puede llevar a pensar que nuestro clasificador no es bueno, pero obtenemos tasas de error razonables, este fenómeno es debido a que la tasa de <Strong>Verdaderos negativos</Strong> realmente indica que nuestro modelo está clasificando la clase correctamente, pero a la hora de generar la matriz de confusión no se está comparando con la clase que se acierta, si no con la contraria. Por todo lo anterior, el punto \"ROC\" aparece, aunque en realidad los resultados no son tan malo como parece si solo miramos la curva.\n",
    "\n",
    "Un poco al hilo de lo último, quizá sería mejor medir nuestro clasificador por la <Strong>Accuracy</Strong>, que se define con la siguiente fórmula: \n",
    "\n",
    "\\begin{equation}\n",
    "Accuracy =  \\frac{TP + TN}{T + TN + FP + FN}\n",
    "\\end{equation}\n",
    "\n",
    "Dandonos una medida más real de la precisión de nuestro clasificador."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
