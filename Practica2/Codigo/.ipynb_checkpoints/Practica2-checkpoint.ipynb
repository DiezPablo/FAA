{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>PRÁCTICA 2 - FUNDAMENTOS DE APRENDIZAJE AUTOMÁTICO</h1>\n",
    "<h3>Realizada la práctica por:<br/>\n",
    "    <ol>\n",
    "    - Pablo Díez del Pozo<br/>\n",
    "    - Alejandro Alcalá Álvarez<br/>\n",
    "   </ol>\n",
    " </h3>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Código</h3>\n",
    "\n",
    "En las siguientes celdas se inserta todo el código desarrollado para poder completar el funcionamiento requerido de la práctica."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>La clase <b>Datos</b> es idéntica a la que desarrollamos para la <b>Práctica 0</b>, con dos funciones añadidas, muy importantes para el correcto funcionamiento de el clasificador de <b>Vecinos Próximos</b>. Estas funciones son las siguientes:</p>\n",
    "<ol> \n",
    "    <li><b>calcularMediasDesv(self, datos):</b> esta función permite calcular las medias y desviaciones típicas de cada uno de los atributos del dataset.3\n",
    "    <li><b>normalizarDatos(self, datos):</b> llama a la función anterior y utiliza los valores que hemos calculado para normalizar el dataset, únicamente los valores continuos, ya que los discretos no es necesario normalizarlos\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Datos:\n",
    "    TiposDeAtributos = ('Continuo', 'Nominal')\n",
    "\n",
    "    # TODO: procesar el fichero para asignar correctamente las variables tipoAtributos, nombreAtributos, nominalAtributos, datos y diccionarios\n",
    "    # NOTA: No confundir TiposDeAtributos con tipoAtributos\n",
    "    def __init__(self, nombreFichero):\n",
    "\n",
    "        with open(nombreFichero, \"r\") as f:\n",
    "            # Guardamos el numero de datos que contiene el DataSet y esta en la primera linea\n",
    "            self.numDatos = int(f.readline())\n",
    "\n",
    "            # Guardamos el nombre de los atributos\n",
    "            self.nombreAtributos = f.readline().strip('\\n').split(',')\n",
    "            # print(self.nombreAtributos)\n",
    "\n",
    "            # Leemos el tipo de los atributos de las variables y eliminamos el ultimo que es un salto de linea\n",
    "            self.tipoAtributos = f.readline().strip('\\n').split(',')\n",
    "            # print(self.tipoAtributos)\n",
    "\n",
    "            # Comprobamos que todos los atributos sean Continuos o Nominales\n",
    "            if any(atr not in Datos.TiposDeAtributos for atr in self.tipoAtributos):\n",
    "                raise ValueError(\"Tipo de atributo erroneo\")\n",
    "\n",
    "            # Segun el atributo, asignamos True o False.\n",
    "            self.nominalAtributos = []\n",
    "\n",
    "            # Guardamos en la lista nominalAtributos en la posicion de cada uno si es o no Nominal\n",
    "            for tipo in self.tipoAtributos:\n",
    "                if tipo == self.TiposDeAtributos[0]:\n",
    "                    self.nominalAtributos.append(False)\n",
    "                else:\n",
    "                    self.nominalAtributos.append(True)\n",
    "\n",
    "\n",
    "            # Guardamos los datos del fichero y los formateamos, de tal forma que cada linea es una lista\n",
    "            datos = f.readlines()\n",
    "            datosFormat = []\n",
    "            for lista in datos:\n",
    "                datosFormat.append(lista.strip('\\n').split(','))\n",
    "\n",
    "            listaDatosAtributos = []\n",
    "            for i in range(len(self.tipoAtributos)):\n",
    "                listaDatosAtributos.append([])\n",
    "\n",
    "            # Hacemos la traspuesta de los datos que guardamos para que cada lista de atributo guarde todos los datos\n",
    "            # de cada atributo.\n",
    "            for lista in datosFormat:\n",
    "                i = 0\n",
    "                for item in lista:\n",
    "                    listaDatosAtributos[i].append(item)\n",
    "                    i += 1\n",
    "\n",
    "            # Ordenamos y hacemos un set para eliminar repetidos.\n",
    "            i = 0\n",
    "            for item in listaDatosAtributos:\n",
    "                listaDatosAtributos[i] = sorted(set(item))\n",
    "                i += 1\n",
    "\n",
    "            # Creacion de lista diccionarios, en caso de que el atributo sea Continuo, el diccionario estara vacio\n",
    "            self.listaDicts = []\n",
    "            for i in range(len(self.tipoAtributos)):\n",
    "                self.listaDicts.append({})\n",
    "\n",
    "            # Creamos el diccionario tal y como se describe en las diapositivas, por orden y asignando valores numericos crecientes\n",
    "            i = 0\n",
    "            for atributo in listaDatosAtributos:\n",
    "                k = 0\n",
    "                if self.tipoAtributos[i] == \"Nominal\":\n",
    "                    for dato in atributo:\n",
    "                        self.listaDicts[i][dato] = k\n",
    "                        k += 1\n",
    "                i += 1\n",
    "\n",
    "            # Creacion de la matriz de datos utilizando el diccionario para mapear los valores\n",
    "            # En primer lugar, creamos una matriz vacia de tamaña numero de atributos.\n",
    "            self.datos = np.empty((int(self.numDatos), int(len(self.tipoAtributos))))\n",
    "            i = 0\n",
    "            j = 0\n",
    "\n",
    "            # Metemos los datos en la matriz, mapeando con los diccionarios en el caso de que sean Nominales, y si son continuos normal.\n",
    "            for i in range(int(self.numDatos)):\n",
    "                for j in range(len(self.tipoAtributos)):\n",
    "                    if self.tipoAtributos[j] == 'Nominal':\n",
    "                        self.datos[i][j] = self.listaDicts[j].get(str(datosFormat[i][j]))\n",
    "                    else:\n",
    "                        self.datos[i][j] = datosFormat[i][j]\n",
    "\n",
    "            f.close()\n",
    "\n",
    "    # TODO: implementar en la practica 1\n",
    "    def extraeDatos(self, idx):\n",
    "        return self.datos[idx]\n",
    "\n",
    "\n",
    "    def calcularMediasDesv(self,datos):\n",
    "\n",
    "        # datosTrain es la matriz directamente, no hay que extraer nada\n",
    "        # Se calcula la desv. tipica y la media para los atributos continuos\n",
    "\n",
    "        # Creamos una lista con los indices que son continuos para calcular la media y desv tipica sobre ellos.\n",
    "        self.indicesContinuos = []\n",
    "        for i in range(len(self.nominalAtributos)):\n",
    "            if self.nominalAtributos[i] == False:\n",
    "                self.indicesContinuos.append(i)\n",
    "\n",
    "        # Matriz que guarda el valor por atributo de la desv. tipica y la media\n",
    "        estadisticas = np.zeros((2,len(self.nominalAtributos)))\n",
    "\n",
    "        # Calculamos la desv tipica y media de los indices continuos sobre todos los datos que recibimos por param.\n",
    "        for i in self.indicesContinuos:\n",
    "            estadisticas[0,i] = np.mean(datos[:,i])\n",
    "            estadisticas[1,i] = np.std(datos[:,i])\n",
    "\n",
    "        return estadisticas\n",
    "\n",
    "\n",
    "    def normalizarDatos(self, datos):\n",
    "\n",
    "        estadisticas = self.calcularMediasDesv(datos)\n",
    "\n",
    "        # Normalizamos los datos\n",
    "        for i in self.indicesContinuos:\n",
    "            datos[:,i] -= estadisticas[0,i]\n",
    "            datos[:,i] /= estadisticas[1,i]\n",
    "\n",
    "        return datos, estadisticas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La siguiente clase, <b>EstrategiaParticionado</b>, es exactamente igual que en la práctica anterior:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABCMeta,abstractmethod\n",
    "\n",
    "from abc import ABCMeta, abstractmethod\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class Particion():\n",
    "\n",
    "  # Esta clase mantiene la lista de �ndices de Train y Test para cada partici�n del conjunto de particiones\n",
    "  def __init__(self, train=[], test=[]):\n",
    "    self.indicesTrain = train\n",
    "    self.indicesTest = test\n",
    "\n",
    "  def __str__(self):\n",
    "    return \"Train: {}\\nTest:  {}\".format(str(self.indicesTrain), str(self.indicesTest))\n",
    "\n",
    "\n",
    "class EstrategiaParticionado:\n",
    "  # Clase abstracta\n",
    "  __metaclass__ = ABCMeta\n",
    "\n",
    "  # Lista de las particiones\n",
    "  def __init__(self, nombre=\"\"):\n",
    "    self.nombreEstrategia = nombre\n",
    "    self.numeroParticiones = 0\n",
    "    self.particiones = []\n",
    "\n",
    "  # Atributos: deben rellenarse adecuadamente para cada estrategia concreta: nombreEstrategia, numeroParticiones, listaParticiones. Se pasan en el constructor\n",
    "\n",
    "  @abstractmethod\n",
    "  # TODO: esta funcion deben ser implementadas en cada estrategia concreta\n",
    "  def creaParticiones(self, datos, seed=None):\n",
    "    pass\n",
    "\n",
    "\n",
    "class ValidacionSimple(EstrategiaParticionado):\n",
    "\n",
    "  def __init__(self, porcentaje):\n",
    "    self.porcentaje = porcentaje\n",
    "    super().__init__(\"Validacion simple\")\n",
    "\n",
    "  # Crea particiones segun el metodo tradicional de division de los datos segun el porcentaje deseado.\n",
    "  # Devuelve una lista de particiones (clase Particion)\n",
    "  # TODO: implementar\n",
    "  def creaParticiones(self, datos, seed=None):\n",
    "    np.random.seed(seed)\n",
    "    self.numeroParticiones = 1\n",
    "\n",
    "    # Generamos una lista con todos los números de datos aleatorios\n",
    "    indicesAleatorios = np.random.permutation(int(datos.numDatos))\n",
    "\n",
    "    # Creamos la particion, en funcion del porcentaje especificado\n",
    "    self.particiones = [Particion(indicesAleatorios[:int(datos.numDatos * self.porcentaje)],\n",
    "                                  indicesAleatorios[int(datos.numDatos * self.porcentaje):])]\n",
    "\n",
    "    return self.particiones\n",
    "\n",
    "\n",
    "class ValidacionCruzada(EstrategiaParticionado):\n",
    "\n",
    "  # Crea particiones segun el metodo de validacion cruzada.\n",
    "  # El conjunto de entrenamiento se crea con las nfolds-1 particiones y el de test con la particion restante\n",
    "  # Esta funcion devuelve una lista de particiones (clase Particion)\n",
    "  # TODO: implementar\n",
    "\n",
    "  def __init__(self, k):\n",
    "    self.k = k\n",
    "    super().__init__(\"Validacion cruzada\")\n",
    "\n",
    "  def creaParticiones(self, datos, seed=None):\n",
    "\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    self.numeroParticiones = self.k\n",
    "\n",
    "    # Generamos una lista con todos los números de datos aleatorios\n",
    "    indicesAleatorios = np.random.permutation(int(datos.numDatos))\n",
    "\n",
    "    # Hallamos el tamaño de cada bloque\n",
    "    tamBloque = int(datos.numDatos / self.k)\n",
    "\n",
    "    datosSobran = datos.numDatos - (tamBloque * self.k)\n",
    "    count = 0\n",
    "    for i in range(self.k):\n",
    "\n",
    "      train = np.delete(indicesAleatorios, range(i * tamBloque, (i + 1) * tamBloque))\n",
    "      test = indicesAleatorios[i * tamBloque:(i + 1) * tamBloque]\n",
    "\n",
    "      # Caso en el que la cuenta es justa\n",
    "      if datosSobran == 0:\n",
    "        self.particiones.append(Particion(train, test))\n",
    "\n",
    "      # Contemplamos el caso de que la division para sacar el numero de subconjuntos no fuese entera\n",
    "      if datosSobran > 0:\n",
    "        count += 1\n",
    "        particionTest = np.append(test, train[(datos.numDatos - tamBloque) - i - 1])\n",
    "        particionTrain = np.delete(train, (datos.numDatos - tamBloque) - i - 1)\n",
    "        datosSobran -= 1\n",
    "        self.particiones.append(Particion(particionTrain, particionTest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación, tenemos la clase <b>Clasificador</b>, junto con <b>ClasificadorNaiveBayes</b>, idénticas a las de la práctica 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABCMeta,abstractmethod\n",
    "import numpy as np\n",
    "import math\n",
    "from collections import Counter\n",
    "from sortedcontainers import SortedDict\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from matplotlib import pyplot as plt\n",
    "from numpy import linalg as LA\n",
    "from statistics import mode\n",
    "\n",
    "class Clasificador:\n",
    "  # Clase abstracta\n",
    "  __metaclass__ = ABCMeta\n",
    "\n",
    "  # Metodos abstractos que se implementan en casa clasificador concreto\n",
    "  @abstractmethod\n",
    "  # TODO: esta funcion debe ser implementada en cada clasificador concreto\n",
    "  # datosTrain: matriz numpy con los datos de entrenamiento\n",
    "  # atributosDiscretos: array bool con la indicatriz de los atributos nominales\n",
    "  # diccionario: array de diccionarios de la estructura Datos utilizados para la codificacion de variables discretas\n",
    "  def entrenamiento(self, datos, datosTrain, atributosDiscretos, diccionario):\n",
    "    pass\n",
    "\n",
    "  @abstractmethod\n",
    "  # TODO: esta funcion debe ser implementada en cada clasificador concreto\n",
    "  # devuelve un numpy array con las predicciones\n",
    "  def clasifica(self, datosTest, atributosDiscretos, diccionario):\n",
    "    pass\n",
    "\n",
    "  # Obtiene el numero de aciertos y errores para calcular la tasa de fallo\n",
    "  # TODO: implementar\n",
    "  def error(self, datos, pred):\n",
    "    # Aqui se compara la prediccion (pred) con las clases reales y se calcula el error\n",
    "    i = 0\n",
    "    real = datos[:, -1]\n",
    "    error = 0\n",
    "    for i in range(len(real)):\n",
    "      if real[i] != pred[i]:\n",
    "        error += 1\n",
    "    err = (error) / (len(real) + 0.0)\n",
    "    return err\n",
    "\n",
    "  # Realiza una clasificacion utilizando una estrategia de particionado determinada\n",
    "  # particionado : estrategia de validacion que queremos utilizar\n",
    "  # dataset : clase de tipo Datos que utilizamos para entrenar y clasificar el modelo\n",
    "  # clasificador: instancia del clasificador que se va a usar\n",
    "  # TODO: implementar esta funcion\n",
    "  def validacion(self, particionado, dataset, clasificador, seed: object = None):\n",
    "\n",
    "    # Creamos las particiones siguiendo la estrategia llamando a particionado.creaParticiones\n",
    "    # - Para validacion cruzada: en el bucle hasta nv entrenamos el clasificador con la particion de train i\n",
    "    # y obtenemos el error en la particion de test i\n",
    "    # - Para validacion simple (hold-out): entrenamos el clasificador con la particion de train\n",
    "    # y obtenemos el error en la particion test. Otra opci�n es repetir la validaci�n simple un n�mero especificado de veces, obteniendo en cada una un error. Finalmente se calcular�a la media.\n",
    "    errores = 0\n",
    "    # particionado.creaParticiones(dataset, seed)\n",
    "    # Comprobamos si es por validación cruzada o simple, por la longitud de la lista de particiones\n",
    "    particionado.particiones = []\n",
    "    particionado.creaParticiones(dataset)\n",
    "\n",
    "    # Validación Simple\n",
    "    if len(particionado.particiones) == 1:\n",
    "      clasificador.entrenamiento(dataset, particionado.particiones[0].indicesTrain)\n",
    "      pred = clasificador.clasifica(dataset.datos, particionado.particiones[0].indicesTest)\n",
    "      ret = self.error(dataset.extraeDatos(particionado.particiones[0].indicesTest), pred)\n",
    "      if ret > 0:\n",
    "        return ret\n",
    "      else:\n",
    "        return 0\n",
    "\n",
    "    # Validación Cruzada\n",
    "    else:\n",
    "      lista_error = []\n",
    "      for particion in particionado.particiones:\n",
    "        clasificador.entrenamiento(dataset, particion.indicesTrain)\n",
    "        pred = clasificador.clasifica(dataset.datos, particion.indicesTest)\n",
    "        ret = self.error(dataset.extraeDatos(particion.indicesTest), pred)\n",
    "        lista_error.append(ret)\n",
    "      return np.mean(lista_error), np.std(lista_error)\n",
    "\n",
    "  def matrizConfusion(self, dataset, datosTest, prediccion):\n",
    "\n",
    "    # Calculamos la matriz de confusion utlizando sk-learn. Solo se calcula en el caso de que la clasificacion sea binaria.\n",
    "    testData = dataset.extraeDatos(datosTest)\n",
    "    clase_real = testData[:, -1]\n",
    "\n",
    "    matriz = confusion_matrix(prediccion, clase_real)\n",
    "\n",
    "    # La funcion ravel() devuelve todas las estadisticas relacionadas con la matriz de confusion\n",
    "    tn, fp, fn, tp = matriz.ravel()\n",
    "\n",
    "    # Calculamos las tasas extraídas de la matriz de confusión\n",
    "    tpr = tp / (tp + fn)\n",
    "    fpr = fp / (fp + fn)\n",
    "\n",
    "    self.lista_tpr.append(tpr)\n",
    "    self.lista_fpr.append(fpr)\n",
    "\n",
    "    return matriz\n",
    "\n",
    "  def curvaROC(self):\n",
    "\n",
    "    x = np.linspace(0, 1, 100)\n",
    "    plt.plot(x, x, c='blue')\n",
    "    for i in range(len(self.lista_fpr)):\n",
    "      plt.plot(self.lista_fpr[i], self.lista_tpr[i], 'ro')\n",
    "    plt.show()\n",
    "\n",
    "class ClasificadorNaiveBayes(Clasificador):\n",
    "\n",
    "  def __init__(self, laplace):\n",
    "    self.laplace = laplace\n",
    "    self.lista_fpr = []\n",
    "    self.lista_tpr = []\n",
    "\n",
    "  def entrenamiento(self, dataset, datosTrain):\n",
    "\n",
    "    # Cargamos todos los datos de la clase del dataset desde la matriz de datos\n",
    "    clasesTrain = dataset.extraeDatos(datosTrain)\n",
    "    self.numClases = clasesTrain[:, -1]\n",
    "\n",
    "    # Contamos las apariciones de cada uno para luego calcular la probabilidad a priori de cada clase\n",
    "    counter = Counter(self.numClases)\n",
    "\n",
    "    # Calculamos la probabilidad de la clase y lo metemos en un diccionario ordenado segun el numero\n",
    "    # correspondiente a cada clase asignado en el diccionario\n",
    "    self.dictPrioris = {}\n",
    "    for k in counter:\n",
    "      k = int(k)\n",
    "      counter[k] = counter[k] / len(self.numClases)\n",
    "      self.dictPrioris[k] = counter[k]\n",
    "\n",
    "    # Aqui ordenamos el diccionario para que esten en el mismo orden de como extraemos los datos del dataset\n",
    "    self.dictPrioris = SortedDict(self.dictPrioris)\n",
    "\n",
    "    # Calcular tablas de probabilidades del entrenamiento. Tenemos que calcular por cada atributo una cuenta\n",
    "    # de las apariciones en cada clase\n",
    "    # Creamos una lista de matrices, donde vamos almacenar todos los datos que hemos obtenido en los datos de Test\n",
    "    self.posteriori = np.zeros(len(dataset.nombreAtributos) - 1, dtype=object)\n",
    "\n",
    "    # Recorremos todos los datos de la matriz sin llegar a la clase\n",
    "    for i in range(len(dataset.nombreAtributos) - 1):\n",
    "\n",
    "      # Si el dato que obtenemos es Nominal haremos el recuento de todas las veces que sale la P(D|H)\n",
    "      if dataset.nominalAtributos[i] == True:\n",
    "\n",
    "        # Creamos una matriz de tamaño X: Número de Atributos menos la clase Y: Número de clases\n",
    "        post = np.zeros((len(dataset.listaDicts[i]), len(dataset.listaDicts[-1])))\n",
    "\n",
    "        # Aqui contamos todos las datos que queremos del datos Train para construir la matriz de entrenamiento\n",
    "        for c in range(len(dataset.listaDicts[-1])):\n",
    "          datosEnt = dataset.extraeDatos(datosTrain)\n",
    "          dat = datosEnt[:, i]\n",
    "          repes = Counter(dat[datosEnt[:, -1] == c])\n",
    "          for r in repes:\n",
    "            post[int(r), c] = repes[r]\n",
    "          if self.laplace == True:\n",
    "            self.posteriori[i] = post + 1\n",
    "          else:\n",
    "            self.posteriori[i] = post\n",
    "\n",
    "      # Si el dato es Continuo obtendremos la media y la desviación tipica de la clase\n",
    "      else:\n",
    "\n",
    "        # Creamos una matriz de X: Los datos de Media y Desivación típica Y: Número de clases\n",
    "        post = np.zeros((2, len(dataset.listaDicts[-1])))\n",
    "\n",
    "        # Aqui obtenemos la media y desviación tipica de cada clase, despues de tener los datos de entrenamiento\n",
    "        for c in range(len(dataset.listaDicts[-1])):\n",
    "          datosEnt = dataset.extraeDatos(datosTrain)\n",
    "          dat = datosEnt[:, i]\n",
    "          datos = dat[datosEnt[:, -1] == c]\n",
    "          post[0][c] = np.mean(datos)\n",
    "          post[1][c] = np.std(datos)\n",
    "        self.posteriori[i] = post\n",
    "\n",
    "\n",
    "    # Calculamos los valores de los posteriori de todos las tablas anteriores\n",
    "    for i in range(len(dataset.listaDicts) - 1):\n",
    "      if dataset.nominalAtributos[i] == True:\n",
    "        self.posteriori[i] /= sum(self.posteriori[i])\n",
    "\n",
    "  def clasifica(self, dataset, datosTest):\n",
    "    acum_probs = 1\n",
    "    self.prediccion = []\n",
    "    datTest = dataset[datosTest]\n",
    "\n",
    "    # Ahora vamos a estudiar la probabilidad de la clase con los datos obtenidos en el entrenamiento\n",
    "    # Recorremos todos las datos de la matriz de los datos Test\n",
    "    for dato in datTest:\n",
    "      mapa = []\n",
    "      # Aqui obtenemos los prioris de cada clase para poder obtener la probabilidad de cada una\n",
    "      for clase in range(len(self.dictPrioris)):\n",
    "        listaVerosimilitudes = []\n",
    "        # Aqui obtenemos cada valor posteriori de nuestro entrenamiento de los datos, es decir, P(D|H)\n",
    "        for atributo in range(len(self.posteriori)):\n",
    "          if dataset.nominalAtributos[atributo] == True:\n",
    "            prob = self.posteriori[atributo][int(dato[atributo])][clase]\n",
    "            listaVerosimilitudes.append(prob)\n",
    "\n",
    "          # Aqui obtenemos la probabilidad de los atibutos continuos\n",
    "          else:\n",
    "            # Hacemos la formula de la distribucion normal\n",
    "            exp1 = 1 / (self.posteriori[atributo][1][clase] * math.sqrt(2 * math.pi))\n",
    "            exp2 = np.power((dato[atributo] - self.posteriori[atributo][0][clase]), 2)\n",
    "            exp3 = np.power(self.posteriori[atributo][1][clase], 2)\n",
    "            exp4 = exp2 / exp3\n",
    "            exp4 = math.exp((-1 / 2) * exp4)\n",
    "            prob = exp1 * exp4\n",
    "            listaVerosimilitudes.append(prob)\n",
    "\n",
    "        for verosimilitud in listaVerosimilitudes:\n",
    "          acum_probs *= verosimilitud\n",
    "        acum_probs *= self.dictPrioris.get(clase)\n",
    "        mapa.append(acum_probs)\n",
    "        acum_probs = 1\n",
    "\n",
    "      # Aqui obtenemos la predicción de mayor probabilidad y la guardamos en nuestra lista de predicciones\n",
    "      self.prediccion.append(np.argmax(mapa))\n",
    "\n",
    "\n",
    "    # Devolvemos la lista con la predicción de nuestro clasifica\n",
    "    return self.prediccion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En las siguientes celdas aparecen las clases <b>ClasificadorVecinosProximos</b> y <b>ClasificadorRegresionLogistica</b>, ambas necesarias para la funcionalidad requerida en esta práctica.\n",
    "\n",
    "La clase <b>ClasificadorVecinosProximos</b> recibe como parámetros en su constructor tanto el valor de <b>K</b>y si se quiere normalizar el dataset o no, por defecto la normalización está activada. El método <b>entrenamiento()</b>, normaliza el dataset si tenemos esa opción a True y guarda los datos de entrenamiento. En el caso de que no queremos normalizar, simplemente guarda los datos de entrenamiento. El método <b>clasifica()</b> implementa el algoritmo de <b>Vecinos Próximos</b>, calculando las distancias de los datos de Test con todos los datos de Train y quedandose con la clase mayoritaría de los k elementos con menor distancia, para cada dato del conjunto de Test. Todo lo anterior genera una lista con las clases predichas, que es lo que devuelve la función.\n",
    "\n",
    "La clase <b>ClasificadorRegresionLogistica</b> se inicializa con dos parámetros, la <b>Constante de Aprendizaje</b>, que se utiliza para actualizar la función de pesos del método <b>entrenamiento()</b>, y el <b>número de épocas</b>. Además, cuenta con una función auxiliar <b>sigmoidal()</b> que ejecuta la sigmoidal del valor dado por el producto escalar de la función de pesos <b>(w)</b> y el vector compuesto por el dato de entrenamiento <b>(x)</b> en el que nos encontramos. La función <b>entrenamiento()</b> utiliza el algoritmo de la Regresión Logistica hasta el número de épocas definido, obteniendo el vector de pesos final, que posteriormente se utiliza en <b>clasifica()</b>, que genera una predicción para cada dato del test, aplicando la sigmoidal del dato multiplicado por el vector de pesos final que obtenemos al entrenar el modelo, si se obtiene un valor mayor de 0.5 el dato es de clase 1, si no de clase 2. \n",
    "\n",
    "<h4>Particularidades Vecinos Próximos</h4>\n",
    "\n",
    "No hemos realizado distinción entre el caso de que el atributo sea discreto o continuo para calcular las distancias, ya que nuestra idea era utilizar distancia Euclídea en el caso de los continuos y distancia Manhattan en el caso de los discretos, pero como codificamos el dataset previamente en formato numérico, esas distancias no se ven afectadas realizando la distinción, ya que son proporcionales.\n",
    "\n",
    "El caso en el que el atributo tiene un orden natural, como el que se describe en el enunciado con el caso de los meses, también lo hemos obviado, ya que la codificación del dataset a numéricos resuelve este problema. Por ejemplo, el orden natural de los meses, en nuestro sería del 0 al 11 y en nuestro caso codificamos los meses según esa notación. El único posible problema es que se codifican en orden de lectura, es decir, si aparece primero Noviembre y luego Junio, esos serían el 0 y el 1. Esta situación no es ningún problema, ya que las distancias se mantienen costantes en todos los casos para ese nuevo orden, siendo proporcionales en todos los casos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clase que define un clasificador utilizando el metodo de Regresion Logistica\n",
    "class ClasificadorRegresionLogistica(Clasificador):\n",
    "\n",
    "  def __init__(self, constante_aprendizaje, epocas):\n",
    "    # Se utiliza para inicializar la constante de aprendizaje y el numero de epocas necesarias para el train\n",
    "    self.constante_aprendizaje = constante_aprendizaje\n",
    "    self.epocas = epocas\n",
    "\n",
    "    super().__init__()\n",
    "\n",
    "  # Funcion que realiza el calculo de la sigmoidal de el numero que recibe por parámetro\n",
    "  def sigmoidal(self, a):\n",
    "\n",
    "    if a >= 100:\n",
    "      return 1\n",
    "    elif a < 100:\n",
    "      return 0\n",
    "    else:\n",
    "      return 1.0 / (1.0 + math.exp(-a))\n",
    "\n",
    "  def entrenamiento(self, dataset, datosTrain):\n",
    "\n",
    "    # Datos de entrenamiento\n",
    "    self.datTrain = dataset.extraeDatos(datosTrain)\n",
    "\n",
    "    # Inicializamos el vector w\n",
    "    self.vector_w = np.random.uniform(low=-0.5, high=0.5, size=len(dataset.tipoAtributos))\n",
    "\n",
    "    i = 0\n",
    "\n",
    "    # Algoritmo del entrenamiento de regresion lineal\n",
    "    while i < self.epocas:\n",
    "      for dato in self.datTrain:\n",
    "\n",
    "        # Calculamos vector x\n",
    "        x = np.append([1],dato[:-1])\n",
    "\n",
    "        # Calculamos w.x\n",
    "        wx = np.dot(self.vector_w,x)\n",
    "\n",
    "        # Sigmoidal de w.x\n",
    "        sigmoidal = self.sigmoidal(wx)\n",
    "\n",
    "        # Actualizamos el vector w\n",
    "        self.vector_w = self.vector_w - (np.dot(self.constante_aprendizaje * (sigmoidal - dato[-1]),x))\n",
    "\n",
    "      i += 1\n",
    "\n",
    "    return self.vector_w\n",
    "\n",
    "  def clasifica(self, dataset, datosTest = None):\n",
    "\n",
    "    if datosTest is None:\n",
    "      datosTest = range(len(dataset))\n",
    "\n",
    "    datTest = dataset[datosTest]\n",
    "    prediccion = []\n",
    "\n",
    "    for dato in datTest:\n",
    "\n",
    "      # Vector x\n",
    "      x = np.append([1], dato[:-1])\n",
    "\n",
    "      # Calculo de la sigmoidal que nos da la probabilidad de que el dato sea de clase 1\n",
    "      wx = np.dot(self.vector_w, x)\n",
    "      sigmoidal = self.sigmoidal(wx)\n",
    "\n",
    "      # Prediccion del clasificador\n",
    "      if(sigmoidal >= 0.5):\n",
    "        prediccion.append(1)\n",
    "      else:\n",
    "        prediccion.append(0)\n",
    "\n",
    "    return prediccion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez explicadas las clases creadas, generamos los datasets y las particiones que vamos a utilizar para la extracción de resultados de los mismos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargamos todos los datasets\n",
    "\n",
    "dataset_example1 =Datos('../Datasets/example1.data')\n",
    "dataset_example2 =Datos('../Datasets/example2.data')\n",
    "dataset_example3 =Datos('../Datasets/example3.data')\n",
    "dataset_example4 = Datos('../Datasets/example4.data')\n",
    "dataset_online = Datos('../Datasets/online_shoppers.data')\n",
    "dataset_wdbc = Datos('../Datasets/wdbc.data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validacion simple\n",
    "\n",
    "VS_example1 = ValidacionSimple(0.7)\n",
    "VS_example2 = ValidacionSimple(0.7)\n",
    "VS_example3 = ValidacionSimple(0.7)\n",
    "VS_example4 = ValidacionSimple(0.7)\n",
    "VS_online = ValidacionSimple(0.7)\n",
    "VS_wdbc = ValidacionSimple(0.7)\n",
    "\n",
    "VS_example1.creaParticiones(dataset_example1)\n",
    "VS_example2.creaParticiones(dataset_example2)\n",
    "VS_example3.creaParticiones(dataset_example3)\n",
    "VS_example4.creaParticiones(dataset_example4)\n",
    "VS_online.creaParticiones(dataset_online)\n",
    "VS_wdbc.creaParticiones(dataset_wdbc)\n",
    "\n",
    "VC_example1 = ValidacionCruzada(5)\n",
    "VC_example2 = ValidacionCruzada(5)\n",
    "VC_example3 = ValidacionCruzada(5)\n",
    "VC_example4 = ValidacionCruzada(5)\n",
    "VC_online = ValidacionCruzada(5)\n",
    "VC_wdbc = ValidacionCruzada(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Apartado 1</h3>\n",
    "\n",
    "En este apartado se detallan los resultados obtenidos utlizando la implementación propia de <b>Vecinos Próximos</b>, tanto normalizando los datos como sin hacerlo. Se recoge una tabla que muestra el rendimiento para los datasets de <b>wdbc.data</b> y <b>online_shoppers.data</b>.\n",
    "\n",
    "En el caso del dataset <b>online_shoppers.data</b> solamente hemos utilizado validación simple, ya que tardaba mucho en ejecutarse, en el caso de <b>wdbc.data</b> si se ha utilizado tanto la validación simple como la cruzada, para observarlas diferencias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Apartado 2</h3>\n",
    "\n",
    "En este apartado se muestra la ejecución, para los mismos datasets que en el apartado 1, del modelo de <b>Regresión Logísitca</b> que hemos implementado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Ejecuciones con Validación Simple y Online_Shoppers.data</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Validacion simple -- Dataset Online_Shoppers -- Epocas: 10 -- n: 0.1 -- Error: 0.11300351446336848\n"
     ]
    }
   ],
   "source": [
    "regresion_logistica = ClasificadorRegresionLogistica(0.1,10)\n",
    "error = regresion_logistica.validacion(VS_online, dataset_online, regresion_logistica)\n",
    "\n",
    "print(\"-- Validacion simple -- Dataset Online_Shoppers -- Epocas: 10 -- n: 0.1 -- Error:\",error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Validacion simple -- Dataset Online_Shoppers -- Epocas: 10 -- n: 1 -- Error: 0.14706677480400107\n"
     ]
    }
   ],
   "source": [
    "regresion_logistica = ClasificadorRegresionLogistica(1,10)\n",
    "error = regresion_logistica.validacion(VS_online, dataset_online, regresion_logistica)\n",
    "\n",
    "print(\"-- Validacion simple -- Dataset Online_Shoppers -- Epocas: 10 -- n: 1 -- Error:\",error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Validacion simple -- Dataset Online_Shoppers -- Epocas: 10 -- n: 0.01 -- Error: 0.5466342254663422\n"
     ]
    }
   ],
   "source": [
    "regresion_logistica = ClasificadorRegresionLogistica(0.01,10)\n",
    "error = regresion_logistica.validacion(VS_online, dataset_online, regresion_logistica)\n",
    "\n",
    "print(\"-- Validacion simple -- Dataset Online_Shoppers -- Epocas: 10 -- n: 0.01 -- Error:\",error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Validacion simple -- Dataset Online_Shoppers -- Epocas: 100 -- n: 0.1 -- Error: 0.12949445796161124\n"
     ]
    }
   ],
   "source": [
    "regresion_logistica = ClasificadorRegresionLogistica(0.1,100)\n",
    "error = regresion_logistica.validacion(VS_online, dataset_online, regresion_logistica)\n",
    "\n",
    "print(\"-- Validacion simple -- Dataset Online_Shoppers -- Epocas: 100 -- n: 0.1 -- Error:\",error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Validacion simple -- Dataset Online_Shoppers -- Epocas: 100 -- n: 1 -- Error: 0.5217626385509597\n"
     ]
    }
   ],
   "source": [
    "regresion_logistica = ClasificadorRegresionLogistica(1,100)\n",
    "error = regresion_logistica.validacion(VS_online, dataset_online, regresion_logistica)\n",
    "\n",
    "print(\"-- Validacion simple -- Dataset Online_Shoppers -- Epocas: 100 -- n: 1 -- Error:\",error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Validacion simple -- Dataset Online_Shoppers -- Epocas: 100 -- n: 0.01 -- Error: 0.13760475804271424\n"
     ]
    }
   ],
   "source": [
    "regresion_logistica = ClasificadorRegresionLogistica(0.01,100)\n",
    "error = regresion_logistica.validacion(VS_online, dataset_online, regresion_logistica)\n",
    "\n",
    "print(\"-- Validacion simple -- Dataset Online_Shoppers -- Epocas: 100 -- n: 0.01 -- Error:\",error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Ejecuciones con Validación Simple y WDBC.data</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Validacion simple -- Dataset WDBC -- Epocas: 10 -- n: 0.1 -- Error: 0.14841849148418493\n"
     ]
    }
   ],
   "source": [
    "regresion_logistica = ClasificadorRegresionLogistica(0.1,10)\n",
    "error = regresion_logistica.validacion(VS_wdbc, dataset_online, regresion_logistica)\n",
    "\n",
    "print(\"-- Validacion simple -- Dataset WDBC -- Epocas: 10 -- n: 0.1 -- Error:\",error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Validacion simple -- Dataset WDBC -- Epocas: 10 -- n: 1 -- Error: 0.1300351446336848\n"
     ]
    }
   ],
   "source": [
    "regresion_logistica = ClasificadorRegresionLogistica(1,10)\n",
    "error = regresion_logistica.validacion(VS_wdbc, dataset_online, regresion_logistica)\n",
    "\n",
    "print(\"-- Validacion simple -- Dataset WDBC -- Epocas: 10 -- n: 1 -- Error:\",error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Validacion simple -- Dataset WDBC -- Epocas: 10 -- n: 0.01 -- Error: 0.6623411732900784\n"
     ]
    }
   ],
   "source": [
    "regresion_logistica = ClasificadorRegresionLogistica(0.01,10)\n",
    "error = regresion_logistica.validacion(VS_wdbc, dataset_online, regresion_logistica)\n",
    "\n",
    "print(\"-- Validacion simple -- Dataset WDBC -- Epocas: 10 -- n: 0.01 -- Error:\",error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Validacion simple -- Dataset WDBC -- Epocas: 100 -- n: 0.1 -- Error: 0.18140037848067045\n"
     ]
    }
   ],
   "source": [
    "regresion_logistica = ClasificadorRegresionLogistica(0.1,100)\n",
    "error = regresion_logistica.validacion(VS_wdbc, dataset_online, regresion_logistica)\n",
    "\n",
    "print(\"-- Validacion simple -- Dataset WDBC -- Epocas: 100 -- n: 0.1 -- Error:\",error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Validacion simple -- Dataset WDBC -- Epocas: 100 -- n: 1 -- Error: 0.12489862124898621\n"
     ]
    }
   ],
   "source": [
    "regresion_logistica = ClasificadorRegresionLogistica(1,100)\n",
    "error = regresion_logistica.validacion(VS_wdbc, dataset_online, regresion_logistica)\n",
    "\n",
    "print(\"-- Validacion simple -- Dataset WDBC -- Epocas: 100 -- n: 1 -- Error:\",error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -- Validacion simple -- Dataset WDBC -- Epocas: 100 -- n: 0.01 -- Error: 0.583941605839416\n"
     ]
    }
   ],
   "source": [
    "regresion_logistica = ClasificadorRegresionLogistica(0.01,100)\n",
    "error = regresion_logistica.validacion(VS_wdbc, dataset_online, regresion_logistica)\n",
    "\n",
    "print(\" -- Validacion simple -- Dataset WDBC -- Epocas: 100 -- n: 0.01 -- Error:\",error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Ejecuciones con Validación Cruzada y Online_Shoppers.data</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Validacion cruzada -- Dataset ONLINE_SHOPPERS -- Epocas: 10 -- n: 0.1 --\n",
      "    Error: 0.13341443633414435\n",
      "    Desv. tipica 0.012530348007817755\n"
     ]
    }
   ],
   "source": [
    "regresion_logistica = ClasificadorRegresionLogistica(0.1,10)\n",
    "error, desv = regresion_logistica.validacion(VC_online, dataset_online, regresion_logistica)\n",
    "\n",
    "\n",
    "print(\"-- Validacion cruzada -- Dataset ONLINE_SHOPPERS -- Epocas: 10 -- n: 0.1 --\")\n",
    "print(\"    Error:\", error)\n",
    "print(\"    Desv. tipica\", desv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Validacion cruzada -- Dataset ONLINE_SHOPPERS -- Epocas: 10 -- n: 0.1 --\n",
      "    Error: 0.11038118410381184\n",
      "    Desv. tipica 0.004969172321829808\n"
     ]
    }
   ],
   "source": [
    "regresion_logistica = ClasificadorRegresionLogistica(1,10)\n",
    "error, desv = regresion_logistica.validacion(VC_online, dataset_online, regresion_logistica)\n",
    "\n",
    "\n",
    "print(\"-- Validacion cruzada -- Dataset ONLINE_SHOPPERS -- Epocas: 10 -- n: 0.1 --\")\n",
    "print(\"    Error:\", error)\n",
    "print(\"    Desv. tipica\", desv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Validacion cruzada -- Dataset ONLINE_SHOPPERS -- Epocas: 10 -- n: 1 --\n",
      "    Error: 0.25117599351175995\n",
      "    Desv. tipica 0.14681874429950126\n"
     ]
    }
   ],
   "source": [
    "regresion_logistica = ClasificadorRegresionLogistica(0.01,10)\n",
    "error, desv = regresion_logistica.validacion(VC_online, dataset_online, regresion_logistica)\n",
    "\n",
    "\n",
    "print(\"-- Validacion cruzada -- Dataset ONLINE_SHOPPERS -- Epocas: 10 -- n: 1 --\")\n",
    "print(\"    Error:\", error)\n",
    "print(\"    Desv. tipica\", desv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Validacion cruzada -- Dataset ONLINE_SHOPPERS -- Epocas: 100 -- n: 0.1 --\n",
      "    Error: 0.13909164639091648\n",
      "    Desv. tipica 0.022145723491809904\n"
     ]
    }
   ],
   "source": [
    "regresion_logistica = ClasificadorRegresionLogistica(0.1,100)\n",
    "error, desv = regresion_logistica.validacion(VC_online, dataset_online, regresion_logistica)\n",
    "\n",
    "\n",
    "print(\"-- Validacion cruzada -- Dataset ONLINE_SHOPPERS -- Epocas: 100 -- n: 0.1 --\")\n",
    "print(\"    Error:\", error)\n",
    "print(\"    Desv. tipica\", desv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Validacion cruzada -- Dataset ONLINE_SHOPPERS -- Epocas: 100 -- n: 1 --\n",
      "    Error: 0.12384428223844282\n",
      "    Desv. tipica 0.0052095658889705545\n"
     ]
    }
   ],
   "source": [
    "regresion_logistica = ClasificadorRegresionLogistica(1,100)\n",
    "error, desv = regresion_logistica.validacion(VC_online, dataset_online, regresion_logistica)\n",
    "\n",
    "\n",
    "print(\"-- Validacion cruzada -- Dataset ONLINE_SHOPPERS -- Epocas: 100 -- n: 1 --\")\n",
    "print(\"    Error:\", error)\n",
    "print(\"    Desv. tipica\", desv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Validacion cruzada -- Dataset ONLINE_SHOPPERS -- Epocas: 100 -- n: 0.01 --\n",
      "    Error: 0.14630981346309813\n",
      "    Desv. tipica 0.04740010745196649\n"
     ]
    }
   ],
   "source": [
    "regresion_logistica = ClasificadorRegresionLogistica(0.01,100)\n",
    "error, desv = regresion_logistica.validacion(VC_online, dataset_online, regresion_logistica)\n",
    "\n",
    "\n",
    "print(\"-- Validacion cruzada -- Dataset ONLINE_SHOPPERS -- Epocas: 100 -- n: 0.01 --\")\n",
    "print(\"    Error:\", error)\n",
    "print(\"    Desv. tipica\", desv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Ejecuciones con Validación Cruzada y WDBC.data</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Validacion cruzada -- Dataset ONLINE_SHOPPERS -- Epocas: 10 -- n: 0.1 --\n",
      "    Error: 0.12181670721816706\n",
      "    Desv. tipica 0.00976137874399228\n"
     ]
    }
   ],
   "source": [
    "regresion_logistica = ClasificadorRegresionLogistica(0.1,10)\n",
    "error, desv = regresion_logistica.validacion(VC_wdbc, dataset_online, regresion_logistica)\n",
    "\n",
    "\n",
    "print(\"-- Validacion cruzada -- Dataset ONLINE_SHOPPERS -- Epocas: 10 -- n: 0.1 --\")\n",
    "print(\"    Error:\", error)\n",
    "print(\"    Desv. tipica\", desv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Validacion cruzada -- Dataset ONLINE_SHOPPERS -- Epocas: 10 -- n: 1 --\n",
      "    Error: 0.6055150040551501\n",
      "    Desv. tipica 0.2454215399189112\n"
     ]
    }
   ],
   "source": [
    "regresion_logistica = ClasificadorRegresionLogistica(1,10)\n",
    "error, desv = regresion_logistica.validacion(VC_wdbc, dataset_online, regresion_logistica)\n",
    "\n",
    "\n",
    "print(\"-- Validacion cruzada -- Dataset ONLINE_SHOPPERS -- Epocas: 10 -- n: 1 --\")\n",
    "print(\"    Error:\", error)\n",
    "print(\"    Desv. tipica\", desv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Validacion cruzada -- Dataset ONLINE_SHOPPERS -- Epocas: 10 -- n: 0.01 --\n",
      "    Error: 0.1280616382806164\n",
      "    Desv. tipica 0.010550873880259153\n"
     ]
    }
   ],
   "source": [
    "regresion_logistica = ClasificadorRegresionLogistica(0.01,10)\n",
    "error, desv = regresion_logistica.validacion(VC_wdbc, dataset_online, regresion_logistica)\n",
    "\n",
    "\n",
    "print(\"-- Validacion cruzada -- Dataset ONLINE_SHOPPERS -- Epocas: 10 -- n: 0.01 --\")\n",
    "print(\"    Error:\", error)\n",
    "print(\"    Desv. tipica\", desv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Validacion cruzada -- Dataset ONLINE_SHOPPERS -- Epocas: 100 -- n: 0.1 --\n",
      "    Error: 0.435198702351987\n",
      "    Desv. tipica 0.16477854572784872\n"
     ]
    }
   ],
   "source": [
    "regresion_logistica = ClasificadorRegresionLogistica(0.1,100)\n",
    "error, desv = regresion_logistica.validacion(VC_wdbc, dataset_online, regresion_logistica)\n",
    "\n",
    "\n",
    "print(\"-- Validacion cruzada -- Dataset ONLINE_SHOPPERS -- Epocas: 100 -- n: 0.1 --\")\n",
    "print(\"    Error:\", error)\n",
    "print(\"    Desv. tipica\", desv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Validacion cruzada -- Dataset ONLINE_SHOPPERS -- Epocas: 100 -- n: 1 --\n",
      "    Error: 0.12222222222222223\n",
      "    Desv. tipica 0.011557106472369634\n"
     ]
    }
   ],
   "source": [
    "regresion_logistica = ClasificadorRegresionLogistica(1,100)\n",
    "error, desv = regresion_logistica.validacion(VC_wdbc, dataset_online, regresion_logistica)\n",
    "\n",
    "\n",
    "print(\"-- Validacion cruzada -- Dataset ONLINE_SHOPPERS -- Epocas: 100 -- n: 1 --\")\n",
    "print(\"    Error:\", error)\n",
    "print(\"    Desv. tipica\", desv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Validacion cruzada -- Dataset ONLINE_SHOPPERS -- Epocas: 100 -- n: 0.01 --\n",
      "    Error: 0.16674776966747767\n",
      "    Desv. tipica 0.09312390505951341\n"
     ]
    }
   ],
   "source": [
    "regresion_logistica = ClasificadorRegresionLogistica(0.01,100)\n",
    "error, desv = regresion_logistica.validacion(VC_wdbc, dataset_online, regresion_logistica)\n",
    "\n",
    "\n",
    "print(\"-- Validacion cruzada -- Dataset ONLINE_SHOPPERS -- Epocas: 100 -- n: 0.01 --\")\n",
    "print(\"    Error:\", error)\n",
    "print(\"    Desv. tipica\", desv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Resultados Regresión Logística</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ONLINE_SHOPPERS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WDBC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Apartado 3</h3>\n",
    "\n",
    "Ejecución de los algoritmos de SKLearn y de la representación de fronteras utilizando nuestros modelos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Clasificador\n",
    "from matplotlib.colors import ListedColormap\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Autor Luis Lago y Manuel Sanchez Montanes\n",
    "# Modificada por Gonzalo\n",
    "def plotModel(x, y, clase, clf, title):\n",
    "    x_min, x_max = x.min() - .2, x.max() + .2\n",
    "    y_min, y_max = y.min() - .2, y.max() + .2\n",
    "\n",
    "    hx = (x_max - x_min) / 100.\n",
    "    hy = (y_max - y_min) / 100.\n",
    "\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, hx), np.arange(y_min, y_max, hy))\n",
    "\n",
    "    if isinstance(clf, Clasificador.Clasificador):\n",
    "        zeros = np.zeros(len(xx.ravel()))\n",
    "        pred = clf.clasifica(np.c_[xx.ravel(), yy.ravel(), zeros])\n",
    "        z = np.array(pred)\n",
    "        # z = clf.clasifica(np.c_[xx.ravel(), yy.ravel()], [False, False, True], diccionarios)\n",
    "    elif hasattr(clf, \"decision_function\"):\n",
    "        z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])\n",
    "    else:\n",
    "        z = clf.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1]\n",
    "\n",
    "    z = z.reshape(xx.shape)\n",
    "    cm = plt.cm.RdBu\n",
    "    cm_bright = ListedColormap(['#FF0000', '#0000FF'])\n",
    "    # ax = plt.subplot(1, 1, 1)\n",
    "    plt.contourf(xx, yy, z, cmap=cm, alpha=.8)\n",
    "    plt.contour(xx, yy, z, [0.5], linewidths=[2], colors=['k'])\n",
    "\n",
    "    if clase is not None:\n",
    "        plt.scatter(x[clase == 0], y[clase == 0], c='#FF0000')\n",
    "        plt.scatter(x[clase == 1], y[clase == 1], c='#0000FF')\n",
    "    else:\n",
    "        plt.plot(x, y, 'g', linewidth=3)\n",
    "\n",
    "    plt.gca().set_xlim(xx.min(), xx.max())\n",
    "    plt.gca().set_ylim(yy.min(), yy.max())\n",
    "    plt.grid(True)\n",
    "    plt.xlabel(\"X\")\n",
    "    plt.ylabel(\"Y\")\n",
    "    plt.title(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Representación de fronteras Vecinos Próximos</h4>\n",
    "\n",
    "Tomamos la K como 11 para todos los casos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ClasificadorVecinosProximos' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-6b64deabe0b4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mknn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mClasificadorVecinosProximos\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m11\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mknn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mentrenamiento\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVS_example1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparticiones\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindicesTrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mknn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasifica\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatos\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mVS_example1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparticiones\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindicesTest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatos\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mVS_example1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparticiones\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindicesTrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ClasificadorVecinosProximos' is not defined"
     ]
    }
   ],
   "source": [
    "knn = ClasificadorVecinosProximos(11)\n",
    "knn.entrenamiento(dataset, VS_example1.particiones[0].indicesTrain)\n",
    "pred = knn.clasifica(dataset.datos,VS_example1.particiones[0].indicesTest)\n",
    "\n",
    "x = dataset.datos[VS_example1.particiones[0].indicesTrain, 0]\n",
    "y = dataset.datos[VS_example1.particiones[0].indicesTrain, 1]\n",
    "clase = dataset.datos[VS_example1.particiones[0].indicesTrain, -1] != 0\n",
    "plotModel(x, y, clase, knn, \"-- Fronteras KNN -- K = 11 -- EXAMPLE1 --\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = ClasificadorVecinosProximos(11)\n",
    "knn.entrenamiento(dataset, VS_example2.particiones[0].indicesTrain)\n",
    "pred = knn.clasifica(dataset.datos,VS_example2.particiones[0].indicesTest)\n",
    "\n",
    "x = dataset.datos[VS_example2.particiones[0].indicesTrain, 0]\n",
    "y = dataset.datos[VS_example2.particiones[0].indicesTrain, 1]\n",
    "clase = dataset.datos[VS_example2.particiones[0].indicesTrain, -1] != 0\n",
    "plotModel(x, y, clase, knn, \"-- Fronteras KNN -- K = 11 -- EXAMPLE2 --\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = ClasificadorVecinosProximos(11)\n",
    "knn.entrenamiento(dataset, VS_example3.particiones[0].indicesTrain)\n",
    "pred = knn.clasifica(dataset.datos,VS_example3.particiones[0].indicesTest)\n",
    "\n",
    "x = dataset.datos[VS_example3.particiones[0].indicesTrain, 0]\n",
    "y = dataset.datos[VS_example3.particiones[0].indicesTrain, 1]\n",
    "clase = dataset.datos[VS_example3.particiones[0].indicesTrain, -1] != 0\n",
    "plotModel(x, y, clase, knn, \"-- Fronteras KNN -- K = 11 -- EXAMPLE3 --\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = ClasificadorVecinosProximos(11)\n",
    "knn.entrenamiento(dataset, VS_example4.particiones[0].indicesTrain)\n",
    "pred = knn.clasifica(dataset.datos,VS_example4.particiones[0].indicesTest)\n",
    "\n",
    "x = dataset.datos[VS_example4.particiones[0].indicesTrain, 0]\n",
    "y = dataset.datos[VS_example4.particiones[0].indicesTrain, 1]\n",
    "clase = dataset.datos[VS_example4.particiones[0].indicesTrain, -1] != 0\n",
    "plotModel(x, y, clase, knn, \"-- Fronteras KNN -- K = 11 -- EXAMPLE4 --\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Representación de fronteras Regresión Logística</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Apartado 4</h3>\n",
    "\n",
    "Comparación de Análisis ROC entre los 3 clasificadores implementados durante el curso para los dataset <b>wdbc.data</b> y <b>online_shoppers</b>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  logistic_reg = ClasificadorRegresionLogistica(0.5,250)\n",
    "    logistic_reg.entrenamiento(dataset,estrategia.particiones[0].indicesTrain)\n",
    "    pred = logistic_reg.clasifica(dataset.datos,estrategia.particiones[0].indicesTest)\n",
    "    print(pred)\n",
    "    error = logistic_reg.error(dataset.extraeDatos(estrategia.particiones[0].indicesTest),pred)\n",
    "    print(error)\n",
    "    matriz = logistic_reg.matrizConfusion(dataset,estrategia.particiones[0].indicesTest,pred)\n",
    "    print(matriz)\n",
    "    logistic_reg.curvaROC()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
