{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>PRÁCTICA 2 - FUNDAMENTOS DE APRENDIZAJE AUTOMÁTICO</h1>\n",
    "<h3>Realizada la práctica por:<br/>\n",
    "    <ol>\n",
    "    - Pablo Díez del Pozo<br/>\n",
    "    - Alejandro Alcalá Álvarez<br/>\n",
    "   </ol>\n",
    " </h3>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Código</h3>\n",
    "\n",
    "En las siguientes celdas se inserta todo el código desarrollado para poder completar el funcionamiento requerido de la práctica."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>La clase <b>Datos</b> es idéntica a la que desarrollamos para la <b>Práctica 0</b>, con dos funciones añadidas, muy importantes para el correcto funcionamiento de el clasificador de <b>Vecinos Próximos</b>. Estas funciones son las siguientes:</p>\n",
    "<ol> \n",
    "    <li><b>calcularMediasDesv(self, datos):</b> esta función permite calcular las medias y desviaciones típicas de cada uno de los atributos del dataset.3\n",
    "    <li><b>normalizarDatos(self, datos):</b> llama a la función anterior y utiliza los valores que hemos calculado para normalizar el dataset, únicamente los valores continuos, ya que los discretos no es necesario normalizarlos\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Datos:\n",
    "    TiposDeAtributos = ('Continuo', 'Nominal')\n",
    "\n",
    "    # TODO: procesar el fichero para asignar correctamente las variables tipoAtributos, nombreAtributos, nominalAtributos, datos y diccionarios\n",
    "    # NOTA: No confundir TiposDeAtributos con tipoAtributos\n",
    "    def __init__(self, nombreFichero):\n",
    "\n",
    "        with open(nombreFichero, \"r\") as f:\n",
    "            # Guardamos el numero de datos que contiene el DataSet y esta en la primera linea\n",
    "            self.numDatos = int(f.readline())\n",
    "\n",
    "            # Guardamos el nombre de los atributos\n",
    "            self.nombreAtributos = f.readline().strip('\\n').split(',')\n",
    "            # print(self.nombreAtributos)\n",
    "\n",
    "            # Leemos el tipo de los atributos de las variables y eliminamos el ultimo que es un salto de linea\n",
    "            self.tipoAtributos = f.readline().strip('\\n').split(',')\n",
    "            # print(self.tipoAtributos)\n",
    "\n",
    "            # Comprobamos que todos los atributos sean Continuos o Nominales\n",
    "            if any(atr not in Datos.TiposDeAtributos for atr in self.tipoAtributos):\n",
    "                raise ValueError(\"Tipo de atributo erroneo\")\n",
    "\n",
    "            # Segun el atributo, asignamos True o False.\n",
    "            self.nominalAtributos = []\n",
    "\n",
    "            # Guardamos en la lista nominalAtributos en la posicion de cada uno si es o no Nominal\n",
    "            for tipo in self.tipoAtributos:\n",
    "                if tipo == self.TiposDeAtributos[0]:\n",
    "                    self.nominalAtributos.append(False)\n",
    "                else:\n",
    "                    self.nominalAtributos.append(True)\n",
    "\n",
    "\n",
    "            # Guardamos los datos del fichero y los formateamos, de tal forma que cada linea es una lista\n",
    "            datos = f.readlines()\n",
    "            datosFormat = []\n",
    "            for lista in datos:\n",
    "                datosFormat.append(lista.strip('\\n').split(','))\n",
    "\n",
    "            listaDatosAtributos = []\n",
    "            for i in range(len(self.tipoAtributos)):\n",
    "                listaDatosAtributos.append([])\n",
    "\n",
    "            # Hacemos la traspuesta de los datos que guardamos para que cada lista de atributo guarde todos los datos\n",
    "            # de cada atributo.\n",
    "            for lista in datosFormat:\n",
    "                i = 0\n",
    "                for item in lista:\n",
    "                    listaDatosAtributos[i].append(item)\n",
    "                    i += 1\n",
    "\n",
    "            # Ordenamos y hacemos un set para eliminar repetidos.\n",
    "            i = 0\n",
    "            for item in listaDatosAtributos:\n",
    "                listaDatosAtributos[i] = sorted(set(item))\n",
    "                i += 1\n",
    "\n",
    "            # Creacion de lista diccionarios, en caso de que el atributo sea Continuo, el diccionario estara vacio\n",
    "            self.listaDicts = []\n",
    "            for i in range(len(self.tipoAtributos)):\n",
    "                self.listaDicts.append({})\n",
    "\n",
    "            # Creamos el diccionario tal y como se describe en las diapositivas, por orden y asignando valores numericos crecientes\n",
    "            i = 0\n",
    "            for atributo in listaDatosAtributos:\n",
    "                k = 0\n",
    "                if self.tipoAtributos[i] == \"Nominal\":\n",
    "                    for dato in atributo:\n",
    "                        self.listaDicts[i][dato] = k\n",
    "                        k += 1\n",
    "                i += 1\n",
    "\n",
    "            # Creacion de la matriz de datos utilizando el diccionario para mapear los valores\n",
    "            # En primer lugar, creamos una matriz vacia de tamaña numero de atributos.\n",
    "            self.datos = np.empty((int(self.numDatos), int(len(self.tipoAtributos))))\n",
    "            i = 0\n",
    "            j = 0\n",
    "\n",
    "            # Metemos los datos en la matriz, mapeando con los diccionarios en el caso de que sean Nominales, y si son continuos normal.\n",
    "            for i in range(int(self.numDatos)):\n",
    "                for j in range(len(self.tipoAtributos)):\n",
    "                    if self.tipoAtributos[j] == 'Nominal':\n",
    "                        self.datos[i][j] = self.listaDicts[j].get(str(datosFormat[i][j]))\n",
    "                    else:\n",
    "                        self.datos[i][j] = datosFormat[i][j]\n",
    "\n",
    "            f.close()\n",
    "\n",
    "    # TODO: implementar en la practica 1\n",
    "    def extraeDatos(self, idx):\n",
    "        return self.datos[idx]\n",
    "\n",
    "\n",
    "    def calcularMediasDesv(self,datos):\n",
    "\n",
    "        # datosTrain es la matriz directamente, no hay que extraer nada\n",
    "        # Se calcula la desv. tipica y la media para los atributos continuos\n",
    "\n",
    "        # Creamos una lista con los indices que son continuos para calcular la media y desv tipica sobre ellos.\n",
    "        self.indicesContinuos = []\n",
    "        for i in range(len(self.nominalAtributos)):\n",
    "            if self.nominalAtributos[i] == False:\n",
    "                self.indicesContinuos.append(i)\n",
    "\n",
    "        # Matriz que guarda el valor por atributo de la desv. tipica y la media\n",
    "        estadisticas = np.zeros((2,len(self.nominalAtributos)))\n",
    "\n",
    "        # Calculamos la desv tipica y media de los indices continuos sobre todos los datos que recibimos por param.\n",
    "        for i in self.indicesContinuos:\n",
    "            estadisticas[0,i] = np.mean(datos[:,i])\n",
    "            estadisticas[1,i] = np.std(datos[:,i])\n",
    "\n",
    "        return estadisticas\n",
    "\n",
    "\n",
    "    def normalizarDatos(self, datos):\n",
    "\n",
    "        estadisticas = self.calcularMediasDesv(datos)\n",
    "\n",
    "        # Normalizamos los datos\n",
    "        for i in self.indicesContinuos:\n",
    "            datos[:,i] -= estadisticas[0,i]\n",
    "            datos[:,i] /= estadisticas[1,i]\n",
    "\n",
    "        return datos, estadisticas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La siguiente clase, <b>EstrategiaParticionado</b>, es exactamente igual que en la práctica anterior:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABCMeta,abstractmethod\n",
    "\n",
    "from abc import ABCMeta, abstractmethod\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class Particion():\n",
    "\n",
    "  # Esta clase mantiene la lista de �ndices de Train y Test para cada partici�n del conjunto de particiones\n",
    "  def __init__(self, train=[], test=[]):\n",
    "    self.indicesTrain = train\n",
    "    self.indicesTest = test\n",
    "\n",
    "  def __str__(self):\n",
    "    return \"Train: {}\\nTest:  {}\".format(str(self.indicesTrain), str(self.indicesTest))\n",
    "\n",
    "\n",
    "class EstrategiaParticionado:\n",
    "  # Clase abstracta\n",
    "  __metaclass__ = ABCMeta\n",
    "\n",
    "  # Lista de las particiones\n",
    "  def __init__(self, nombre=\"\"):\n",
    "    self.nombreEstrategia = nombre\n",
    "    self.numeroParticiones = 0\n",
    "    self.particiones = []\n",
    "\n",
    "  # Atributos: deben rellenarse adecuadamente para cada estrategia concreta: nombreEstrategia, numeroParticiones, listaParticiones. Se pasan en el constructor\n",
    "\n",
    "  @abstractmethod\n",
    "  # TODO: esta funcion deben ser implementadas en cada estrategia concreta\n",
    "  def creaParticiones(self, datos, seed=None):\n",
    "    pass\n",
    "\n",
    "\n",
    "class ValidacionSimple(EstrategiaParticionado):\n",
    "\n",
    "  def __init__(self, porcentaje):\n",
    "    self.porcentaje = porcentaje\n",
    "    super().__init__(\"Validacion simple\")\n",
    "\n",
    "  # Crea particiones segun el metodo tradicional de division de los datos segun el porcentaje deseado.\n",
    "  # Devuelve una lista de particiones (clase Particion)\n",
    "  # TODO: implementar\n",
    "  def creaParticiones(self, datos, seed=None):\n",
    "    np.random.seed(seed)\n",
    "    self.numeroParticiones = 1\n",
    "\n",
    "    # Generamos una lista con todos los números de datos aleatorios\n",
    "    indicesAleatorios = np.random.permutation(int(datos.numDatos))\n",
    "\n",
    "    # Creamos la particion, en funcion del porcentaje especificado\n",
    "    self.particiones = [Particion(indicesAleatorios[:int(datos.numDatos * self.porcentaje)],\n",
    "                                  indicesAleatorios[int(datos.numDatos * self.porcentaje):])]\n",
    "\n",
    "    return self.particiones\n",
    "\n",
    "\n",
    "class ValidacionCruzada(EstrategiaParticionado):\n",
    "\n",
    "  # Crea particiones segun el metodo de validacion cruzada.\n",
    "  # El conjunto de entrenamiento se crea con las nfolds-1 particiones y el de test con la particion restante\n",
    "  # Esta funcion devuelve una lista de particiones (clase Particion)\n",
    "  # TODO: implementar\n",
    "\n",
    "  def __init__(self, k):\n",
    "    self.k = k\n",
    "    super().__init__(\"Validacion cruzada\")\n",
    "\n",
    "  def creaParticiones(self, datos, seed=None):\n",
    "\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    self.numeroParticiones = self.k\n",
    "\n",
    "    # Generamos una lista con todos los números de datos aleatorios\n",
    "    indicesAleatorios = np.random.permutation(int(datos.numDatos))\n",
    "\n",
    "    # Hallamos el tamaño de cada bloque\n",
    "    tamBloque = int(datos.numDatos / self.k)\n",
    "\n",
    "    datosSobran = datos.numDatos - (tamBloque * self.k)\n",
    "    count = 0\n",
    "    for i in range(self.k):\n",
    "\n",
    "      train = np.delete(indicesAleatorios, range(i * tamBloque, (i + 1) * tamBloque))\n",
    "      test = indicesAleatorios[i * tamBloque:(i + 1) * tamBloque]\n",
    "\n",
    "      # Caso en el que la cuenta es justa\n",
    "      if datosSobran == 0:\n",
    "        self.particiones.append(Particion(train, test))\n",
    "\n",
    "      # Contemplamos el caso de que la division para sacar el numero de subconjuntos no fuese entera\n",
    "      if datosSobran > 0:\n",
    "        count += 1\n",
    "        particionTest = np.append(test, train[(datos.numDatos - tamBloque) - i - 1])\n",
    "        particionTrain = np.delete(train, (datos.numDatos - tamBloque) - i - 1)\n",
    "        datosSobran -= 1\n",
    "        self.particiones.append(Particion(particionTrain, particionTest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación, tenemos la clase <b>Clasificador</b>, junto con <b>ClasificadorNaiveBayes</b>, idénticas a las de la práctica 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABCMeta,abstractmethod\n",
    "import numpy as np\n",
    "import math\n",
    "from collections import Counter\n",
    "from sortedcontainers import SortedDict\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from matplotlib import pyplot as plt\n",
    "from numpy import linalg as LA\n",
    "from statistics import mode\n",
    "\n",
    "class Clasificador:\n",
    "  # Clase abstracta\n",
    "  __metaclass__ = ABCMeta\n",
    "\n",
    "  # Metodos abstractos que se implementan en casa clasificador concreto\n",
    "  @abstractmethod\n",
    "  # TODO: esta funcion debe ser implementada en cada clasificador concreto\n",
    "  # datosTrain: matriz numpy con los datos de entrenamiento\n",
    "  # atributosDiscretos: array bool con la indicatriz de los atributos nominales\n",
    "  # diccionario: array de diccionarios de la estructura Datos utilizados para la codificacion de variables discretas\n",
    "  def entrenamiento(self, datos, datosTrain, atributosDiscretos, diccionario):\n",
    "    pass\n",
    "\n",
    "  @abstractmethod\n",
    "  # TODO: esta funcion debe ser implementada en cada clasificador concreto\n",
    "  # devuelve un numpy array con las predicciones\n",
    "  def clasifica(self, datosTest, atributosDiscretos, diccionario):\n",
    "    pass\n",
    "\n",
    "  # Obtiene el numero de aciertos y errores para calcular la tasa de fallo\n",
    "  # TODO: implementar\n",
    "  def error(self, datos, pred):\n",
    "    # Aqui se compara la prediccion (pred) con las clases reales y se calcula el error\n",
    "    i = 0\n",
    "    real = datos[:, -1]\n",
    "    error = 0\n",
    "    for i in range(len(real)):\n",
    "      if real[i] != pred[i]:\n",
    "        error += 1\n",
    "    err = (error) / (len(real) + 0.0)\n",
    "    return err\n",
    "\n",
    "  # Realiza una clasificacion utilizando una estrategia de particionado determinada\n",
    "  # particionado : estrategia de validacion que queremos utilizar\n",
    "  # dataset : clase de tipo Datos que utilizamos para entrenar y clasificar el modelo\n",
    "  # clasificador: instancia del clasificador que se va a usar\n",
    "  # TODO: implementar esta funcion\n",
    "  def validacion(self, particionado, dataset, clasificador, seed: object = None):\n",
    "\n",
    "    # Creamos las particiones siguiendo la estrategia llamando a particionado.creaParticiones\n",
    "    # - Para validacion cruzada: en el bucle hasta nv entrenamos el clasificador con la particion de train i\n",
    "    # y obtenemos el error en la particion de test i\n",
    "    # - Para validacion simple (hold-out): entrenamos el clasificador con la particion de train\n",
    "    # y obtenemos el error en la particion test. Otra opci�n es repetir la validaci�n simple un n�mero especificado de veces, obteniendo en cada una un error. Finalmente se calcular�a la media.\n",
    "    errores = 0\n",
    "    # particionado.creaParticiones(dataset, seed)\n",
    "    # Comprobamos si es por validación cruzada o simple, por la longitud de la lista de particiones\n",
    "    particionado.particiones = []\n",
    "    particionado.creaParticiones(dataset)\n",
    "\n",
    "    # Validación Simple\n",
    "    if len(particionado.particiones) == 1:\n",
    "      clasificador.entrenamiento(dataset, particionado.particiones[0].indicesTrain)\n",
    "      pred = clasificador.clasifica(dataset, particionado.particiones[0].indicesTest)\n",
    "      ret = self.error(dataset.extraeDatos(particionado.particiones[0].indicesTest), pred)\n",
    "      if ret > 0:\n",
    "        return ret\n",
    "      else:\n",
    "        return 0\n",
    "\n",
    "    # Validación Cruzada\n",
    "    else:\n",
    "      lista_error = []\n",
    "      for particion in particionado.particiones:\n",
    "        clasificador.entrenamiento(dataset, particion.indicesTrain)\n",
    "        pred = clasificador.clasifica(dataset, particion.indicesTest)\n",
    "        ret = self.error(dataset.extraeDatos(particion.indicesTest), pred)\n",
    "        lista_error.append(ret)\n",
    "      return np.mean(lista_error), np.std(lista_error)\n",
    "\n",
    "  def matrizConfusion(self, dataset, datosTest, prediccion):\n",
    "\n",
    "    # Calculamos la matriz de confusion utlizando sk-learn. Solo se calcula en el caso de que la clasificacion sea binaria.\n",
    "    testData = dataset.extraeDatos(datosTest)\n",
    "    clase_real = testData[:, -1]\n",
    "\n",
    "    matriz = confusion_matrix(prediccion, clase_real)\n",
    "\n",
    "    # La funcion ravel() devuelve todas las estadisticas relacionadas con la matriz de confusion\n",
    "    tn, fp, fn, tp = matriz.ravel()\n",
    "\n",
    "    # Calculamos las tasas extraídas de la matriz de confusión\n",
    "    tpr = tp / (tp + fn)\n",
    "    fpr = fp / (fp + fn)\n",
    "\n",
    "    self.lista_tpr.append(tpr)\n",
    "    self.lista_fpr.append(fpr)\n",
    "\n",
    "    return matriz\n",
    "\n",
    "  def curvaROC(self):\n",
    "\n",
    "    x = np.linspace(0, 1, 100)\n",
    "    plt.plot(x, x, c='blue')\n",
    "    for i in range(len(self.lista_fpr)):\n",
    "      plt.plot(self.lista_fpr[i], self.lista_tpr[i], 'ro')\n",
    "    plt.show()\n",
    "\n",
    "class ClasificadorNaiveBayes(Clasificador):\n",
    "\n",
    "  def __init__(self, laplace):\n",
    "    self.laplace = laplace\n",
    "    self.lista_fpr = []\n",
    "    self.lista_tpr = []\n",
    "\n",
    "  def entrenamiento(self, dataset, datosTrain):\n",
    "\n",
    "    # Cargamos todos los datos de la clase del dataset desde la matriz de datos\n",
    "    clasesTrain = dataset.extraeDatos(datosTrain)\n",
    "    self.numClases = clasesTrain[:, -1]\n",
    "\n",
    "    # Contamos las apariciones de cada uno para luego calcular la probabilidad a priori de cada clase\n",
    "    counter = Counter(self.numClases)\n",
    "\n",
    "    # Calculamos la probabilidad de la clase y lo metemos en un diccionario ordenado segun el numero\n",
    "    # correspondiente a cada clase asignado en el diccionario\n",
    "    self.dictPrioris = {}\n",
    "    for k in counter:\n",
    "      k = int(k)\n",
    "      counter[k] = counter[k] / len(self.numClases)\n",
    "      self.dictPrioris[k] = counter[k]\n",
    "\n",
    "    # Aqui ordenamos el diccionario para que esten en el mismo orden de como extraemos los datos del dataset\n",
    "    self.dictPrioris = SortedDict(self.dictPrioris)\n",
    "\n",
    "    # Calcular tablas de probabilidades del entrenamiento. Tenemos que calcular por cada atributo una cuenta\n",
    "    # de las apariciones en cada clase\n",
    "    # Creamos una lista de matrices, donde vamos almacenar todos los datos que hemos obtenido en los datos de Test\n",
    "    self.posteriori = np.zeros(len(dataset.nombreAtributos) - 1, dtype=object)\n",
    "\n",
    "    # Recorremos todos los datos de la matriz sin llegar a la clase\n",
    "    for i in range(len(dataset.nombreAtributos) - 1):\n",
    "\n",
    "      # Si el dato que obtenemos es Nominal haremos el recuento de todas las veces que sale la P(D|H)\n",
    "      if dataset.nominalAtributos[i] == True:\n",
    "\n",
    "        # Creamos una matriz de tamaño X: Número de Atributos menos la clase Y: Número de clases\n",
    "        post = np.zeros((len(dataset.listaDicts[i]), len(dataset.listaDicts[-1])))\n",
    "\n",
    "        # Aqui contamos todos las datos que queremos del datos Train para construir la matriz de entrenamiento\n",
    "        for c in range(len(dataset.listaDicts[-1])):\n",
    "          datosEnt = dataset.extraeDatos(datosTrain)\n",
    "          dat = datosEnt[:, i]\n",
    "          repes = Counter(dat[datosEnt[:, -1] == c])\n",
    "          for r in repes:\n",
    "            post[int(r), c] = repes[r]\n",
    "          if self.laplace == True:\n",
    "            self.posteriori[i] = post + 1\n",
    "          else:\n",
    "            self.posteriori[i] = post\n",
    "\n",
    "      # Si el dato es Continuo obtendremos la media y la desviación tipica de la clase\n",
    "      else:\n",
    "\n",
    "        # Creamos una matriz de X: Los datos de Media y Desivación típica Y: Número de clases\n",
    "        post = np.zeros((2, len(dataset.listaDicts[-1])))\n",
    "\n",
    "        # Aqui obtenemos la media y desviación tipica de cada clase, despues de tener los datos de entrenamiento\n",
    "        for c in range(len(dataset.listaDicts[-1])):\n",
    "          datosEnt = dataset.extraeDatos(datosTrain)\n",
    "          dat = datosEnt[:, i]\n",
    "          datos = dat[datosEnt[:, -1] == c]\n",
    "          post[0][c] = np.mean(datos)\n",
    "          post[1][c] = np.std(datos)\n",
    "        self.posteriori[i] = post\n",
    "\n",
    "\n",
    "    # Calculamos los valores de los posteriori de todos las tablas anteriores\n",
    "    for i in range(len(dataset.listaDicts) - 1):\n",
    "      if dataset.nominalAtributos[i] == True:\n",
    "        self.posteriori[i] /= sum(self.posteriori[i])\n",
    "\n",
    "  def clasifica(self, dataset, datosTest):\n",
    "    acum_probs = 1\n",
    "    self.prediccion = []\n",
    "    datTest = dataset.extraeDatos(datosTest)\n",
    "\n",
    "    # Ahora vamos a estudiar la probabilidad de la clase con los datos obtenidos en el entrenamiento\n",
    "    # Recorremos todos las datos de la matriz de los datos Test\n",
    "    for dato in datTest:\n",
    "      mapa = []\n",
    "      # Aqui obtenemos los prioris de cada clase para poder obtener la probabilidad de cada una\n",
    "      for clase in range(len(self.dictPrioris)):\n",
    "        listaVerosimilitudes = []\n",
    "        # Aqui obtenemos cada valor posteriori de nuestro entrenamiento de los datos, es decir, P(D|H)\n",
    "        for atributo in range(len(self.posteriori)):\n",
    "          if dataset.nominalAtributos[atributo] == True:\n",
    "            prob = self.posteriori[atributo][int(dato[atributo])][clase]\n",
    "            listaVerosimilitudes.append(prob)\n",
    "\n",
    "          # Aqui obtenemos la probabilidad de los atibutos continuos\n",
    "          else:\n",
    "            # Hacemos la formula de la distribucion normal\n",
    "            exp1 = 1 / (self.posteriori[atributo][1][clase] * math.sqrt(2 * math.pi))\n",
    "            exp2 = np.power((dato[atributo] - self.posteriori[atributo][0][clase]), 2)\n",
    "            exp3 = np.power(self.posteriori[atributo][1][clase], 2)\n",
    "            exp4 = exp2 / exp3\n",
    "            exp4 = math.exp((-1 / 2) * exp4)\n",
    "            prob = exp1 * exp4\n",
    "            listaVerosimilitudes.append(prob)\n",
    "\n",
    "        for verosimilitud in listaVerosimilitudes:\n",
    "          acum_probs *= verosimilitud\n",
    "        acum_probs *= self.dictPrioris.get(clase)\n",
    "        mapa.append(acum_probs)\n",
    "        acum_probs = 1\n",
    "\n",
    "      # Aqui obtenemos la predicción de mayor probabilidad y la guardamos en nuestra lista de predicciones\n",
    "      self.prediccion.append(np.argmax(mapa))\n",
    "\n",
    "\n",
    "    # Devolvemos la lista con la predicción de nuestro clasifica\n",
    "    return self.prediccion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En las siguientes celdas aparecen las clases <b>ClasificadorVecinosProximos</b> y <b>ClasificadorRegresionLogistica</b>, ambas necesarias para la funcionalidad requerida en esta práctica.\n",
    "\n",
    "La clase <b>ClasificadorVecinosProximos</b> recibe como parámetros en su constructor tanto el valor de <b>K</b>y si se quiere normalizar el dataset o no, por defecto la normalización está activada. El método <b>entrenamiento()</b>, normaliza el dataset si tenemos esa opción a True y guarda los datos de entrenamiento. En el caso de que no queremos normalizar, simplemente guarda los datos de entrenamiento. El método <b>clasifica()</b> implementa el algoritmo de <b>Vecinos Próximos</b>, calculando las distancias de los datos de Test con todos los datos de Train y quedandose con la clase mayoritaría de los k elementos con menor distancia, para cada dato del conjunto de Test. Todo lo anterior genera una lista con las clases predichas, que es lo que devuelve la función.\n",
    "\n",
    "La clase <b>ClasificadorRegresionLogistica</b> se inicializa con dos parámetros, la <b>Constante de Aprendizaje</b> y el <b>número de épocas</b> que va a tener el algoritmo. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
